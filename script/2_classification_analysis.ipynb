{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ce8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ASV CLASSIFICATION PIPELINE\n",
      "Version 3.0 - Intra-Species Variants & No Mixed\n",
      "================================================================================\n",
      "\n",
      "Started: 2025-10-24 12:49:55\n",
      "\n",
      "Loading data from: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_Complete_Analysis.csv\n",
      "  ✓ Loaded 175,955 rows\n",
      "  ✓ Columns: 149\n",
      "\n",
      "================================================================================\n",
      "STEP 1: BUILDING AUTHENTICATED DATABASE\n",
      "================================================================================\n",
      "\n",
      "Criteria: autopropose='select' AND match='match'\n",
      "\n",
      "  Found 15,901 authenticated instances\n",
      "\n",
      "  ✓ Unique authenticated ASVs: 12,922\n",
      "  ✓ Total authenticated instances: 15,901\n",
      "\n",
      "  Authenticated ASVs per sample:\n",
      "      1 sample(s): 11,133 ASVs\n",
      "      2 sample(s): 1,244 ASVs\n",
      "      3 sample(s):   314 ASVs\n",
      "      4 sample(s):   104 ASVs\n",
      "      5 sample(s):    55 ASVs\n",
      "      6 sample(s):    25 ASVs\n",
      "      7 sample(s):    14 ASVs\n",
      "      8 sample(s):     7 ASVs\n",
      "      9 sample(s):     5 ASVs\n",
      "     10 sample(s):     5 ASVs\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CALCULATING SAMPLE STATISTICS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Processing ASVs: 100%|██████████| 64544/64544 [00:02<00:00, 21687.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Calculated statistics for 64,544 unique ASVs\n",
      "\n",
      "================================================================================\n",
      "PRE-FILTER: TECHNICAL ARTIFACTS (reads < 4)\n",
      "================================================================================\n",
      "\n",
      "Filtering reads < 4:\n",
      "  Technical_Artifacts: 84,551 (48.1%)\n",
      "  Biological samples:  91,404 (51.9%)\n",
      "\n",
      "  ✓ Pre-filter complete\n",
      "  ✓ Continuing with 91,404 biological samples\n",
      "\n",
      "================================================================================\n",
      "STEP 3: DETECTING INTRA-SPECIES VARIANTS\n",
      "================================================================================\n",
      "\n",
      "Method: Co-occurrence pattern analysis\n",
      "Threshold: ≥2 co-occurrences with same authenticated ASV\n",
      "\n",
      "  Found 61,577 unique (authenticated_ASV, secondary_ASV) pairs\n",
      "\n",
      "  Strong evidence Intra-Species Variants:\n",
      "    Secondary ASVs: 1,790\n",
      "    Co-occurrence counts:\n",
      "      Min: 2\n",
      "      Max: 15\n",
      "      Avg: 2.5\n",
      "\n",
      "    Distribution:\n",
      "      2 co-occurrences: 1,378 secondary ASVs\n",
      "      3 co-occurrences: 256 secondary ASVs\n",
      "      4 co-occurrences: 57 secondary ASVs\n",
      "      5 co-occurrences: 39 secondary ASVs\n",
      "      6 co-occurrences: 18 secondary ASVs\n",
      "      7 co-occurrences: 13 secondary ASVs\n",
      "      8 co-occurrences: 11 secondary ASVs\n",
      "      9 co-occurrences: 6 secondary ASVs\n",
      "      10 co-occurrences: 4 secondary ASVs\n",
      "      11 co-occurrences: 6 secondary ASVs\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: RULE-BASED CLASSIFICATION (BIOLOGICAL)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Classifying: 100%|██████████| 91404/91404 [00:16<00:00, 5708.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1 Summary (Biological classes):\n",
      "\n",
      "                             count  avg_confidence  percentage\n",
      "phase1_class                                                  \n",
      "Uncertain                    43807        0.500000   47.926787\n",
      "Authenticated                15901        1.000000   17.396394\n",
      "Environmental_Contamination  13767        0.823524   15.061704\n",
      "Cross_Contamination          13163        0.940682   14.400901\n",
      "Intra_Species_Variant         3359        0.735368    3.674894\n",
      "Technical_Artifacts           1407        0.797655    1.539320\n",
      "\n",
      "Confidence Distribution (Biological):\n",
      "  High (≥0.90):       29,596 (32.38%)\n",
      "  Medium (0.70-0.89): 18,001 (19.69%)\n",
      "  Low (<0.70):        43,807 (47.93%)\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: ML TRAINING & PREDICTION (BIOLOGICAL)\n",
      "================================================================================\n",
      "\n",
      "Extracting ML features (biological samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Extracting features: 100%|██████████| 91404/91404 [00:27<00:00, 3334.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Extracted 57 features\n",
      "\n",
      "Training samples: 47,597 (confidence ≥0.70, excluding Uncertain)\n",
      "  Excluded Uncertain: 43,807\n",
      "\n",
      "================================================================================\n",
      "STEP 7: TRAINING ML MODEL\n",
      "================================================================================\n",
      "\n",
      "Training data: 47,597 samples (excluding Uncertain)\n",
      "Features: 57\n",
      "\n",
      "Classes (5):\n",
      "  Authenticated                      : 15,901 (33.41%)\n",
      "  Cross_Contamination                : 13,163 (27.66%)\n",
      "  Environmental_Contamination        : 13,767 (28.92%)\n",
      "  Intra_Species_Variant              :  3,359 ( 7.06%)\n",
      "  Technical_Artifacts                :  1,407 ( 2.96%)\n",
      "\n",
      "Splitting with stratification...\n",
      "\n",
      "Split:\n",
      "  Train: 38,077\n",
      "  Test:  9,520\n",
      "\n",
      "Class weights:\n",
      "  Authenticated                      : 0.599\n",
      "  Cross_Contamination                : 0.723\n",
      "  Environmental_Contamination        : 0.691\n",
      "  Intra_Species_Variant              : 2.834\n",
      "  Technical_Artifacts                : 6.766\n",
      "\n",
      "Training Random Forest...\n",
      "  ✓ Training complete\n",
      "\n",
      "✓ Test Accuracy: 0.995\n",
      "\n",
      "Cross-validation (5-fold)...\n",
      "  ✓ CV Accuracy: 0.996 ± 0.001\n",
      "\n",
      "Classification Report:\n",
      "--------------------------------------------------------------------------------\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "              Authenticated       1.00      1.00      1.00      3180\n",
      "        Cross_Contamination       1.00      1.00      1.00      2633\n",
      "Environmental_Contamination       1.00      0.98      0.99      2754\n",
      "      Intra_Species_Variant       0.94      1.00      0.97       672\n",
      "        Technical_Artifacts       1.00      1.00      1.00       281\n",
      "\n",
      "                   accuracy                           0.99      9520\n",
      "                  macro avg       0.99      1.00      0.99      9520\n",
      "               weighted avg       1.00      0.99      1.00      9520\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                             Authenticated  Cross_Contamination  \\\n",
      "Authenticated                         3180                    0   \n",
      "Cross_Contamination                      0                 2633   \n",
      "Environmental_Contamination              0                    0   \n",
      "Intra_Species_Variant                    0                    0   \n",
      "Technical_Artifacts                      0                    0   \n",
      "\n",
      "                             Environmental_Contamination  \\\n",
      "Authenticated                                          0   \n",
      "Cross_Contamination                                    0   \n",
      "Environmental_Contamination                         2710   \n",
      "Intra_Species_Variant                                  3   \n",
      "Technical_Artifacts                                    1   \n",
      "\n",
      "                             Intra_Species_Variant  Technical_Artifacts  \n",
      "Authenticated                                    0                    0  \n",
      "Cross_Contamination                              0                    0  \n",
      "Environmental_Contamination                     43                    1  \n",
      "Intra_Species_Variant                          669                    0  \n",
      "Technical_Artifacts                              0                  280  \n",
      "\n",
      "Top 20 Important Features:\n",
      "  n_authenticated_samples            : 0.11629\n",
      "  is_authenticated_elsewhere         : 0.09416\n",
      "  phylo_distance                     : 0.07860\n",
      "  likely_cross_cont                  : 0.07770\n",
      "  log_phylo_distance                 : 0.05936\n",
      "  taxonomy_match                     : 0.05879\n",
      "  taxonomy_mismatch                  : 0.05831\n",
      "  log_total_reads_all                : 0.04728\n",
      "  total_reads_all_samples            : 0.03230\n",
      "  percentage_reads                   : 0.03148\n",
      "  family_match                       : 0.03008\n",
      "  log_abundance                      : 0.02990\n",
      "  likely_artifact                    : 0.02832\n",
      "  max_reads_in_sample                : 0.02463\n",
      "  quality_score                      : 0.02385\n",
      "  abundance                          : 0.02346\n",
      "  likely_environmental               : 0.01944\n",
      "  phylo_very_close                   : 0.01801\n",
      "  n_samples_present                  : 0.01685\n",
      "  GC_content                         : 0.01593\n",
      "\n",
      "================================================================================\n",
      "STEP 8: ML PREDICTION & FINAL CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "Predicting all biological samples...\n",
      "  ✓ Predictions complete\n",
      "\n",
      "AGREEMENT ANALYSIS:\n",
      "  Overall agreement: 51.9%\n",
      "\n",
      "  Agreement by Phase 1 confidence:\n",
      "    High (≥0.90)        : 100.0% (29,596 cases)\n",
      "    Medium (0.70-0.89)  :  99.3% (18,001 cases)\n",
      "    Low (<0.70)         :   0.0% (43,807 cases)\n",
      "\n",
      "DISAGREEMENTS (Phase 1 ≠ ML):\n",
      "  Total: 43,932 (48.1%)\n",
      "\n",
      "  Top 10 disagreement patterns:\n",
      "    Phase1: Uncertain                 → ML: Intra_Species_Variant     : 16,362 (37.2%)\n",
      "    Phase1: Uncertain                 → ML: Technical_Artifacts       : 16,103 (36.7%)\n",
      "    Phase1: Uncertain                 → ML: Environmental_Contamination : 11,328 (25.8%)\n",
      "    Phase1: Environmental_Contamination → ML: Intra_Species_Variant     :   117 ( 0.3%)\n",
      "    Phase1: Uncertain                 → ML: Authenticated             :    14 ( 0.0%)\n",
      "    Phase1: Environmental_Contamination → ML: Technical_Artifacts       :     4 ( 0.0%)\n",
      "    Phase1: Intra_Species_Variant     → ML: Environmental_Contamination :     3 ( 0.0%)\n",
      "    Phase1: Technical_Artifacts       → ML: Environmental_Contamination :     1 ( 0.0%)\n",
      "\n",
      "FINAL CLASSIFICATION DECISION:\n",
      "\n",
      "  Decision method distribution:\n",
      "    ML_Both_Uncertain                  : 34,475 (37.72%)\n",
      "    ML_Confirmed_by_Rule               : 31,177 (34.11%)\n",
      "    Rule_Confirmed_by_ML               : 16,295 (17.83%)\n",
      "    ML_High_Rule_Lower                 :  9,439 (10.33%)\n",
      "    Rule_Weighted                      :     17 ( 0.02%)\n",
      "    Rule_High_ML_Uncertain             :      1 ( 0.00%)\n",
      "\n",
      "✓ Model saved: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Classification_Model.pkl\n",
      "\n",
      "================================================================================\n",
      "STEP 9: POST-VALIDATION FIXES\n",
      "================================================================================\n",
      "  ✓ Flagged 13 suspicious Authenticated for review\n",
      "\n",
      "  Checking 1-Authenticated-per-sample rule...\n",
      "  ⚠️  Found 14 samples with multiple Authenticated\n",
      "  → Need to fix 14 ASVs\n",
      "  ✓ Fixed 14 cases:\n",
      "    - Reclassified as Intra_Species_Variant\n",
      "    - Kept one Authenticated per sample (Phase 1 priority)\n",
      "    - Flagged for review\n",
      "\n",
      "  Final validation:\n",
      "  ✓ All samples have ≤1 Authenticated\n",
      "\n",
      "  Checking taxonomy consistency...\n",
      "  ✓ All Intra-Species have match='match'\n",
      "  ⚠️  Found 515 Environmental with match='match'\n",
      "      → Keeping as Environmental but flagging for review\n",
      "      (These are same species but distant populations/contamination)\n",
      "  ✓ Flagged 515 for review\n",
      "\n",
      "  Taxonomy verification:\n",
      "  ✅ Intra-Species (19,831): 100% match='match'\n",
      "  ℹ️  Environmental (24,996):\n",
      "      - match≠'match': 24,481 (97.9%)\n",
      "      - match='match': 515 (2.1%) ← flagged\n",
      "\n",
      "  Phylogenetic distance analysis (informational):\n",
      "  Intra-Species (19,831):\n",
      "    <0.05: 9,573 (48.3%)\n",
      "    0.05-0.15: 7,129 (35.9%)\n",
      "    ≥0.15: 3,129 (15.8%)\n",
      "    Note: 3,129 distant but same species (kept as Intra)\n",
      "  Environmental (24,996):\n",
      "    <0.05: 3,704 (14.8%)\n",
      "    0.05-0.15: 1,108 (4.4%)\n",
      "    ≥0.15: 20,184 (80.7%)\n",
      "    Note: 3,704 close but different species (kept as Env)\n",
      "\n",
      "  ✓ Total fixes applied: 14\n",
      "\n",
      "================================================================================\n",
      "STEP 10: CREATING SEQUENCE-LEVEL SUMMARY (No Mixed)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Processing ASVs: 100%|██████████| 64544/64544 [00:21<00:00, 2987.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Created summary for 64,544 unique ASVs\n",
      "  ✓ Confirmed: No Mixed category\n",
      "\n",
      "================================================================================\n",
      "STEP 11: VALIDATION & STATISTICS\n",
      "================================================================================\n",
      "\n",
      "1. VALIDATION CHECKS:\n",
      "  ✓ All 15,901 Authenticated have autopropose='select' AND match='match'\n",
      "  ✓ All 13,163 Cross-Contamination in authenticated database\n",
      "  ✓ Technical_Artifacts: 84,551 with reads <4, 17,513 other types\n",
      "  ✓ No Uncertain in final classification (all classified by ML)\n",
      "      Phylo distance distribution:\n",
      "        <0.05: 9,573 (48.3%)\n",
      "        0.05-0.15: 7,129 (35.9%)\n",
      "      Phylo distance distribution:\n",
      "        <0.05: 3,704 (14.8%)\n",
      "        0.05-0.15: 1,108 (4.4%)\n",
      "        ≥0.15: 20,184 (80.7%)\n",
      "\n",
      "  Issues found:\n",
      "  ❌ 3129 Intra-Species with phylo ≥ 0.15\n",
      "  ❌ 515 Environmental with match='match'\n",
      "  ❌ 3704 Environmental with phylo < 0.05\n",
      "\n",
      "2. CLASSIFICATION STATISTICS:\n",
      "\n",
      "  Per-Sample Level (175,955 total):\n",
      "\n",
      "                              count  avg_confidence  total_reads  reads_percentage  percentage\n",
      "final_classification                                                                          \n",
      "Technical_Artifacts          101222        0.874382     221441.0          0.607355   57.527209\n",
      "Environmental_Contamination   24996        0.893507    3458593.0          9.486018   14.205905\n",
      "Intra_Species_Variant         19831        0.756071    1186425.0          3.254054   11.270495\n",
      "Authenticated                 15901        1.000000   30272094.0         83.028450    9.036970\n",
      "Cross_Contamination           13163        0.999357    1321352.0          3.624124    7.480890\n",
      "\n",
      "  Sequence Level (64,544 unique ASVs):\n",
      "    Technical_Artifacts                     : 24,035 (37.24%)\n",
      "    Intra_Species_Variant                   : 16,632 (25.77%)\n",
      "    Authenticated                           : 12,922 (20.02%)\n",
      "    Environmental_Contamination             : 10,955 (16.97%)\n",
      "\n",
      "3. CONFIDENCE DISTRIBUTION:\n",
      "  High (≥0.80): 141,145 (80.2%)\n",
      "  Medium (0.60-0.80): 14,589 (8.3%)\n",
      "  Low (<0.60): 20,221 (11.5%)\n",
      "\n",
      "  Flagged for review: 20,560 (11.7%)\n",
      "\n",
      "================================================================================\n",
      "STEP 12: EXPORTING RESULTS\n",
      "================================================================================\n",
      "\n",
      "1. Main Classification File:\n",
      "   ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/ASV_Final_Classification.csv\n",
      "     175,955 rows × 165 columns\n",
      "\n",
      "2. Sequence Summary:\n",
      "   ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/ASV_Sequence_Summary.csv\n",
      "     64,544 unique ASVs\n",
      "\n",
      "3. Classification Statistics:\n",
      "   ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Classification_Statistics.csv\n",
      "\n",
      "4. Feature Importance:\n",
      "   ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Feature_Importance.csv\n",
      "\n",
      "5. Detailed Report:\n",
      "   ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Classification_Report.txt\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Dataset:\n",
      "  Total records: 175,955\n",
      "  Unique samples: 20,001\n",
      "  Unique ASVs: 64,544\n",
      "  Total reads: 36,459,905\n",
      "\n",
      "Pre-filter Results:\n",
      "  Technical_Artifacts: 84,551 (48.1%)\n",
      "  Biological samples:  91,404 (51.9%)\n",
      "\n",
      "Final Classification Results:\n",
      "  Technical_Artifacts                     : 102,064 (58.01%) [conf=0.874]\n",
      "  Environmental_Contamination             : 24,996 (14.21%) [conf=0.894]\n",
      "  Intra_Species_Variant                   : 19,831 (11.27%) [conf=0.756]\n",
      "  Authenticated                           : 15,901 ( 9.04%) [conf=1.000]\n",
      "  Cross_Contamination                     : 13,163 ( 7.48%) [conf=0.999]\n",
      "\n",
      "  ✓ No Uncertain in final classification\n",
      "\n",
      "Quality Control:\n",
      "  Needs review: 20,560 (11.7%)\n",
      "  High confidence (≥0.80): 141,145\n",
      "\n",
      "Output Files:\n",
      "  ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/ASV_Final_Classification.csv\n",
      "  ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/ASV_Sequence_Summary.csv\n",
      "  ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Classification_Statistics.csv\n",
      "  ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Classification_Report.txt\n",
      "  ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Feature_Importance.csv\n",
      "  ✓ /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Classification_Model.pkl\n",
      "\n",
      "================================================================================\n",
      "✓ PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Finished: 2025-10-24 12:56:23\n",
      "\n",
      "Next steps:\n",
      "  1. Review flagged sequences in: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/ASV_Final_Classification.csv\n",
      "  2. Check detailed report: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis/Classification_Report.txt\n",
      "  3. Use Authenticated sequences for downstream analysis\n",
      "  4. Intra-Species Variants require genomic validation\n",
      "  5. Consider reads < 4 as technical noise (excluded)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete ASV Classification Pipeline\n",
    "Version: 3.0 - Final with Intra-Species Variants & No Mixed\n",
    "\n",
    "Key Features:\n",
    "- Pre-filter Technical_Artifacts (reads < 4)\n",
    "- Intra-Species Variants based on co-occurrence pattern\n",
    "- Phase 1: Strong evidence only (no Uncertain in training)\n",
    "- ML trained on strong evidence, predicts all including Uncertain\n",
    "- Sequence Summary without Mixed (priority-based)\n",
    "- All Uncertain classified by ML\n",
    "\n",
    "Author: Scientific Pipeline\n",
    "Date: 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "    print(\"⚠️  tqdm not available. Install with: pip install tqdm\")\n",
    "\n",
    "# Machine Learning\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.utils import class_weight\n",
    "    import joblib\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"❌ ERROR: scikit-learn required\")\n",
    "    print(\"Install with: pip install scikit-learn\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# SMOTE for imbalanced data\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    SMOTE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SMOTE_AVAILABLE = False\n",
    "    print(\"⚠️  imbalanced-learn not available. Install with: pip install imbalanced-learn\")\n",
    "\n",
    "# ========================\n",
    "# CONFIGURATION\n",
    "# ========================\n",
    "\n",
    "INPUT_FILE = \"/Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_Complete_Analysis.csv\"\n",
    "OUTPUT_DIR = \"/Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis//classification_analysis\"\n",
    "\n",
    "# Output files\n",
    "OUTPUT_CLASSIFIED = f\"{OUTPUT_DIR}/ASV_Final_Classification.csv\"\n",
    "OUTPUT_SEQUENCE_SUMMARY = f\"{OUTPUT_DIR}/ASV_Sequence_Summary.csv\"\n",
    "OUTPUT_STATISTICS = f\"{OUTPUT_DIR}/Classification_Statistics.csv\"\n",
    "OUTPUT_FEATURE_IMPORTANCE = f\"{OUTPUT_DIR}/Feature_Importance.csv\"\n",
    "OUTPUT_REPORT = f\"{OUTPUT_DIR}/Classification_Report.txt\"\n",
    "OUTPUT_MODEL = f\"{OUTPUT_DIR}/Classification_Model.pkl\"\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================\n",
    "# THRESHOLDS\n",
    "# ========================\n",
    "\n",
    "THRESHOLDS = {\n",
    "    # Abundance\n",
    "    'technical_artifacts_reads': 4,\n",
    "    'very_low_reads': 10,\n",
    "    'low_reads': 100,\n",
    "    'medium_reads_min': 100,\n",
    "    'medium_reads_max': 1000,\n",
    "    'high_reads': 1000,\n",
    "    \n",
    "    'very_low_abundance': 0.01,\n",
    "    'low_abundance': 0.05,\n",
    "    'medium_abundance_min': 0.05,\n",
    "    'medium_abundance_max': 0.50,\n",
    "    'high_abundance': 0.50,\n",
    "    \n",
    "    # Cross-contamination\n",
    "    'cross_cont_clear': 0.05,\n",
    "    'cross_cont_likely': 0.10,\n",
    "    'cross_cont_possible': 0.20,\n",
    "    \n",
    "    # Environmental\n",
    "    'env_order_mismatch': 0.20,\n",
    "    'env_class_mismatch': 0.30,\n",
    "    'env_phylo_distance': 0.20,\n",
    "    'env_phylo_threshold': 0.15,\n",
    "    \n",
    "    # Quality\n",
    "    'quality_low': 60,\n",
    "    'quality_medium': 70,\n",
    "    'quality_high': 80,\n",
    "    \n",
    "    # Composition (for NUMTs)\n",
    "    'gc_normal_min': 32,\n",
    "    'gc_normal_max': 40,\n",
    "    'motif_score_degraded': 70,\n",
    "    \n",
    "    # Phylogenetic\n",
    "    'phylo_same_species': 0.02,\n",
    "    'phylo_same_genus': 0.05,\n",
    "    'phylo_divergent': 0.10,\n",
    "    'phylo_intra_max': 0.15,\n",
    "    'phylo_env_min': 0.05, \n",
    "    \n",
    "    # Distribution\n",
    "    'widespread_threshold': 5,\n",
    "    \n",
    "    # Length\n",
    "    'length_min': 200,\n",
    "    'length_max': 900,\n",
    "    \n",
    "    # Intra-Species Variants\n",
    "    'intra_species_min_cooccurrence': 2,\n",
    "}\n",
    "\n",
    "# ========================\n",
    "# HELPER FUNCTIONS\n",
    "# ========================\n",
    "\n",
    "def safe_float(val, default=0.0):\n",
    "    \"\"\"Safely convert to float\"\"\"\n",
    "    try:\n",
    "        if pd.isna(val):\n",
    "            return default\n",
    "        return float(val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def safe_int(val, default=0):\n",
    "    \"\"\"Safely convert to int\"\"\"\n",
    "    try:\n",
    "        if pd.isna(val):\n",
    "            return default\n",
    "        return int(val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def safe_str(val, default=''):\n",
    "    \"\"\"Safely convert to string\"\"\"\n",
    "    try:\n",
    "        if pd.isna(val):\n",
    "            return default\n",
    "        return str(val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def safe_bool_to_int(val, default=False):\n",
    "    \"\"\"Safely convert to boolean then int\"\"\"\n",
    "    try:\n",
    "        if pd.isna(val):\n",
    "            return int(default)\n",
    "        if isinstance(val, bool):\n",
    "            return int(val)\n",
    "        if isinstance(val, (int, float)):\n",
    "            if pd.isna(val):\n",
    "                return int(default)\n",
    "            return int(bool(val))\n",
    "        if isinstance(val, str):\n",
    "            return int(val.lower() in ['true', '1', 'yes', 't'])\n",
    "        return int(default)\n",
    "    except:\n",
    "        return int(default)\n",
    "\n",
    "# ========================\n",
    "# STEP 1: BUILD AUTHENTICATED DATABASE\n",
    "# ========================\n",
    "\n",
    "def build_authenticated_database(df):\n",
    "    \"\"\"Build database of authenticated ASVs\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: BUILDING AUTHENTICATED DATABASE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nCriteria: autopropose='select' AND match='match'\")\n",
    "    \n",
    "    # Filter authenticated\n",
    "    auth_mask = (df['autopropose'] == 'select') & (df['match'] == 'match')\n",
    "    auth_rows = df[auth_mask].copy()\n",
    "    \n",
    "    print(f\"\\n  Found {len(auth_rows):,} authenticated instances\")\n",
    "    \n",
    "    if len(auth_rows) == 0:\n",
    "        print(\"  ⚠️  WARNING: No authenticated ASVs found!\")\n",
    "        return {}\n",
    "    \n",
    "    # Build database\n",
    "    auth_db = {}\n",
    "    \n",
    "    for _, row in auth_rows.iterrows():\n",
    "        asv_id = safe_str(row.get('asv_id', ''))\n",
    "        sample_id = safe_str(row.get('project_readfile_id', ''))\n",
    "        \n",
    "        if not asv_id or not sample_id:\n",
    "            continue\n",
    "        \n",
    "        if asv_id not in auth_db:\n",
    "            auth_db[asv_id] = {\n",
    "                'sample_ids': [],\n",
    "                'reads_list': [],\n",
    "                'family': safe_str(row.get('family', '')),\n",
    "                'subfamily': safe_str(row.get('subfamily', '')),\n",
    "                'order': safe_str(row.get('blast_tax_order', '')),\n",
    "                'genus': safe_str(row.get('blast_tax_genus', '')),\n",
    "            }\n",
    "        \n",
    "        auth_db[asv_id]['sample_ids'].append(sample_id)\n",
    "        auth_db[asv_id]['reads_list'].append(safe_float(row.get('reads', 0)))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    for asv_id in auth_db:\n",
    "        reads_list = auth_db[asv_id]['reads_list']\n",
    "        auth_db[asv_id]['total_reads'] = sum(reads_list)\n",
    "        auth_db[asv_id]['avg_reads'] = np.mean(reads_list) if reads_list else 0\n",
    "        auth_db[asv_id]['n_samples'] = len(auth_db[asv_id]['sample_ids'])\n",
    "    \n",
    "    print(f\"\\n  ✓ Unique authenticated ASVs: {len(auth_db):,}\")\n",
    "    print(f\"  ✓ Total authenticated instances: {len(auth_rows):,}\")\n",
    "    \n",
    "    # Distribution\n",
    "    n_samples_dist = Counter([info['n_samples'] for info in auth_db.values()])\n",
    "    print(f\"\\n  Authenticated ASVs per sample:\")\n",
    "    for n in sorted(n_samples_dist.keys())[:10]:\n",
    "        count = n_samples_dist[n]\n",
    "        print(f\"    {n:3d} sample(s): {count:5,} ASVs\")\n",
    "    \n",
    "    return auth_db\n",
    "\n",
    "# ========================\n",
    "# STEP 2: CALCULATE SAMPLE STATISTICS\n",
    "# ========================\n",
    "\n",
    "def calculate_sample_statistics(df):\n",
    "    \"\"\"Pre-calculate statistics per ASV across all samples\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: CALCULATING SAMPLE STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    asv_stats = {}\n",
    "    \n",
    "    # Group by ASV\n",
    "    asv_groups = df.groupby('asv_id')\n",
    "    \n",
    "    iterator = asv_groups\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(asv_groups, desc=\"  Processing ASVs\")\n",
    "    \n",
    "    for asv_id, group in iterator:\n",
    "        asv_stats[asv_id] = {\n",
    "            'n_samples': len(group),\n",
    "            'total_reads_all_samples': group['reads'].sum(),\n",
    "            'avg_reads_per_sample': group['reads'].mean(),\n",
    "            'max_reads': group['reads'].max(),\n",
    "            'min_reads': group['reads'].min(),\n",
    "            'is_singleton': len(group) == 1,\n",
    "            'is_widespread': len(group) >= THRESHOLDS['widespread_threshold'],\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n  ✓ Calculated statistics for {len(asv_stats):,} unique ASVs\")\n",
    "    \n",
    "    return asv_stats\n",
    "\n",
    "# ========================\n",
    "# STEP 3: DETECT INTRA-SPECIES VARIANTS\n",
    "# ========================\n",
    "\n",
    "def detect_intra_species_variants(df, auth_db):\n",
    "    \"\"\"\n",
    "    Detect Intra-Species Variants based on co-occurrence pattern\n",
    "    Only classify if secondary ASV appears ≥2 times with same authenticated ASV\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: DETECTING INTRA-SPECIES VARIANTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nMethod: Co-occurrence pattern analysis\")\n",
    "    print(f\"Threshold: ≥{THRESHOLDS['intra_species_min_cooccurrence']} co-occurrences with same authenticated ASV\")\n",
    "    \n",
    "    # Build co-occurrence database\n",
    "    co_occurrence = defaultdict(int)\n",
    "    \n",
    "    # For each sample with authenticated ASV\n",
    "    for sample_id in df['project_readfile_id'].unique():\n",
    "        sample_df = df[df['project_readfile_id'] == sample_id]\n",
    "        \n",
    "        # Find authenticated ASV in this sample\n",
    "        auth_asv = sample_df[\n",
    "            (sample_df['autopropose'] == 'select') &\n",
    "            (sample_df['match'] == 'match')\n",
    "        ]\n",
    "        \n",
    "        if len(auth_asv) == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(auth_asv) > 1:\n",
    "            # Multiple authenticated - take highest reads\n",
    "            auth_asv = auth_asv.nlargest(1, 'reads')\n",
    "        \n",
    "        auth_asv_id = auth_asv.iloc[0]['asv_id']\n",
    "        \n",
    "        # Find all secondary ASVs in this sample\n",
    "        secondary_asvs = sample_df[\n",
    "            (sample_df['asv_id'] != auth_asv_id) &\n",
    "            (sample_df['reads'] >= THRESHOLDS['technical_artifacts_reads'])\n",
    "        ]\n",
    "        \n",
    "        # Count co-occurrences\n",
    "        for _, sec_row in secondary_asvs.iterrows():\n",
    "            sec_asv_id = sec_row['asv_id']\n",
    "            key = (auth_asv_id, sec_asv_id)\n",
    "            co_occurrence[key] += 1\n",
    "    \n",
    "    print(f\"\\n  Found {len(co_occurrence):,} unique (authenticated_ASV, secondary_ASV) pairs\")\n",
    "    \n",
    "    # Build strong evidence set\n",
    "    intra_species_db = {}\n",
    "    \n",
    "    for (auth_asv_id, sec_asv_id), count in co_occurrence.items():\n",
    "        if count >= THRESHOLDS['intra_species_min_cooccurrence']:\n",
    "            # Calculate confidence based on count\n",
    "            if count >= 3:\n",
    "                conf = 0.80\n",
    "                level = 'Strong'\n",
    "            else:  # count == 2\n",
    "                conf = 0.70\n",
    "                level = 'Moderate'\n",
    "            \n",
    "            # Store (keep highest count if secondary appears with multiple auth ASVs)\n",
    "            if sec_asv_id not in intra_species_db or \\\n",
    "               count > intra_species_db[sec_asv_id][1]:\n",
    "                intra_species_db[sec_asv_id] = (auth_asv_id, count, conf, level)\n",
    "    \n",
    "    print(f\"\\n  Strong evidence Intra-Species Variants:\")\n",
    "    print(f\"    Secondary ASVs: {len(intra_species_db):,}\")\n",
    "    \n",
    "    if intra_species_db:\n",
    "        counts = [v[1] for v in intra_species_db.values()]\n",
    "        print(f\"    Co-occurrence counts:\")\n",
    "        print(f\"      Min: {min(counts)}\")\n",
    "        print(f\"      Max: {max(counts)}\")\n",
    "        print(f\"      Avg: {np.mean(counts):.1f}\")\n",
    "        \n",
    "        # Distribution\n",
    "        count_dist = Counter(counts)\n",
    "        print(f\"\\n    Distribution:\")\n",
    "        for cnt in sorted(count_dist.keys())[:10]:\n",
    "            print(f\"      {cnt} co-occurrences: {count_dist[cnt]:,} secondary ASVs\")\n",
    "    \n",
    "    return intra_species_db\n",
    "\n",
    "# ========================\n",
    "# STEP 4: HELPER FUNCTIONS FOR CLASSIFICATION\n",
    "# ========================\n",
    "\n",
    "def get_authenticated_for_sample(sample_id, auth_db):\n",
    "    \"\"\"Get authenticated ASV info for a specific sample\"\"\"\n",
    "    for asv_id, info in auth_db.items():\n",
    "        if sample_id in info['sample_ids']:\n",
    "            return info\n",
    "    return None\n",
    "\n",
    "def check_family_match(row, auth_info):\n",
    "    \"\"\"Check if ASV family matches authenticated\"\"\"\n",
    "    if not auth_info:\n",
    "        return False\n",
    "    \n",
    "    row_family = (\n",
    "        safe_str(row.get('asv_family_v1', '')) or\n",
    "        safe_str(row.get('asv_family_v2', '')) or\n",
    "        safe_str(row.get('asv_family_v3', '')) or\n",
    "        safe_str(row.get('family', ''))\n",
    "    )\n",
    "    \n",
    "    auth_family = auth_info.get('family', '')\n",
    "    \n",
    "    if not row_family or not auth_family:\n",
    "        return False\n",
    "    \n",
    "    return row_family == auth_family\n",
    "\n",
    "# ========================\n",
    "# STEP 5: PHASE 1 - RULE-BASED CLASSIFICATION\n",
    "# ========================\n",
    "\n",
    "def classify_phase1_rule_based(row, auth_db, asv_stats, intra_species_db):\n",
    "    \"\"\"\n",
    "    Phase 1: Rule-based classification (STRONG EVIDENCE ONLY)\n",
    "    \n",
    "    Philosophy:\n",
    "    - Only classify when CONFIDENT\n",
    "    - Use Uncertain when not sure\n",
    "    - Let ML learn from confident cases\n",
    "    - ML predicts Uncertain based on sequence patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract variables\n",
    "    asv_id = safe_str(row.get('asv_id', ''))\n",
    "    sample_id = safe_str(row.get('project_readfile_id', ''))\n",
    "    reads = safe_float(row.get('reads', 0))\n",
    "    total_reads = safe_float(row.get('total_asv_reads', 1))\n",
    "    abundance = reads / total_reads if total_reads > 0 else 0\n",
    "    \n",
    "    asv_stat = asv_stats.get(asv_id, {\n",
    "        'n_samples': 1,\n",
    "        'is_singleton': True,\n",
    "        'is_widespread': False\n",
    "    })\n",
    "    \n",
    "    quality_score = safe_float(row.get('ANALYSIS_sequence_quality_score', 0))\n",
    "    seq_length = safe_int(row.get('ANALYSIS_seq_length', 417))\n",
    "    phylo_dist = safe_float(row.get('Phylogenetic_distance', 0))\n",
    "    match_status = safe_str(row.get('match', ''))\n",
    "    \n",
    "    # ================================================\n",
    "    # CLASS 0: Authenticated (100% confident)\n",
    "    # ================================================\n",
    "    \n",
    "    if safe_str(row.get('autopropose', '')) == 'select' and match_status == 'match':\n",
    "        return {\n",
    "            'class': 'Authenticated',\n",
    "            'confidence': 1.00,\n",
    "            'method': 'Rule',\n",
    "            'reason': 'autopropose=select AND match=match'\n",
    "        }\n",
    "    \n",
    "    # ================================================\n",
    "    # CLASS 1: Cross-Contamination (High confidence)\n",
    "    # ================================================\n",
    "    \n",
    "    if asv_id in auth_db:\n",
    "        if sample_id not in auth_db[asv_id]['sample_ids']:\n",
    "            # In authenticated DB but not in this sample\n",
    "            # → Cross-contamination from other samples\n",
    "            \n",
    "            if abundance < THRESHOLDS['cross_cont_clear']:\n",
    "                conf = 0.95\n",
    "                level = 'Very_Clear'\n",
    "            elif abundance < THRESHOLDS['cross_cont_likely']:\n",
    "                conf = 0.90\n",
    "                level = 'Clear'\n",
    "            elif abundance < THRESHOLDS['cross_cont_possible']:\n",
    "                conf = 0.85\n",
    "                level = 'Likely'\n",
    "            else:\n",
    "                conf = 0.75\n",
    "                level = 'Possible_Shared_Species'\n",
    "            \n",
    "            n_auth_samples = len(auth_db[asv_id]['sample_ids'])\n",
    "            \n",
    "            return {\n",
    "                'class': 'Cross_Contamination',\n",
    "                'subtype': level,\n",
    "                'confidence': conf,\n",
    "                'method': 'Rule',\n",
    "                'reason': f'Authenticated in {n_auth_samples} other sample(s), abundance={abundance:.1%}'\n",
    "            }\n",
    "    \n",
    "    # CLASS 2: Intra-Species Variant (Strong Evidence)\n",
    "    # Co-occurrence ≥2 times with same authenticated ASV\n",
    "    \n",
    "    if asv_id in intra_species_db:\n",
    "        auth_asv_id, count, conf, level = intra_species_db[asv_id]\n",
    "        \n",
    "        # ✅ เพิ่มการเช็ค taxonomy!\n",
    "        if match_status == 'match':  # ← เพิ่มบรรทัดนี้!\n",
    "            return {\n",
    "                'class': 'Intra_Species_Variant',\n",
    "                'subtype': level,\n",
    "                'confidence': conf,\n",
    "                'method': 'Rule_CoOccurrence',\n",
    "                'reason': f'Co-occurs {count} times with authenticated ASV {auth_asv_id}, match=match',\n",
    "                'co_occurrence_count': count\n",
    "            }\n",
    "        else:  # ← เพิ่มส่วนนี้!\n",
    "            # Co-occurs but match≠'match' → Environmental\n",
    "            return {\n",
    "                'class': 'Environmental_Contamination',\n",
    "                'subtype': 'Co_occurring_Different_Species',\n",
    "                'confidence': 0.80,\n",
    "                'method': 'Rule_CoOccurrence',\n",
    "                'reason': f'Co-occurs {count} times but match≠match (different species)',\n",
    "                'co_occurrence_count': count\n",
    "            }\n",
    "    \n",
    "    # ================================================\n",
    "    # CLASS 3: Failed (100% confident)\n",
    "    # ================================================\n",
    "    \n",
    "    sequence = safe_str(row.get('ANALYSIS_corrected_sequence_full', ''))\n",
    "    if not sequence or sequence == '' or reads == 0:\n",
    "        return {\n",
    "            'class': 'Failed',\n",
    "            'confidence': 1.00,\n",
    "            'method': 'Rule',\n",
    "            'reason': 'No sequence data or zero reads'\n",
    "        }\n",
    "    \n",
    "    # ================================================\n",
    "    # CLASS 4: Technical Artifacts (High confidence)\n",
    "    # ================================================\n",
    "    \n",
    "    # Extreme length\n",
    "    if seq_length < THRESHOLDS['length_min'] or seq_length > THRESHOLDS['length_max']:\n",
    "        return {\n",
    "            'class': 'Technical_Artifacts',\n",
    "            'subtype': 'Abnormal_Length',\n",
    "            'confidence': 0.85,\n",
    "            'method': 'Rule',\n",
    "            'reason': f'Extreme length: {seq_length}bp'\n",
    "        }\n",
    "    \n",
    "    # Low quality singleton\n",
    "    if reads < 10 and asv_stat['is_singleton'] and quality_score < THRESHOLDS['quality_medium']:\n",
    "        return {\n",
    "            'class': 'Technical_Artifacts',\n",
    "            'subtype': 'Low_Quality_Singleton',\n",
    "            'confidence': 0.80,\n",
    "            'method': 'Rule',\n",
    "            'reason': f'Singleton with {reads} reads and quality={quality_score:.1f}'\n",
    "        }\n",
    "    \n",
    "    # Very low quality\n",
    "    if reads < 10 and quality_score < THRESHOLDS['quality_low']:\n",
    "        return {\n",
    "            'class': 'Technical_Artifacts',\n",
    "            'subtype': 'Very_Low_Quality',\n",
    "            'confidence': 0.75,\n",
    "            'method': 'Rule',\n",
    "            'reason': f'Low reads ({reads}) with very low quality ({quality_score:.1f})'\n",
    "        }\n",
    "    \n",
    "    # ================================================\n",
    "    # CLASS 5: Environmental Contamination\n",
    "    # ONLY when CONFIDENT it's NOT Coleoptera\n",
    "    # ================================================\n",
    "    \n",
    "    auth_info = get_authenticated_for_sample(sample_id, auth_db)\n",
    "    \n",
    "    # 5A. Proposed but taxonomy mismatch\n",
    "    if safe_str(row.get('autopropose', '')) == 'select' and match_status != 'match':\n",
    "        return {\n",
    "            'class': 'Environmental_Contamination',\n",
    "            'subtype': 'Proposed_No_Match',\n",
    "            'confidence': 0.90,\n",
    "            'method': 'Rule',\n",
    "            'reason': 'Proposed as authenticated but taxonomy mismatch'\n",
    "        }\n",
    "    \n",
    "    # 5B. Non-Insecta (very confident NOT Coleoptera)\n",
    "    asv_class = safe_str(row.get('blast_tax_class', ''))\n",
    "    if asv_class and asv_class != 'Insecta' and abundance < THRESHOLDS['env_class_mismatch']:\n",
    "        return {\n",
    "            'class': 'Environmental_Contamination',\n",
    "            'subtype': 'Non_Insect',\n",
    "            'confidence': 0.90,\n",
    "            'method': 'Rule',\n",
    "            'reason': f'Non-insect class: {asv_class}, abundance={abundance:.1%}'\n",
    "        }\n",
    "    \n",
    "    # 5C. Family mismatch (confident it's different family)\n",
    "    if auth_info:\n",
    "        row_family = (\n",
    "            safe_str(row.get('asv_family_v1', '')) or\n",
    "            safe_str(row.get('asv_family_v2', '')) or\n",
    "            safe_str(row.get('asv_family_v3', '')) or\n",
    "            safe_str(row.get('family', ''))\n",
    "        )\n",
    "        auth_family = auth_info.get('family', '')\n",
    "        \n",
    "        # Only classify as Environmental if BOTH families are known AND different\n",
    "        if row_family and auth_family and row_family != auth_family:\n",
    "            # AND abundance is low (not dominant)\n",
    "            if abundance < THRESHOLDS['env_order_mismatch']:\n",
    "                return {\n",
    "                    'class': 'Environmental_Contamination',\n",
    "                    'subtype': 'Family_Mismatch',\n",
    "                    'confidence': 0.85,\n",
    "                    'method': 'Rule',\n",
    "                    'reason': f'Confident family mismatch: ASV={row_family} vs Auth={auth_family}, abundance={abundance:.1%}'\n",
    "                }\n",
    "    \n",
    "    # 5D. Order mismatch within Coleoptera (if both known)\n",
    "    if auth_info:\n",
    "        row_order = safe_str(row.get('blast_tax_order', ''))\n",
    "        auth_order = auth_info.get('order', '')\n",
    "        \n",
    "        # Only if BOTH orders are known AND different AND both are Coleoptera\n",
    "        if row_order and auth_order and \\\n",
    "           row_order == 'Coleoptera' and auth_order == 'Coleoptera':\n",
    "            # Different families within Coleoptera\n",
    "            if row_family and auth_family and row_family != auth_family:\n",
    "                if abundance < THRESHOLDS['env_order_mismatch']:\n",
    "                    return {\n",
    "                        'class': 'Environmental_Contamination',\n",
    "                        'subtype': 'Different_Coleoptera_Family',\n",
    "                        'confidence': 0.80,\n",
    "                        'method': 'Rule',\n",
    "                        'reason': f'Different Coleoptera family: ASV={row_family} vs Auth={auth_family}'\n",
    "                    }\n",
    "    \n",
    "    # 5E. Very high phylogenetic distance + low abundance\n",
    "    # (likely different species/genus)\n",
    "    if phylo_dist > THRESHOLDS['env_phylo_distance'] and \\\n",
    "       abundance < THRESHOLDS['env_phylo_threshold']:\n",
    "        return {\n",
    "            'class': 'Environmental_Contamination',\n",
    "            'subtype': 'High_Divergence',\n",
    "            'confidence': 0.80,\n",
    "            'method': 'Rule',\n",
    "            'reason': f'Very divergent: phylo_dist={phylo_dist:.3f}, abundance={abundance:.1%}'\n",
    "        }\n",
    "    \n",
    "    # ================================================\n",
    "    # CLASS 6: Uncertain\n",
    "    # Not confident enough to classify\n",
    "    # → Let ML predict based on sequence patterns\n",
    "    # ================================================\n",
    "    \n",
    "    hints = []\n",
    "    \n",
    "    # Provide hints for debugging\n",
    "    if reads < THRESHOLDS['very_low_reads']:\n",
    "        hints.append(f'very_low_reads({reads})')\n",
    "    elif reads < THRESHOLDS['low_reads']:\n",
    "        hints.append(f'low_reads({reads})')\n",
    "    \n",
    "    if abundance < THRESHOLDS['low_abundance']:\n",
    "        hints.append(f'low_abundance({abundance:.1%})')\n",
    "    elif THRESHOLDS['medium_abundance_min'] <= abundance < THRESHOLDS['medium_abundance_max']:\n",
    "        hints.append(f'medium_abundance({abundance:.1%})')\n",
    "    \n",
    "    if quality_score < THRESHOLDS['quality_medium']:\n",
    "        hints.append(f'low_quality({quality_score:.1f})')\n",
    "    \n",
    "    if asv_stat['is_singleton']:\n",
    "        hints.append('singleton')\n",
    "    elif asv_stat['is_widespread']:\n",
    "        hints.append(f'widespread({asv_stat[\"n_samples\"]}_samples)')\n",
    "    \n",
    "    if match_status == 'match':\n",
    "        hints.append('match=match')\n",
    "    elif match_status == 'mismatch':\n",
    "        hints.append('match=mismatch')\n",
    "    \n",
    "    return {\n",
    "        'class': 'Uncertain',\n",
    "        'confidence': 0.50,\n",
    "        'method': 'Rule',\n",
    "        'reason': f'No clear pattern: {\", \".join(hints) if hints else \"insufficient evidence\"}',\n",
    "        'hints': hints,\n",
    "        'note': 'Will be classified by ML based on sequence patterns'\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# STEP 6: EXTRACT ML FEATURES\n",
    "# ========================\n",
    "\n",
    "def extract_ml_features(row, auth_db, asv_stats):\n",
    "    \"\"\"Extract comprehensive features for ML\"\"\"\n",
    "    \n",
    "    asv_id = safe_str(row.get('asv_id', ''))\n",
    "    sample_id = safe_str(row.get('project_readfile_id', ''))\n",
    "    reads = safe_float(row.get('reads', 0))\n",
    "    total_reads = safe_float(row.get('total_asv_reads', 1))\n",
    "    abundance = reads / total_reads if total_reads > 0 else 0\n",
    "    \n",
    "    asv_stat = asv_stats.get(asv_id, {\n",
    "        'n_samples': 1,\n",
    "        'is_singleton': True,\n",
    "        'is_widespread': False,\n",
    "        'total_reads_all_samples': reads,\n",
    "        'avg_reads_per_sample': reads,\n",
    "        'max_reads': reads\n",
    "    })\n",
    "    \n",
    "    auth_info = get_authenticated_for_sample(sample_id, auth_db)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # GROUP 1: ABUNDANCE FEATURES\n",
    "    features.update({\n",
    "        'reads': float(reads),\n",
    "        'log_reads': float(np.log10(reads + 1)),\n",
    "        'abundance': float(abundance),\n",
    "        'log_abundance': float(np.log10(abundance + 1e-10)),\n",
    "        'percentage_reads': safe_float(row.get('percentage_reads', 0)),\n",
    "    })\n",
    "    \n",
    "    # GROUP 2: DISTRIBUTION FEATURES\n",
    "    features.update({\n",
    "        'n_samples_present': int(asv_stat['n_samples']),\n",
    "        'is_singleton': int(asv_stat['is_singleton']),\n",
    "        'is_widespread': int(asv_stat['is_widespread']),\n",
    "        'total_reads_all_samples': float(asv_stat['total_reads_all_samples']),\n",
    "        'log_total_reads_all': float(np.log10(asv_stat['total_reads_all_samples'] + 1)),\n",
    "        'avg_reads_per_sample': float(asv_stat['avg_reads_per_sample']),\n",
    "        'max_reads_in_sample': float(asv_stat['max_reads']),\n",
    "    })\n",
    "    \n",
    "    # GROUP 3: AUTHENTICATED REFERENCE\n",
    "    is_auth_elsewhere = int(asv_id in auth_db and sample_id not in auth_db[asv_id]['sample_ids'])\n",
    "    n_auth_samples = len(auth_db[asv_id]['sample_ids']) if asv_id in auth_db else 0\n",
    "    \n",
    "    features.update({\n",
    "        'is_authenticated_elsewhere': is_auth_elsewhere,\n",
    "        'n_authenticated_samples': int(n_auth_samples),\n",
    "        'has_authenticated_reference': int(auth_info is not None),\n",
    "    })\n",
    "    \n",
    "    # ================================================\n",
    "    # GROUP 4: TAXONOMY FEATURES (UPDATED!)\n",
    "    # ADD: match status as critical feature\n",
    "    # ================================================\n",
    "    \n",
    "    family_match = int(check_family_match(row, auth_info)) if auth_info else 0\n",
    "    match_status = safe_str(row.get('match', ''))\n",
    "    \n",
    "    features.update({\n",
    "        'family_match': family_match,\n",
    "        \n",
    "        # NEW: Critical taxonomy features!\n",
    "        'taxonomy_match': int(match_status == 'match'),      # 1 if match, 0 otherwise\n",
    "        'taxonomy_mismatch': int(match_status == 'no'),      # 1 if no, 0 otherwise\n",
    "        'taxonomy_unknown': int(match_status not in ['match', 'no']),  # 1 if unknown\n",
    "    })\n",
    "    \n",
    "    # GROUP 5: PHYLOGENETIC FEATURES\n",
    "    phylo_dist = safe_float(row.get('Phylogenetic_distance', 1.0))\n",
    "    \n",
    "    features.update({\n",
    "        'phylo_distance': float(phylo_dist),\n",
    "        'log_phylo_distance': float(np.log10(phylo_dist + 1e-10)),\n",
    "        'phylo_very_close': int(phylo_dist < THRESHOLDS['phylo_same_species']),\n",
    "        'phylo_close': int(phylo_dist < THRESHOLDS['phylo_same_genus']),\n",
    "        'phylo_divergent': int(phylo_dist > THRESHOLDS['phylo_divergent']),\n",
    "    })\n",
    "    \n",
    "    # GROUP 6: QUALITY FEATURES\n",
    "    tga_validated = safe_bool_to_int(row.get('ANALYSIS_tga_trp_validated', False))\n",
    "    coi_confidence = safe_str(row.get('ANALYSIS_coi_confidence', ''))\n",
    "    \n",
    "    features.update({\n",
    "        'quality_score': safe_float(row.get('ANALYSIS_sequence_quality_score', 0)),\n",
    "        'motif_score': safe_float(row.get('ANALYSIS_motif_score', 0)),\n",
    "        'tga_trp_validated': tga_validated,\n",
    "        'coi_confidence_high': int(coi_confidence == 'High'),\n",
    "        'coi_confidence_medium': int(coi_confidence == 'Medium'),\n",
    "        'coi_confidence_low': int(coi_confidence in ['Low', 'Very_Low']),\n",
    "    })\n",
    "    \n",
    "    # GROUP 7: COMPOSITION FEATURES\n",
    "    gc_content = safe_float(row.get('ANALYSIS_GC_content', 36))\n",
    "    \n",
    "    features.update({\n",
    "        'GC_content': float(gc_content),\n",
    "        'AT_content': safe_float(row.get('ANALYSIS_AT_content', 64)),\n",
    "        'GC_skew': safe_float(row.get('ANALYSIS_GC_skew', 0)),\n",
    "        'AT_skew': safe_float(row.get('ANALYSIS_AT_skew', 0)),\n",
    "        'shannon_entropy': safe_float(row.get('ANALYSIS_shannon_entropy', 0)),\n",
    "        'GC_unusual': int(gc_content > THRESHOLDS['gc_normal_max'] or gc_content < THRESHOLDS['gc_normal_min']),\n",
    "    })\n",
    "    \n",
    "    # GROUP 8: LENGTH FEATURES\n",
    "    seq_length = safe_int(row.get('ANALYSIS_seq_length', 417))\n",
    "    \n",
    "    features.update({\n",
    "        'seq_length': float(seq_length),\n",
    "        'protein_length': safe_float(row.get('ANALYSIS_protein_length', 139)),\n",
    "        'length_abnormal': int(seq_length < THRESHOLDS['length_min'] or seq_length > THRESHOLDS['length_max']),\n",
    "    })\n",
    "    \n",
    "    # GROUP 9: PROTEIN PROPERTIES\n",
    "    features.update({\n",
    "        'hydrophobic_percent': safe_float(row.get('ANALYSIS_hydrophobic_percent', 50)),\n",
    "        'leucine_percent': safe_float(row.get('ANALYSIS_leucine_percent', 15)),\n",
    "        'aromatic_percent': safe_float(row.get('ANALYSIS_aromatic_percent', 10)),\n",
    "        'polar_percent': safe_float(row.get('ANALYSIS_polar_percent', 25)),\n",
    "        'gravy_score': safe_float(row.get('ANALYSIS_gravy_score', 0)),\n",
    "    })\n",
    "    \n",
    "    # GROUP 10: MOTIF FEATURES\n",
    "    features.update({\n",
    "        'dna_motif_count': safe_int(row.get('ANALYSIS_dna_motif_count', 0)),\n",
    "        'protein_motif_count': safe_int(row.get('ANALYSIS_protein_motif_count', 0)),\n",
    "        'dna_motif_coverage': safe_float(row.get('ANALYSIS_dna_motif_coverage', 0)),\n",
    "        'protein_motif_coverage': safe_float(row.get('ANALYSIS_protein_motif_coverage', 0)),\n",
    "    })\n",
    "    \n",
    "    # GROUP 11: CODON USAGE\n",
    "    features.update({\n",
    "        'codon_diversity': safe_float(row.get('ANALYSIS_codon_diversity', 50)),\n",
    "        'total_codons': safe_int(row.get('ANALYSIS_total_codons', 139)),\n",
    "        'unique_codons': safe_int(row.get('ANALYSIS_unique_codons', 50)),\n",
    "    })\n",
    "    \n",
    "    # GROUP 12: DERIVED FEATURES\n",
    "    # NUMT signal score\n",
    "    numt_score = 0\n",
    "    if gc_content > THRESHOLDS['gc_normal_max'] or gc_content < THRESHOLDS['gc_normal_min']:\n",
    "        numt_score += 1\n",
    "    if safe_float(row.get('ANALYSIS_motif_score', 100)) < THRESHOLDS['motif_score_degraded']:\n",
    "        numt_score += 1\n",
    "    if tga_validated == 0:\n",
    "        numt_score += 1\n",
    "    if phylo_dist > THRESHOLDS['phylo_divergent']:\n",
    "        numt_score += 1\n",
    "    \n",
    "    features['numt_signal_score'] = numt_score\n",
    "    \n",
    "    # Likely patterns\n",
    "    features['likely_numt'] = int(\n",
    "        abundance < THRESHOLDS['low_abundance'] and\n",
    "        asv_stat['n_samples'] >= 2 and\n",
    "        numt_score >= 2\n",
    "    )\n",
    "    \n",
    "    features['likely_heteroplasmy'] = int(\n",
    "        THRESHOLDS['medium_abundance_min'] <= abundance < THRESHOLDS['medium_abundance_max'] and\n",
    "        safe_float(row.get('ANALYSIS_sequence_quality_score', 0)) >= THRESHOLDS['quality_high'] and\n",
    "        safe_float(row.get('ANALYSIS_motif_score', 0)) >= THRESHOLDS['quality_high'] and\n",
    "        phylo_dist < THRESHOLDS['phylo_same_genus']\n",
    "    )\n",
    "    \n",
    "    features['likely_artifact'] = int(\n",
    "        reads < THRESHOLDS['very_low_reads'] and\n",
    "        (asv_stat['is_singleton'] or safe_float(row.get('ANALYSIS_sequence_quality_score', 0)) < THRESHOLDS['quality_medium'])\n",
    "    )\n",
    "    \n",
    "    features['likely_cross_cont'] = int(is_auth_elsewhere and abundance < THRESHOLDS['cross_cont_possible'])\n",
    "    \n",
    "    features['likely_environmental'] = int(\n",
    "        family_match == 0 and\n",
    "        abundance < THRESHOLDS['env_phylo_threshold']\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "# ========================\n",
    "# STEP 7: TRAIN ML MODEL\n",
    "# ========================\n",
    "\n",
    "def train_ml_model(training_df, features_df):\n",
    "    \"\"\"\n",
    "    Train Random Forest classifier\n",
    "    NOTE: Excludes Uncertain from training\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 7: TRAINING ML MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Prepare data\n",
    "    X = features_df.copy()\n",
    "    y = training_df['phase1_class'].values\n",
    "    \n",
    "    print(f\"\\nTraining data: {len(X):,} samples (excluding Uncertain)\")\n",
    "    print(f\"Features: {len(X.columns)}\")\n",
    "    \n",
    "    # Check for NaN\n",
    "    nan_count = X.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"\\n⚠️  Found {nan_count} NaN values, filling with 0...\")\n",
    "        X = X.fillna(0)\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    print(f\"\\nClasses ({len(le.classes_)}):\")\n",
    "    class_counts = Counter(y)\n",
    "    for cls in le.classes_:\n",
    "        count = class_counts[cls]\n",
    "        print(f\"  {cls:35s}: {count:6,} ({count/len(y)*100:5.2f}%)\")\n",
    "    \n",
    "    # Remove rare classes (< 2 samples)\n",
    "    min_samples_required = 2\n",
    "    rare_classes = [cls for cls, count in class_counts.items() if count < min_samples_required]\n",
    "    \n",
    "    if rare_classes:\n",
    "        print(f\"\\n⚠️  Removing rare classes (< {min_samples_required} samples):\")\n",
    "        for cls in rare_classes:\n",
    "            print(f\"    - {cls}: {class_counts[cls]} sample(s)\")\n",
    "        \n",
    "        mask = ~pd.Series(y).isin(rare_classes)\n",
    "        X = X[mask.values].reset_index(drop=True)\n",
    "        y = y[mask.values]\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        \n",
    "        print(f\"\\n  After filtering:\")\n",
    "        print(f\"    Samples: {len(X):,}\")\n",
    "        print(f\"    Classes: {len(le.classes_)}\")\n",
    "        \n",
    "        class_counts = Counter(y)\n",
    "        for cls in le.classes_:\n",
    "            count = class_counts[cls]\n",
    "            print(f\"    {cls:35s}: {count:6,} ({count/len(y)*100:5.2f}%)\")\n",
    "    \n",
    "    if len(le.classes_) < 2:\n",
    "        print(\"\\n❌ ERROR: Need at least 2 classes for ML training\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Apply SMOTE if needed\n",
    "    if SMOTE_AVAILABLE:\n",
    "        min_class_count = min(class_counts.values())\n",
    "        if min_class_count < 100:\n",
    "            print(f\"\\n  Applying SMOTE (min class: {min_class_count})...\")\n",
    "            try:\n",
    "                k_neighbors = min(5, min_class_count - 1)\n",
    "                if k_neighbors > 0:\n",
    "                    smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "                    X_balanced, y_balanced = smote.fit_resample(X, y_encoded)\n",
    "                    print(f\"  After SMOTE: {len(X_balanced):,} samples\")\n",
    "                else:\n",
    "                    X_balanced, y_balanced = X, y_encoded\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️  SMOTE failed: {e}\")\n",
    "                X_balanced, y_balanced = X, y_encoded\n",
    "        else:\n",
    "            X_balanced, y_balanced = X, y_encoded\n",
    "    else:\n",
    "        X_balanced, y_balanced = X, y_encoded\n",
    "    \n",
    "    # Split\n",
    "    min_class_in_balanced = min(Counter(y_balanced).values())\n",
    "    use_stratify = min_class_in_balanced >= 2\n",
    "    \n",
    "    if use_stratify:\n",
    "        print(f\"\\nSplitting with stratification...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Classes too small for stratification, splitting randomly...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_balanced, y_balanced, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nSplit:\")\n",
    "    print(f\"  Train: {len(X_train):,}\")\n",
    "    print(f\"  Test:  {len(X_test):,}\")\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Class weights\n",
    "    classes = np.unique(y_balanced)\n",
    "    weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_balanced)\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    \n",
    "    print(f\"\\nClass weights:\")\n",
    "    for cls_idx, weight in class_weight_dict.items():\n",
    "        cls_name = le.inverse_transform([cls_idx])[0]\n",
    "        print(f\"  {cls_name:35s}: {weight:.3f}\")\n",
    "    \n",
    "    # Train Random Forest\n",
    "    print(f\"\\nTraining Random Forest...\")\n",
    "    \n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features='sqrt',\n",
    "        class_weight=class_weight_dict,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    print(f\"  ✓ Training complete\")\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n✓ Test Accuracy: {test_acc:.3f}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    if len(X_train) >= 50:\n",
    "        print(f\"\\nCross-validation (5-fold)...\")\n",
    "        try:\n",
    "            cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=min(5, len(X_train)//10), n_jobs=-1)\n",
    "            print(f\"  ✓ CV Accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️  Cross-validation failed: {e}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    target_names = le.inverse_transform(np.unique(y_test))\n",
    "    report = classification_report(y_test, y_pred, target_names=target_names, zero_division=0)\n",
    "    print(report)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=le.inverse_transform(np.unique(y_test)),\n",
    "        columns=le.inverse_transform(np.unique(y_test))\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm_df)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': clf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 20 Important Features:\")\n",
    "    for idx, row in feature_importance.head(20).iterrows():\n",
    "        print(f\"  {row['feature']:35s}: {row['importance']:.5f}\")\n",
    "    \n",
    "    return clf, scaler, le, feature_importance\n",
    "\n",
    "# ========================\n",
    "# STEP 8: ML PREDICTION & FINAL DECISION\n",
    "# ========================\n",
    "\n",
    "def validate_with_ml(df, clf, scaler, le, features_df):\n",
    "    \"\"\"Use ML to predict all samples (including Uncertain)\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 8: ML PREDICTION & FINAL CLASSIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaled = scaler.transform(features_df)\n",
    "    \n",
    "    # Predict\n",
    "    print(\"\\nPredicting all biological samples...\")\n",
    "    ml_predictions = clf.predict(X_scaled)\n",
    "    ml_probabilities = clf.predict_proba(X_scaled)\n",
    "    \n",
    "    # Decode\n",
    "    ml_classes = le.inverse_transform(ml_predictions)\n",
    "    ml_confidences = ml_probabilities.max(axis=1)\n",
    "    \n",
    "    # Add to dataframe\n",
    "    df['ml_prediction'] = ml_classes\n",
    "    df['ml_confidence'] = ml_confidences\n",
    "    \n",
    "    # Add probability for each class\n",
    "    for i, cls_name in enumerate(le.classes_):\n",
    "        df[f'ml_prob_{cls_name}'] = ml_probabilities[:, i]\n",
    "    \n",
    "    print(f\"  ✓ Predictions complete\")\n",
    "    \n",
    "    # Agreement analysis\n",
    "    print(f\"\\nAGREEMENT ANALYSIS:\")\n",
    "    \n",
    "    agreement = (df['phase1_class'] == df['ml_prediction'])\n",
    "    agreement_rate = agreement.sum() / len(df) * 100\n",
    "    print(f\"  Overall agreement: {agreement_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n  Agreement by Phase 1 confidence:\")\n",
    "    for threshold, label in [(0.90, 'High (≥0.90)'), (0.70, 'Medium (0.70-0.89)'), (0, 'Low (<0.70)')]:\n",
    "        if threshold == 0.90:\n",
    "            mask = df['phase1_confidence'] >= 0.90\n",
    "        elif threshold == 0.70:\n",
    "            mask = (df['phase1_confidence'] >= 0.70) & (df['phase1_confidence'] < 0.90)\n",
    "        else:\n",
    "            mask = df['phase1_confidence'] < 0.70\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            agree = (df[mask]['phase1_class'] == df[mask]['ml_prediction']).sum()\n",
    "            agree_rate = agree / mask.sum() * 100\n",
    "            print(f\"    {label:20s}: {agree_rate:5.1f}% ({mask.sum():,} cases)\")\n",
    "    \n",
    "    # Disagreements\n",
    "    print(f\"\\nDISAGREEMENTS (Phase 1 ≠ ML):\")\n",
    "    disagree = df[~agreement].copy()\n",
    "    \n",
    "    if len(disagree) > 0:\n",
    "        print(f\"  Total: {len(disagree):,} ({len(disagree)/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        disagree_patterns = disagree.groupby(['phase1_class', 'ml_prediction']).size().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\n  Top 10 disagreement patterns:\")\n",
    "        for (rule_cls, ml_cls), count in disagree_patterns.head(10).items():\n",
    "            pct = count / len(disagree) * 100\n",
    "            print(f\"    Phase1: {rule_cls:25s} → ML: {ml_cls:25s} : {count:5,} ({pct:4.1f}%)\")\n",
    "    \n",
    "    # Final classification decision\n",
    "    print(f\"\\nFINAL CLASSIFICATION DECISION:\")\n",
    "    \n",
    "    final_classes = []\n",
    "    final_confidences = []\n",
    "    decision_methods = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        rule_class = row['phase1_class']\n",
    "        rule_conf = row['phase1_confidence']\n",
    "        ml_class = row['ml_prediction']\n",
    "        ml_conf = row['ml_confidence']\n",
    "        \n",
    "        # Decision logic\n",
    "        if rule_class == ml_class:\n",
    "            # Agreement\n",
    "            if rule_conf >= ml_conf:\n",
    "                final_class = rule_class\n",
    "                final_conf = min(1.0, rule_conf * 1.05)\n",
    "                method = 'Rule_Confirmed_by_ML'\n",
    "            else:\n",
    "                final_class = ml_class\n",
    "                final_conf = min(1.0, ml_conf * 1.05)\n",
    "                method = 'ML_Confirmed_by_Rule'\n",
    "        \n",
    "        elif rule_conf >= 0.90:\n",
    "            # High confidence rule\n",
    "            if ml_conf >= 0.80:\n",
    "                # Both confident but disagree\n",
    "                final_class = rule_class\n",
    "                final_conf = 0.70\n",
    "                method = 'Rule_High_ML_Disagrees_REVIEW'\n",
    "            else:\n",
    "                # Rule confident, ML uncertain\n",
    "                final_class = rule_class\n",
    "                final_conf = rule_conf\n",
    "                method = 'Rule_High_ML_Uncertain'\n",
    "        \n",
    "        elif ml_conf >= 0.80:\n",
    "            # ML very confident\n",
    "            final_class = ml_class\n",
    "            final_conf = ml_conf\n",
    "            method = 'ML_High_Rule_Lower'\n",
    "        \n",
    "        elif rule_conf >= 0.70 and ml_conf >= 0.70:\n",
    "            # Both moderately confident but disagree\n",
    "            if ml_conf > rule_conf:\n",
    "                final_class = ml_class\n",
    "                final_conf = ml_conf * 0.95\n",
    "                method = 'ML_Weighted'\n",
    "            else:\n",
    "                final_class = rule_class\n",
    "                final_conf = rule_conf * 0.95\n",
    "                method = 'Rule_Weighted'\n",
    "        \n",
    "        else:\n",
    "            # Both uncertain → use ML\n",
    "            final_class = ml_class\n",
    "            final_conf = ml_conf\n",
    "            method = 'ML_Both_Uncertain'\n",
    "        \n",
    "        final_classes.append(final_class)\n",
    "        final_confidences.append(final_conf)\n",
    "        decision_methods.append(method)\n",
    "    \n",
    "    df['final_classification'] = final_classes\n",
    "    df['final_confidence'] = final_confidences\n",
    "    df['decision_method'] = decision_methods\n",
    "    \n",
    "    # Flag for review\n",
    "    df['needs_review'] = [\n",
    "        'REVIEW' in method or conf < 0.60\n",
    "        for method, conf in zip(decision_methods, final_confidences)\n",
    "    ]\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n  Decision method distribution:\")\n",
    "    method_counts = pd.Series(decision_methods).value_counts()\n",
    "    for method, count in method_counts.items():\n",
    "        print(f\"    {method:35s}: {count:6,} ({count/len(df)*100:5.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ========================\n",
    "# STEP 9: POST-VALIDATION FIX\n",
    "# ========================\n",
    "\n",
    "def post_validation_fix(df, auth_db):\n",
    "    \"\"\"Fix validation issues after classification\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 9: POST-VALIDATION FIXES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_fixed = 0\n",
    "    \n",
    "    # ================================================\n",
    "    # FIX 1: Cross-Contamination must be in auth_db\n",
    "    # ================================================\n",
    "    \n",
    "    bad_cross = (df['final_classification'] == 'Cross_Contamination') & \\\n",
    "                (~df['asv_id'].isin(auth_db.keys()))\n",
    "    \n",
    "    if bad_cross.sum() > 0:\n",
    "        df.loc[bad_cross, 'final_classification'] = 'Environmental_Contamination'\n",
    "        df.loc[bad_cross, 'final_confidence'] *= 0.9\n",
    "        df.loc[bad_cross, 'decision_method'] += '_AUTO_CORRECTED'\n",
    "        n_fixed += bad_cross.sum()\n",
    "        print(f\"  ✓ Fixed {bad_cross.sum():,} Cross-Contamination → Environmental\")\n",
    "    \n",
    "    # ================================================\n",
    "    # FIX 2: Flag suspicious Authenticated (ML-predicted)\n",
    "    # ================================================\n",
    "    \n",
    "    suspicious_auth = (df['final_classification'] == 'Authenticated') & \\\n",
    "                    ((df['autopropose'] != 'select') | \n",
    "                    (df['match'] != 'match')) & \\\n",
    "                    (df['final_confidence'] < 0.95)\n",
    "    \n",
    "    if suspicious_auth.sum() > 0:\n",
    "        df.loc[suspicious_auth, 'needs_review'] = True\n",
    "        print(f\"  ✓ Flagged {suspicious_auth.sum():,} suspicious Authenticated for review\")\n",
    "    \n",
    "    # ================================================\n",
    "    # FIX 3: Enforce 1 Authenticated per sample\n",
    "    # CRITICAL: Multiple Authenticated in same sample\n",
    "    # ================================================\n",
    "    \n",
    "    print(f\"\\n  Checking 1-Authenticated-per-sample rule...\")\n",
    "    \n",
    "    violations = []\n",
    "    samples_to_fix = []\n",
    "    \n",
    "    for sample_id in df['project_readfile_id'].unique():\n",
    "        sample_df = df[df['project_readfile_id'] == sample_id]\n",
    "        \n",
    "        # Count Authenticated in this sample\n",
    "        auth_mask = sample_df['final_classification'] == 'Authenticated'\n",
    "        n_auth = auth_mask.sum()\n",
    "        \n",
    "        if n_auth > 1:\n",
    "            violations.append(sample_id)\n",
    "            \n",
    "            # Get all Authenticated ASVs\n",
    "            auth_asvs = sample_df[auth_mask].copy()\n",
    "            \n",
    "            # Priority: Keep Phase 1 Authenticated (autopropose='select')\n",
    "            phase1_auth = auth_asvs[\n",
    "                (auth_asvs['autopropose'] == 'select') &\n",
    "                (auth_asvs['match'] == 'match')\n",
    "            ]\n",
    "            \n",
    "            if len(phase1_auth) > 0:\n",
    "                # Keep Phase 1 Authenticated (highest priority)\n",
    "                keep_idx = phase1_auth['reads'].idxmax()  # Keep highest reads\n",
    "                \n",
    "                # Reclassify others → Intra_Species_Variant\n",
    "                for idx, row in auth_asvs.iterrows():\n",
    "                    if idx != keep_idx:\n",
    "                        samples_to_fix.append({\n",
    "                            'index': idx,\n",
    "                            'sample_id': sample_id,\n",
    "                            'asv_id': row['asv_id'],\n",
    "                            'reason': 'Multiple Authenticated - keeping Phase 1 with highest reads'\n",
    "                        })\n",
    "            else:\n",
    "                # No Phase 1 Authenticated → Keep highest reads\n",
    "                keep_idx = auth_asvs['reads'].idxmax()\n",
    "                \n",
    "                for idx, row in auth_asvs.iterrows():\n",
    "                    if idx != keep_idx:\n",
    "                        samples_to_fix.append({\n",
    "                            'index': idx,\n",
    "                            'sample_id': sample_id,\n",
    "                            'asv_id': row['asv_id'],\n",
    "                            'reason': 'Multiple Authenticated - keeping highest reads'\n",
    "                        })\n",
    "    \n",
    "    if len(violations) > 0:\n",
    "        print(f\"  ⚠️  Found {len(violations):,} samples with multiple Authenticated\")\n",
    "        print(f\"  → Need to fix {len(samples_to_fix):,} ASVs\")\n",
    "        \n",
    "        # Apply fixes\n",
    "        for fix in samples_to_fix:\n",
    "            idx = fix['index']\n",
    "            \n",
    "            df.loc[idx, 'final_classification'] = 'Intra_Species_Variant'\n",
    "            df.loc[idx, 'final_confidence'] *= 0.85  # Lower confidence\n",
    "            df.loc[idx, 'decision_method'] += '_AUTO_CORRECTED_MultiAuth'\n",
    "            df.loc[idx, 'needs_review'] = True\n",
    "            \n",
    "            n_fixed += 1\n",
    "        \n",
    "        print(f\"  ✓ Fixed {len(samples_to_fix):,} cases:\")\n",
    "        print(f\"    - Reclassified as Intra_Species_Variant\")\n",
    "        print(f\"    - Kept one Authenticated per sample (Phase 1 priority)\")\n",
    "        print(f\"    - Flagged for review\")\n",
    "    else:\n",
    "        print(f\"  ✓ All samples have ≤1 Authenticated\")\n",
    "    \n",
    "    # ================================================\n",
    "    # FINAL VALIDATION: Re-check rule\n",
    "    # ================================================\n",
    "    \n",
    "    print(f\"\\n  Final validation:\")\n",
    "    \n",
    "    violation_count = 0\n",
    "    for sample_id in df['project_readfile_id'].unique():\n",
    "        sample_df = df[df['project_readfile_id'] == sample_id]\n",
    "        n_auth = (sample_df['final_classification'] == 'Authenticated').sum()\n",
    "        \n",
    "        if n_auth > 1:\n",
    "            violation_count += 1\n",
    "    \n",
    "    if violation_count == 0:\n",
    "        print(f\"  ✓ All samples have ≤1 Authenticated\")\n",
    "    else:\n",
    "        print(f\"  ❌ ERROR: Still have {violation_count} violations!\")\n",
    "    \n",
    "    # ================================================\n",
    "    # FIX 4: Taxonomy Consistency ONLY (No Phylo Loop)\n",
    "    # Taxonomy = Ground Truth, Phylo = Flag only\n",
    "    # ================================================\n",
    "    \n",
    "    print(f\"\\n  Checking taxonomy consistency...\")\n",
    "    \n",
    "    if 'match' not in df.columns:\n",
    "        print(f\"  ⚠️  WARNING: 'match' column not found\")\n",
    "    else:\n",
    "        is_intra = df['final_classification'] == 'Intra_Species_Variant'\n",
    "        is_env = df['final_classification'] == 'Environmental_Contamination'\n",
    "        match = df['match']\n",
    "        \n",
    "        # ================================================\n",
    "        # SIMPLE FIX: Taxonomy only (ONE PASS)\n",
    "        # ================================================\n",
    "        \n",
    "        # Fix 1: Intra-Species MUST have match='match'\n",
    "        intra_bad = is_intra & (match != 'match')\n",
    "        n_intra_bad = intra_bad.sum()\n",
    "        \n",
    "        if n_intra_bad > 0:\n",
    "            print(f\"  ⚠️  Reclassifying {n_intra_bad:,} Intra-Species → Environmental\")\n",
    "            print(f\"      Reason: match≠'match' (different species)\")\n",
    "            \n",
    "            df.loc[intra_bad, 'final_classification'] = 'Environmental_Contamination'\n",
    "            df.loc[intra_bad, 'final_confidence'] *= 0.80\n",
    "            df.loc[intra_bad, 'decision_method'] += '_TAX_CORRECTED'\n",
    "            df.loc[intra_bad, 'needs_review'] = True\n",
    "            n_fixed += n_intra_bad\n",
    "            \n",
    "            print(f\"  ✓ Fixed {n_intra_bad:,}\")\n",
    "        else:\n",
    "            print(f\"  ✓ All Intra-Species have match='match'\")\n",
    "        \n",
    "        # Fix 2: Environmental with match='match' → flag only (don't reclassify)\n",
    "        env_match = is_env & (match == 'match')\n",
    "        n_env_match = env_match.sum()\n",
    "        \n",
    "        if n_env_match > 0:\n",
    "            print(f\"  ⚠️  Found {n_env_match:,} Environmental with match='match'\")\n",
    "            print(f\"      → Keeping as Environmental but flagging for review\")\n",
    "            print(f\"      (These are same species but distant populations/contamination)\")\n",
    "            \n",
    "            df.loc[env_match, 'needs_review'] = True\n",
    "            df.loc[env_match, 'final_confidence'] *= 0.85\n",
    "            \n",
    "            print(f\"  ✓ Flagged {n_env_match:,} for review\")\n",
    "        \n",
    "        # ================================================\n",
    "        # VERIFICATION\n",
    "        # ================================================\n",
    "        \n",
    "        print(f\"\\n  Taxonomy verification:\")\n",
    "        \n",
    "        final_intra = df['final_classification'] == 'Intra_Species_Variant'\n",
    "        final_env = df['final_classification'] == 'Environmental_Contamination'\n",
    "        \n",
    "        intra_ok = (final_intra & (match == 'match')).sum()\n",
    "        intra_total = final_intra.sum()\n",
    "        \n",
    "        env_no = (final_env & (match != 'match')).sum()\n",
    "        env_yes = (final_env & (match == 'match')).sum()\n",
    "        env_total = final_env.sum()\n",
    "        \n",
    "        if intra_total > 0:\n",
    "            if intra_ok == intra_total:\n",
    "                print(f\"  ✅ Intra-Species ({intra_total:,}): 100% match='match'\")\n",
    "            else:\n",
    "                print(f\"  ❌ Intra-Species: {intra_total - intra_ok:,} still with match≠'match'\")\n",
    "        \n",
    "        if env_total > 0:\n",
    "            print(f\"  ℹ️  Environmental ({env_total:,}):\")\n",
    "            print(f\"      - match≠'match': {env_no:,} ({env_no/env_total*100:.1f}%)\")\n",
    "            print(f\"      - match='match': {env_yes:,} ({env_yes/env_total*100:.1f}%) ← flagged\")\n",
    "        \n",
    "        # ================================================\n",
    "        # PHYLO ANALYSIS (informational only)\n",
    "        # ================================================\n",
    "        \n",
    "        if 'Phylogenetic_distance' in df.columns:\n",
    "            print(f\"\\n  Phylogenetic distance analysis (informational):\")\n",
    "            \n",
    "            phylo = df['Phylogenetic_distance']\n",
    "            \n",
    "            if intra_total > 0:\n",
    "                intra_close = (final_intra & (phylo < 0.05)).sum()\n",
    "                intra_mod = (final_intra & (phylo >= 0.05) & (phylo < 0.15)).sum()\n",
    "                intra_dist = (final_intra & (phylo >= 0.15)).sum()\n",
    "                \n",
    "                print(f\"  Intra-Species ({intra_total:,}):\")\n",
    "                print(f\"    <0.05: {intra_close:,} ({intra_close/intra_total*100:.1f}%)\")\n",
    "                print(f\"    0.05-0.15: {intra_mod:,} ({intra_mod/intra_total*100:.1f}%)\")\n",
    "                print(f\"    ≥0.15: {intra_dist:,} ({intra_dist/intra_total*100:.1f}%)\")\n",
    "                \n",
    "                if intra_dist > 0:\n",
    "                    print(f\"    Note: {intra_dist:,} distant but same species (kept as Intra)\")\n",
    "            \n",
    "            if env_total > 0:\n",
    "                env_close = (final_env & (phylo < 0.05)).sum()\n",
    "                env_mod = (final_env & (phylo >= 0.05) & (phylo < 0.15)).sum()\n",
    "                env_dist = (final_env & (phylo >= 0.15)).sum()\n",
    "                \n",
    "                print(f\"  Environmental ({env_total:,}):\")\n",
    "                print(f\"    <0.05: {env_close:,} ({env_close/env_total*100:.1f}%)\")\n",
    "                print(f\"    0.05-0.15: {env_mod:,} ({env_mod/env_total*100:.1f}%)\")\n",
    "                print(f\"    ≥0.15: {env_dist:,} ({env_dist/env_total*100:.1f}%)\")\n",
    "                \n",
    "                if env_close > 0:\n",
    "                    print(f\"    Note: {env_close:,} close but different species (kept as Env)\")\n",
    "    \n",
    "    # ================================================\n",
    "    # SUMMARY\n",
    "    # ================================================\n",
    "    \n",
    "    if n_fixed == 0 and len(violations) == 0 and suspicious_auth.sum() == 0:\n",
    "        print(f\"\\n  ✓ No fixes needed\")\n",
    "    else:\n",
    "        print(f\"\\n  ✓ Total fixes applied: {n_fixed:,}\")\n",
    "    \n",
    "    return df  # ✅ CRITICAL!\n",
    "\n",
    "# ========================\n",
    "# STEP 10: CREATE SEQUENCE SUMMARY (NO MIXED)\n",
    "# ========================\n",
    "\n",
    "def create_sequence_summary_no_mixed(df):\n",
    "    \"\"\"\n",
    "    Create sequence summary without Mixed category\n",
    "    Priority: Authenticated > Intra_Species > Environmental > Technical\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 10: CREATING SEQUENCE-LEVEL SUMMARY (No Mixed)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    asv_groups = df.groupby('asv_id')\n",
    "    \n",
    "    iterator = asv_groups\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(asv_groups, desc=\"  Processing ASVs\")\n",
    "    \n",
    "    for asv_id, group in iterator:\n",
    "        # Count by classification\n",
    "        class_counts = group['final_classification'].value_counts()\n",
    "        total_occurrences = len(group)\n",
    "        \n",
    "        # Priority decision (NO MIXED)\n",
    "        if 'Authenticated' in class_counts.index:\n",
    "            overall_class = 'Authenticated'\n",
    "            n_auth = class_counts['Authenticated']\n",
    "            note = f'Authenticated in {n_auth}/{total_occurrences} samples'\n",
    "            \n",
    "            # Check for cross-contamination\n",
    "            if 'Cross_Contamination' in class_counts.index:\n",
    "                n_cross = class_counts['Cross_Contamination']\n",
    "                note += f', Cross-contamination in {n_cross} samples'\n",
    "        \n",
    "        elif 'Cross_Contamination' in class_counts.index:\n",
    "            # Cross-Cont only → was authenticated elsewhere\n",
    "            overall_class = 'Authenticated'\n",
    "            n_cross = class_counts['Cross_Contamination']\n",
    "            note = f'Cross-contamination only ({n_cross} samples) - authenticated elsewhere'\n",
    "        \n",
    "        elif 'Intra_Species_Variant' in class_counts.index:\n",
    "            overall_class = 'Intra_Species_Variant'\n",
    "            n_intra = class_counts['Intra_Species_Variant']\n",
    "            note = f'Intra-Species Variant in {n_intra}/{total_occurrences} samples'\n",
    "        \n",
    "        elif 'Environmental_Contamination' in class_counts.index:\n",
    "            overall_class = 'Environmental_Contamination'\n",
    "            n_env = class_counts['Environmental_Contamination']\n",
    "            note = f'Environmental in {n_env}/{total_occurrences} samples'\n",
    "        \n",
    "        elif 'Technical_Artifacts' in class_counts.index:\n",
    "            overall_class = 'Technical_Artifacts'\n",
    "            note = f'Technical artifacts only'\n",
    "        \n",
    "        elif 'Failed' in class_counts.index:\n",
    "            overall_class = 'Failed'\n",
    "            note = f'Failed'\n",
    "        \n",
    "        else:\n",
    "            # Should not happen\n",
    "            overall_class = 'Unknown'\n",
    "            note = 'ERROR: Not classified'\n",
    "        \n",
    "        # Statistics\n",
    "        total_reads = group['reads'].sum() if 'reads' in group.columns else 0\n",
    "        avg_reads = group['reads'].mean() if 'reads' in group.columns else 0\n",
    "        max_reads = group['reads'].max() if 'reads' in group.columns else 0\n",
    "        avg_confidence = group['final_confidence'].mean()\n",
    "        \n",
    "        # Authenticated samples count\n",
    "        authenticated_samples = (group['final_classification'] == 'Authenticated').sum()\n",
    "        cross_cont_samples = (group['final_classification'] == 'Cross_Contamination').sum()\n",
    "        intra_species_samples = (group['final_classification'] == 'Intra_Species_Variant').sum()\n",
    "        \n",
    "        # Get best quality sample\n",
    "        if 'ANALYSIS_sequence_quality_score' in group.columns:\n",
    "            quality_scores = group['ANALYSIS_sequence_quality_score'].fillna(0)\n",
    "            best_idx = quality_scores.idxmax()\n",
    "        else:\n",
    "            best_idx = group.index[0]\n",
    "        \n",
    "        best_row = group.loc[best_idx]\n",
    "        \n",
    "        summary_data.append({\n",
    "            'asv_id': asv_id,\n",
    "            \n",
    "            # Overall classification (NO MIXED)\n",
    "            'overall_classification': overall_class,\n",
    "            'classification_note': note,\n",
    "            \n",
    "            # Distribution\n",
    "            'total_samples': total_occurrences,\n",
    "            'authenticated_samples': authenticated_samples,\n",
    "            'cross_contamination_samples': cross_cont_samples,\n",
    "            'intra_species_variant_samples': intra_species_samples,\n",
    "            \n",
    "            # Per-sample breakdown\n",
    "            'classification_distribution': str(dict(class_counts)),\n",
    "            \n",
    "            # Abundance\n",
    "            'total_reads_all_samples': total_reads,\n",
    "            'avg_reads_per_sample': avg_reads,\n",
    "            'max_reads_in_sample': max_reads,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            \n",
    "            # Taxonomy (from best sample)\n",
    "            'family': safe_str(best_row.get('family', '')),\n",
    "            'subfamily': safe_str(best_row.get('subfamily', '')),\n",
    "            'blast_tax_family': safe_str(best_row.get('blast_tax_family', '')),\n",
    "            'blast_tax_order': safe_str(best_row.get('blast_tax_order', '')),\n",
    "            'blast_tax_class': safe_str(best_row.get('blast_tax_class', '')),\n",
    "            \n",
    "            # Quality (from best sample)\n",
    "            'best_quality_score': safe_float(best_row.get('ANALYSIS_sequence_quality_score', 0)),\n",
    "            'best_motif_score': safe_float(best_row.get('ANALYSIS_motif_score', 0)),\n",
    "            'best_coi_confidence': safe_str(best_row.get('ANALYSIS_coi_confidence', '')),\n",
    "            \n",
    "            # Sequence info\n",
    "            'sequence_length': safe_int(best_row.get('ANALYSIS_seq_length', 0)),\n",
    "            'protein_length': safe_int(best_row.get('ANALYSIS_protein_length', 0)),\n",
    "            'GC_content': safe_float(best_row.get('ANALYSIS_GC_content', 0)),\n",
    "            'phylo_distance': safe_float(best_row.get('Phylogenetic_distance', 0)),\n",
    "            \n",
    "            # Sequence\n",
    "            'sequence': safe_str(best_row.get('ANALYSIS_corrected_sequence_full', '')),\n",
    "            'protein': safe_str(best_row.get('ANALYSIS_corrected_protein_full', ''))\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(f\"\\n  ✓ Created summary for {len(summary_df):,} unique ASVs\")\n",
    "    \n",
    "    # Check: No Mixed\n",
    "    if 'Mixed' in summary_df['overall_classification'].values:\n",
    "        print(f\"  ⚠️  WARNING: Found Mixed in summary!\")\n",
    "    else:\n",
    "        print(f\"  ✓ Confirmed: No Mixed category\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# ========================\n",
    "# STEP 11: VALIDATION & STATISTICS\n",
    "# ========================\n",
    "\n",
    "def validate_and_create_statistics(df, summary_df, auth_db):\n",
    "    \"\"\"Validate results and create statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 11: VALIDATION & STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # VALIDATION CHECKS\n",
    "    print(\"\\n1. VALIDATION CHECKS:\")\n",
    "    \n",
    "    # Check 1: Authenticated validation\n",
    "    auth = df[df['final_classification'] == 'Authenticated']\n",
    "    \n",
    "    if 'autopropose' in df.columns and 'match' in df.columns:\n",
    "        valid_auth = auth[\n",
    "            (auth['autopropose'] == 'select') &\n",
    "            (auth['match'] == 'match')\n",
    "        ]\n",
    "        \n",
    "        if len(valid_auth) == len(auth):\n",
    "            print(f\"  ✓ All {len(auth):,} Authenticated have autopropose='select' AND match='match'\")\n",
    "        else:\n",
    "            invalid_count = len(auth) - len(valid_auth)\n",
    "            issues.append(f\"❌ {invalid_count} Authenticated without proper criteria\")\n",
    "    \n",
    "    # Check 2: Cross-contamination validation\n",
    "    cross_cont = df[df['final_classification'] == 'Cross_Contamination']\n",
    "    \n",
    "    if 'asv_id' in df.columns:\n",
    "        not_in_db = cross_cont[~cross_cont['asv_id'].isin(auth_db.keys())]\n",
    "        \n",
    "        if len(not_in_db) == 0:\n",
    "            print(f\"  ✓ All {len(cross_cont):,} Cross-Contamination in authenticated database\")\n",
    "        else:\n",
    "            issues.append(f\"❌ {len(not_in_db)} Cross-Contamination not in authenticated database\")\n",
    "    \n",
    "    # Check 3: Technical Artifacts validation\n",
    "    tech_art = df[df['final_classification'] == 'Technical_Artifacts']\n",
    "    \n",
    "    if 'reads' in df.columns and len(tech_art) > 0:\n",
    "        tech_low_reads = (tech_art['reads'] < THRESHOLDS['technical_artifacts_reads']).sum()\n",
    "        if tech_low_reads == len(tech_art):\n",
    "            print(f\"  ✓ All {len(tech_art):,} Technical_Artifacts have reads < {THRESHOLDS['technical_artifacts_reads']}\")\n",
    "        else:\n",
    "            other_tech = len(tech_art) - tech_low_reads\n",
    "            print(f\"  ✓ Technical_Artifacts: {tech_low_reads:,} with reads <4, {other_tech:,} other types\")\n",
    "    \n",
    "    # Check 4: No Uncertain in final\n",
    "    uncertain_count = (df['final_classification'] == 'Uncertain').sum()\n",
    "    if uncertain_count == 0:\n",
    "        print(f\"  ✓ No Uncertain in final classification (all classified by ML)\")\n",
    "    else:\n",
    "        issues.append(f\"⚠️  Still have {uncertain_count} Uncertain cases\")\n",
    "    \n",
    "    # Check 5: Phylogenetic + Taxonomy consistency (UPDATED!)\n",
    "    if 'match' in df.columns and 'Phylogenetic_distance' in df.columns:\n",
    "        # Check 5A: Intra-Species validation\n",
    "        intra = df[df['final_classification'] == 'Intra_Species_Variant']\n",
    "        \n",
    "        if len(intra) > 0:\n",
    "            # All should have match='match'\n",
    "            intra_tax_bad = (intra['match'] != 'match').sum()\n",
    "            \n",
    "            # Check phylo distance\n",
    "            intra_phylo_bad = (intra['Phylogenetic_distance'] >= THRESHOLDS['phylo_intra_max']).sum()\n",
    "            \n",
    "            if intra_tax_bad == 0 and intra_phylo_bad == 0:\n",
    "                print(f\"  ✓ All {len(intra):,} Intra-Species valid:\")\n",
    "                print(f\"      - match='match': 100%\")\n",
    "                print(f\"      - phylo < {THRESHOLDS['phylo_intra_max']}: 100%\")\n",
    "            else:\n",
    "                if intra_tax_bad > 0:\n",
    "                    issues.append(f\"❌ {intra_tax_bad} Intra-Species with match≠'match'\")\n",
    "                if intra_phylo_bad > 0:\n",
    "                    issues.append(f\"❌ {intra_phylo_bad} Intra-Species with phylo ≥ {THRESHOLDS['phylo_intra_max']}\")\n",
    "            \n",
    "            # Show phylo distribution\n",
    "            print(f\"      Phylo distance distribution:\")\n",
    "            very_close = (intra['Phylogenetic_distance'] < 0.05).sum()\n",
    "            moderate = ((intra['Phylogenetic_distance'] >= 0.05) & \n",
    "                       (intra['Phylogenetic_distance'] < THRESHOLDS['phylo_intra_max'])).sum()\n",
    "            print(f\"        <0.05: {very_close:,} ({very_close/len(intra)*100:.1f}%)\")\n",
    "            print(f\"        0.05-{THRESHOLDS['phylo_intra_max']}: {moderate:,} ({moderate/len(intra)*100:.1f}%)\")\n",
    "        \n",
    "        # Check 5B: Environmental validation\n",
    "        env = df[df['final_classification'] == 'Environmental_Contamination']\n",
    "        \n",
    "        if len(env) > 0:\n",
    "            # All should have match≠'match'\n",
    "            env_tax_bad = (env['match'] == 'match').sum()\n",
    "            \n",
    "            # Check phylo distance\n",
    "            env_phylo_bad = (env['Phylogenetic_distance'] < THRESHOLDS['phylo_env_min']).sum()\n",
    "            \n",
    "            if env_tax_bad == 0 and env_phylo_bad == 0:\n",
    "                print(f\"  ✓ All {len(env):,} Environmental valid:\")\n",
    "                print(f\"      - match≠'match': 100%\")\n",
    "                print(f\"      - phylo ≥ {THRESHOLDS['phylo_env_min']}: 100%\")\n",
    "            else:\n",
    "                if env_tax_bad > 0:\n",
    "                    issues.append(f\"❌ {env_tax_bad} Environmental with match='match'\")\n",
    "                if env_phylo_bad > 0:\n",
    "                    issues.append(f\"❌ {env_phylo_bad} Environmental with phylo < {THRESHOLDS['phylo_env_min']}\")\n",
    "            \n",
    "            # Show phylo distribution\n",
    "            print(f\"      Phylo distance distribution:\")\n",
    "            close = (env['Phylogenetic_distance'] < THRESHOLDS['phylo_env_min']).sum()\n",
    "            moderate_env = ((env['Phylogenetic_distance'] >= THRESHOLDS['phylo_env_min']) & \n",
    "                           (env['Phylogenetic_distance'] < 0.15)).sum()\n",
    "            distant = (env['Phylogenetic_distance'] >= 0.15).sum()\n",
    "            print(f\"        <{THRESHOLDS['phylo_env_min']}: {close:,} ({close/len(env)*100:.1f}%)\")\n",
    "            print(f\"        {THRESHOLDS['phylo_env_min']}-0.15: {moderate_env:,} ({moderate_env/len(env)*100:.1f}%)\")\n",
    "            print(f\"        ≥0.15: {distant:,} ({distant/len(env)*100:.1f}%)\")\n",
    "    \n",
    "    # Print issues\n",
    "    if issues:\n",
    "        print(f\"\\n  Issues found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "    \n",
    "    # STATISTICS\n",
    "    print(f\"\\n2. CLASSIFICATION STATISTICS:\")\n",
    "    \n",
    "    print(f\"\\n  Per-Sample Level ({len(df):,} total):\")\n",
    "    \n",
    "    class_dist = df.groupby('final_classification').agg({\n",
    "        'asv_id': 'count',\n",
    "        'final_confidence': 'mean',\n",
    "    }).rename(columns={\n",
    "        'asv_id': 'count',\n",
    "        'final_confidence': 'avg_confidence',\n",
    "    })\n",
    "    \n",
    "    if 'reads' in df.columns:\n",
    "        reads_sum = df.groupby('final_classification')['reads'].sum()\n",
    "        class_dist['total_reads'] = reads_sum\n",
    "        class_dist['reads_percentage'] = (class_dist['total_reads'] / df['reads'].sum()) * 100\n",
    "    \n",
    "    class_dist['percentage'] = (class_dist['count'] / len(df)) * 100\n",
    "    class_dist = class_dist.sort_values('count', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + class_dist.to_string())\n",
    "    \n",
    "    # Sequence level\n",
    "    print(f\"\\n  Sequence Level ({len(summary_df):,} unique ASVs):\")\n",
    "    \n",
    "    seq_dist = summary_df['overall_classification'].value_counts()\n",
    "    for cls, count in seq_dist.items():\n",
    "        pct = count / len(summary_df) * 100\n",
    "        print(f\"    {cls:40s}: {count:6,} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    print(f\"\\n3. CONFIDENCE DISTRIBUTION:\")\n",
    "    \n",
    "    high_conf = (df['final_confidence'] >= 0.80).sum()\n",
    "    med_conf = ((df['final_confidence'] >= 0.60) & (df['final_confidence'] < 0.80)).sum()\n",
    "    low_conf = (df['final_confidence'] < 0.60).sum()\n",
    "    \n",
    "    print(f\"  High (≥0.80): {high_conf:,} ({high_conf/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Medium (0.60-0.80): {med_conf:,} ({med_conf/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Low (<0.60): {low_conf:,} ({low_conf/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Review needed\n",
    "    if 'needs_review' in df.columns:\n",
    "        review_needed = df['needs_review'].sum()\n",
    "        print(f\"\\n  Flagged for review: {review_needed:,} ({review_needed/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Create statistics dataframe\n",
    "    stats_data = []\n",
    "    \n",
    "    for cls in class_dist.index:\n",
    "        class_df = df[df['final_classification'] == cls]\n",
    "        \n",
    "        row_data = {\n",
    "            'Classification': cls,\n",
    "            'Count': len(class_df),\n",
    "            'Percentage': len(class_df) / len(df) * 100,\n",
    "            'Avg_Confidence': class_df['final_confidence'].mean(),\n",
    "        }\n",
    "        \n",
    "        if 'reads' in df.columns:\n",
    "            row_data['Total_Reads'] = class_df['reads'].sum()\n",
    "            row_data['Reads_Percentage'] = class_df['reads'].sum() / df['reads'].sum() * 100\n",
    "            row_data['Avg_Reads_Per_Sample'] = class_df['reads'].mean()\n",
    "        \n",
    "        if 'needs_review' in df.columns:\n",
    "            row_data['Needs_Review'] = class_df['needs_review'].sum()\n",
    "        \n",
    "        stats_data.append(row_data)\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    \n",
    "    return stats_df, issues\n",
    "\n",
    "# ========================\n",
    "# STEP 12: EXPORT RESULTS\n",
    "# ========================\n",
    "\n",
    "def export_results(df, summary_df, stats_df, feature_importance, auth_db, issues):\n",
    "    \"\"\"Export all results to files\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 12: EXPORTING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Main classified file\n",
    "    print(f\"\\n1. Main Classification File:\")\n",
    "    df.to_csv(OUTPUT_CLASSIFIED, index=False)\n",
    "    print(f\"   ✓ {OUTPUT_CLASSIFIED}\")\n",
    "    print(f\"     {len(df):,} rows × {len(df.columns)} columns\")\n",
    "    \n",
    "    # 2. Sequence summary\n",
    "    print(f\"\\n2. Sequence Summary:\")\n",
    "    summary_df.to_csv(OUTPUT_SEQUENCE_SUMMARY, index=False)\n",
    "    print(f\"   ✓ {OUTPUT_SEQUENCE_SUMMARY}\")\n",
    "    print(f\"     {len(summary_df):,} unique ASVs\")\n",
    "    \n",
    "    # 3. Statistics\n",
    "    print(f\"\\n3. Classification Statistics:\")\n",
    "    stats_df.to_csv(OUTPUT_STATISTICS, index=False)\n",
    "    print(f\"   ✓ {OUTPUT_STATISTICS}\")\n",
    "    \n",
    "    # 4. Feature importance\n",
    "    if feature_importance is not None:\n",
    "        print(f\"\\n4. Feature Importance:\")\n",
    "        feature_importance.to_csv(OUTPUT_FEATURE_IMPORTANCE, index=False)\n",
    "        print(f\"   ✓ {OUTPUT_FEATURE_IMPORTANCE}\")\n",
    "    \n",
    "    # 5. Detailed report\n",
    "    print(f\"\\n5. Detailed Report:\")\n",
    "    create_detailed_report(df, summary_df, stats_df, auth_db, issues)\n",
    "    print(f\"   ✓ {OUTPUT_REPORT}\")\n",
    "\n",
    "# ========================\n",
    "# CREATE DETAILED REPORT\n",
    "# ========================\n",
    "\n",
    "def create_detailed_report(df, summary_df, stats_df, auth_db, issues):\n",
    "    \"\"\"Create comprehensive text report\"\"\"\n",
    "    \n",
    "    with open(OUTPUT_REPORT, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"ASV CLASSIFICATION PIPELINE - COMPREHENSIVE REPORT\\n\")\n",
    "        f.write(\"Version 3.0 - Intra-Species Variants & No Mixed\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Generated: {pd.Timestamp.now()}\\n\\n\")\n",
    "        \n",
    "        # Dataset summary\n",
    "        f.write(\"DATASET SUMMARY\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(f\"Total ASV records: {len(df):,}\\n\")\n",
    "        \n",
    "        if 'project_readfile_id' in df.columns:\n",
    "            f.write(f\"Unique samples: {df['project_readfile_id'].nunique():,}\\n\")\n",
    "        \n",
    "        if 'asv_id' in df.columns:\n",
    "            f.write(f\"Unique ASV IDs: {df['asv_id'].nunique():,}\\n\")\n",
    "        \n",
    "        if 'reads' in df.columns:\n",
    "            f.write(f\"Total reads: {df['reads'].sum():,}\\n\")\n",
    "        \n",
    "        f.write(f\"Authenticated ASVs in database: {len(auth_db):,}\\n\\n\")\n",
    "        \n",
    "        # Methodology\n",
    "        f.write(\"METHODOLOGY\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"Three-Phase Classification Strategy:\\n\\n\")\n",
    "        \n",
    "        f.write(\"Pre-filter: Technical Artifacts (reads < 4)\\n\")\n",
    "        f.write(\"  - Applied BEFORE Phase 1\\n\")\n",
    "        f.write(\"  - Removes low-read technical noise\\n\\n\")\n",
    "        \n",
    "        f.write(\"Phase 1: Rule-Based Classification\\n\")\n",
    "        f.write(\"  - Authenticated: autopropose='select' AND match='match'\\n\")\n",
    "        f.write(\"  - Cross-Contamination: In authenticated_db but not primary\\n\")\n",
    "        f.write(\"  - Intra-Species Variants: Co-occurrence ≥2 times with same auth ASV\\n\")\n",
    "        f.write(\"  - Environmental: Taxonomy mismatch or high divergence\\n\")\n",
    "        f.write(\"  - Technical Artifacts: Abnormal length, low quality\\n\")\n",
    "        f.write(\"  - Uncertain: No clear pattern (will be classified by ML)\\n\\n\")\n",
    "        \n",
    "        f.write(\"Phase 2: ML Classification\\n\")\n",
    "        f.write(\"  - Trained on strong evidence only (excludes Uncertain)\\n\")\n",
    "        f.write(\"  - Predicts ALL samples including Uncertain\\n\")\n",
    "        f.write(\"  - Random Forest with biological features\\n\")\n",
    "        f.write(\"  - NO Uncertain in final output\\n\\n\")\n",
    "        \n",
    "        f.write(\"Phase 3: Sequence Summary\\n\")\n",
    "        f.write(\"  - Priority-based (no Mixed category)\\n\")\n",
    "        f.write(\"  - Priority: Authenticated > Intra-Species > Environmental > Technical\\n\")\n",
    "        f.write(\"  - Cross-Contamination counted as Authenticated (was authenticated elsewhere)\\n\\n\")\n",
    "        \n",
    "        # Important notes\n",
    "        f.write(\"IMPORTANT NOTES\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"  • Technical_Artifacts (reads < 4) pre-filtered before ML\\n\")\n",
    "        f.write(\"  • Intra-Species Variants detected by co-occurrence pattern\\n\")\n",
    "        f.write(\"  • Cannot distinguish Heteroplasmy vs NUMTs without genomic data\\n\")\n",
    "        f.write(\"  • All sequences passed filtertranslate (no internal stops)\\n\")\n",
    "        f.write(\"  • Same ASV can be different class in different samples\\n\")\n",
    "        f.write(\"  • Cross-Contamination only for authenticated ASVs\\n\")\n",
    "        f.write(\"  • NO Mixed category in sequence summary\\n\")\n",
    "        f.write(\"  • NO Uncertain in final classification\\n\\n\")\n",
    "        \n",
    "        # Validation\n",
    "        f.write(\"VALIDATION CHECKS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        if issues:\n",
    "            f.write(\"Issues found:\\n\")\n",
    "            for issue in issues:\n",
    "                f.write(f\"  {issue}\\n\")\n",
    "        else:\n",
    "            f.write(\"  ✓ All validation checks passed\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Classification statistics\n",
    "        f.write(\"CLASSIFICATION STATISTICS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Per-Sample Level:\\n\")\n",
    "        f.write(stats_df.to_string(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Sequence Level:\\n\")\n",
    "        if 'overall_classification' in summary_df.columns:\n",
    "            seq_dist = summary_df['overall_classification'].value_counts()\n",
    "            for cls, count in seq_dist.items():\n",
    "                pct = count / len(summary_df) * 100\n",
    "                f.write(f\"  {cls:40s}: {count:6,} ({pct:5.2f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Detailed class analysis\n",
    "        f.write(\"DETAILED CLASS ANALYSIS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for cls in stats_df['Classification']:\n",
    "            class_df = df[df['final_classification'] == cls]\n",
    "            \n",
    "            f.write(f\"\\n{cls}\\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\")\n",
    "            f.write(f\"Count: {len(class_df):,}\\n\")\n",
    "            f.write(f\"Percentage: {len(class_df)/len(df)*100:.2f}%\\n\")\n",
    "            f.write(f\"Avg Confidence: {class_df['final_confidence'].mean():.3f}\\n\")\n",
    "            \n",
    "            if 'reads' in class_df.columns:\n",
    "                f.write(f\"Total Reads: {class_df['reads'].sum():,}\\n\")\n",
    "                f.write(f\"Avg Reads: {class_df['reads'].mean():.1f}\\n\")\n",
    "            \n",
    "            if 'needs_review' in class_df.columns:\n",
    "                f.write(f\"Needs Review: {class_df['needs_review'].sum():,}\\n\")\n",
    "        \n",
    "        # Thresholds\n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"THRESHOLDS USED\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for key, value in THRESHOLDS.items():\n",
    "            f.write(f\"  {key:35s}: {value}\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"RECOMMENDATIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"1. Use Authenticated sequences for:\\n\")\n",
    "        f.write(\"   • Phylogenetic analysis\\n\")\n",
    "        f.write(\"   • Diversity studies\\n\")\n",
    "        f.write(\"   • Species identification\\n\\n\")\n",
    "        \n",
    "        f.write(\"2. Intra-Species Variants:\\n\")\n",
    "        f.write(\"   • Could be Heteroplasmy OR NUMTs\\n\")\n",
    "        f.write(\"   • Require genomic validation to distinguish\\n\")\n",
    "        f.write(\"   • Exclude from phylogenetic analyses\\n\\n\")\n",
    "        \n",
    "        f.write(\"3. Cross-Contamination:\\n\")\n",
    "        f.write(\"   • Review patterns to identify sources\\n\")\n",
    "        f.write(\"   • Consider lab protocol improvements\\n\\n\")\n",
    "        \n",
    "        f.write(\"4. Environmental Contamination:\\n\")\n",
    "        f.write(\"   • Verify taxonomy assignments\\n\")\n",
    "        f.write(\"   • May include prey, parasites, or environmental DNA\\n\\n\")\n",
    "        \n",
    "        f.write(\"5. Technical Artifacts:\\n\")\n",
    "        f.write(\"   • Exclude from all analyses\\n\")\n",
    "        f.write(\"   • Represent sequencing noise\\n\\n\")\n",
    "        \n",
    "        # Footer\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"END OF REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ========================\n",
    "# MAIN PIPELINE\n",
    "# ========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main classification pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ASV CLASSIFICATION PIPELINE\")\n",
    "    print(\"Version 3.0 - Intra-Species Variants & No Mixed\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nStarted: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"\\nLoading data from: {INPUT_FILE}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_FILE, low_memory=False)\n",
    "        print(f\"  ✓ Loaded {len(df):,} rows\")\n",
    "        print(f\"  ✓ Columns: {len(df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR loading data: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Build authenticated database\n",
    "    auth_db = build_authenticated_database(df)\n",
    "    \n",
    "    if len(auth_db) == 0:\n",
    "        print(\"\\n⚠️  WARNING: No authenticated ASVs found!\")\n",
    "    \n",
    "    # Calculate sample statistics\n",
    "    asv_stats = calculate_sample_statistics(df)\n",
    "    \n",
    "    # PRE-FILTER: TECHNICAL ARTIFACTS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRE-FILTER: TECHNICAL ARTIFACTS (reads < 4)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tech_mask = df['reads'] < THRESHOLDS['technical_artifacts_reads']\n",
    "    n_tech = tech_mask.sum()\n",
    "    \n",
    "    print(f\"\\nFiltering reads < {THRESHOLDS['technical_artifacts_reads']}:\")\n",
    "    print(f\"  Technical_Artifacts: {n_tech:,} ({n_tech/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Biological samples:  {(~tech_mask).sum():,} ({(~tech_mask).sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Mark Technical_Artifacts\n",
    "    df.loc[tech_mask, 'final_classification'] = 'Technical_Artifacts'\n",
    "    df.loc[tech_mask, 'final_confidence'] = 0.95\n",
    "    df.loc[tech_mask, 'phase1_class'] = 'Technical_Artifacts'\n",
    "    df.loc[tech_mask, 'phase1_confidence'] = 0.95\n",
    "    df.loc[tech_mask, 'phase1_method'] = 'Pre-filter'\n",
    "    df.loc[tech_mask, 'phase1_reason'] = f'reads < {THRESHOLDS[\"technical_artifacts_reads\"]} (minimum threshold)'\n",
    "    df.loc[tech_mask, 'decision_method'] = 'Pre-filter'\n",
    "    df.loc[tech_mask, 'needs_review'] = False\n",
    "    \n",
    "    # Get clean data\n",
    "    clean_df = df[~tech_mask].copy()\n",
    "    clean_indices = clean_df.index\n",
    "    \n",
    "    print(f\"\\n  ✓ Pre-filter complete\")\n",
    "    print(f\"  ✓ Continuing with {len(clean_df):,} biological samples\")\n",
    "    \n",
    "    # Detect Intra-Species Variants\n",
    "    intra_species_db = detect_intra_species_variants(clean_df, auth_db)\n",
    "    \n",
    "    # PHASE 1: RULE-BASED CLASSIFICATION\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 1: RULE-BASED CLASSIFICATION (BIOLOGICAL)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    phase1_results = []\n",
    "    \n",
    "    iterator = clean_df.iterrows()\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(clean_df.iterrows(), total=len(clean_df), desc=\"  Classifying\")\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        result = classify_phase1_rule_based(row, auth_db, asv_stats, intra_species_db)\n",
    "        phase1_results.append(result)\n",
    "    \n",
    "    # Add to clean dataframe\n",
    "    clean_df['phase1_class'] = [r['class'] for r in phase1_results]\n",
    "    clean_df['phase1_confidence'] = [r['confidence'] for r in phase1_results]\n",
    "    clean_df['phase1_method'] = [r['method'] for r in phase1_results]\n",
    "    clean_df['phase1_reason'] = [r['reason'] for r in phase1_results]\n",
    "    clean_df['phase1_subtype'] = [r.get('subtype', '') for r in phase1_results]\n",
    "    \n",
    "    # Update main dataframe\n",
    "    for col in ['phase1_class', 'phase1_confidence', 'phase1_method', 'phase1_reason', 'phase1_subtype']:\n",
    "        df.loc[clean_indices, col] = clean_df[col]\n",
    "    \n",
    "    # Phase 1 summary\n",
    "    print(f\"\\nPhase 1 Summary (Biological classes):\")\n",
    "    \n",
    "    class_dist = clean_df.groupby('phase1_class').agg({\n",
    "        'asv_id': 'count',\n",
    "        'phase1_confidence': 'mean'\n",
    "    }).rename(columns={'asv_id': 'count', 'phase1_confidence': 'avg_confidence'})\n",
    "    \n",
    "    class_dist['percentage'] = (class_dist['count'] / len(clean_df)) * 100\n",
    "    class_dist = class_dist.sort_values('count', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + class_dist.to_string())\n",
    "    \n",
    "    # Confidence levels\n",
    "    high_conf = (clean_df['phase1_confidence'] >= 0.90).sum()\n",
    "    med_conf = ((clean_df['phase1_confidence'] >= 0.70) & (clean_df['phase1_confidence'] < 0.90)).sum()\n",
    "    low_conf = (clean_df['phase1_confidence'] < 0.70).sum()\n",
    "    \n",
    "    print(f\"\\nConfidence Distribution (Biological):\")\n",
    "    print(f\"  High (≥0.90):       {high_conf:6,} ({high_conf/len(clean_df)*100:5.2f}%)\")\n",
    "    print(f\"  Medium (0.70-0.89): {med_conf:6,} ({med_conf/len(clean_df)*100:5.2f}%)\")\n",
    "    print(f\"  Low (<0.70):        {low_conf:6,} ({low_conf/len(clean_df)*100:5.2f}%)\")\n",
    "    \n",
    "    # PHASE 2: ML TRAINING & PREDICTION\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 2: ML TRAINING & PREDICTION (BIOLOGICAL)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"\\nExtracting ML features (biological samples)...\")\n",
    "    \n",
    "    bio_features = []\n",
    "    \n",
    "    iterator = clean_df.iterrows()\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(clean_df.iterrows(), total=len(clean_df), desc=\"  Extracting features\")\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        features = extract_ml_features(row, auth_db, asv_stats)\n",
    "        bio_features.append(features)\n",
    "    \n",
    "    features_df = pd.DataFrame(bio_features)\n",
    "    features_df.index = clean_df.index\n",
    "    \n",
    "    print(f\"\\n  ✓ Extracted {len(features_df.columns)} features\")\n",
    "    \n",
    "    # Training data: confidence ≥0.70 AND class ≠ Uncertain\n",
    "    training_mask = (\n",
    "        (clean_df['phase1_confidence'] >= 0.70) &\n",
    "        (clean_df['phase1_class'] != 'Uncertain')\n",
    "    )\n",
    "    \n",
    "    training_df = clean_df[training_mask].copy()\n",
    "    training_features = features_df[training_mask]\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(training_df):,} (confidence ≥0.70, excluding Uncertain)\")\n",
    "    print(f\"  Excluded Uncertain: {(clean_df['phase1_class'] == 'Uncertain').sum():,}\")\n",
    "    \n",
    "    if len(training_df) < 100:\n",
    "        print(\"\\n⚠️  WARNING: Very few training samples!\")\n",
    "    \n",
    "    # Train model\n",
    "    clf, scaler, le, feature_importance = train_ml_model(training_df, training_features)\n",
    "    \n",
    "    if clf is None:\n",
    "        print(\"\\n❌ ML training failed. Using Phase 1 results only.\")\n",
    "        df.loc[clean_indices, 'final_classification'] = clean_df['phase1_class']\n",
    "        df.loc[clean_indices, 'final_confidence'] = clean_df['phase1_confidence']\n",
    "        df.loc[clean_indices, 'decision_method'] = 'Rule_Only'\n",
    "        df.loc[clean_indices, 'needs_review'] = clean_df['phase1_confidence'] < 0.70\n",
    "    else:\n",
    "        # Predict all biological samples (including Uncertain)\n",
    "        clean_df = validate_with_ml(clean_df, clf, scaler, le, features_df)\n",
    "        \n",
    "        # Update main dataframe\n",
    "        for col in ['ml_prediction', 'ml_confidence', 'final_classification', \n",
    "                    'final_confidence', 'decision_method', 'needs_review']:\n",
    "            if col in clean_df.columns:\n",
    "                df.loc[clean_indices, col] = clean_df[col]\n",
    "        \n",
    "        # Add ML probabilities\n",
    "        for col in clean_df.columns:\n",
    "            if col.startswith('ml_prob_'):\n",
    "                df.loc[clean_indices, col] = clean_df[col]\n",
    "        \n",
    "        # Save model\n",
    "        try:\n",
    "            joblib.dump({\n",
    "                'clf': clf,\n",
    "                'scaler': scaler,\n",
    "                'le': le,\n",
    "                'feature_names': features_df.columns.tolist(),\n",
    "                'thresholds': THRESHOLDS\n",
    "            }, OUTPUT_MODEL)\n",
    "            print(f\"\\n✓ Model saved: {OUTPUT_MODEL}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Could not save model: {e}\")\n",
    "    \n",
    "    # Post-validation fix\n",
    "    df = post_validation_fix(df, auth_db)\n",
    "    \n",
    "    # Create sequence summary (NO MIXED)\n",
    "    summary_df = create_sequence_summary_no_mixed(df)\n",
    "    \n",
    "    # Validation and statistics\n",
    "    stats_df, issues = validate_and_create_statistics(df, summary_df, auth_db)\n",
    "    \n",
    "    # Export results\n",
    "    export_results(df, summary_df, stats_df, feature_importance, auth_db, issues)\n",
    "    \n",
    "    # FINAL SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"  Total records: {len(df):,}\")\n",
    "    \n",
    "    if 'project_readfile_id' in df.columns:\n",
    "        print(f\"  Unique samples: {df['project_readfile_id'].nunique():,}\")\n",
    "    \n",
    "    if 'asv_id' in df.columns:\n",
    "        print(f\"  Unique ASVs: {df['asv_id'].nunique():,}\")\n",
    "    \n",
    "    if 'reads' in df.columns:\n",
    "        print(f\"  Total reads: {df['reads'].sum():,.0f}\")\n",
    "    \n",
    "    print(f\"\\nPre-filter Results:\")\n",
    "    print(f\"  Technical_Artifacts: {n_tech:,} ({n_tech/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Biological samples:  {len(clean_df):,} ({len(clean_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nFinal Classification Results:\")\n",
    "    if 'final_classification' in df.columns:\n",
    "        final_dist = df['final_classification'].value_counts()\n",
    "        for cls, count in final_dist.items():\n",
    "            pct = count / len(df) * 100\n",
    "            avg_conf = df[df['final_classification'] == cls]['final_confidence'].mean()\n",
    "            print(f\"  {cls:40s}: {count:6,} ({pct:5.2f}%) [conf={avg_conf:.3f}]\")\n",
    "    \n",
    "    # Check for Uncertain\n",
    "    uncertain_final = (df['final_classification'] == 'Uncertain').sum()\n",
    "    if uncertain_final > 0:\n",
    "        print(f\"\\n  ⚠️  WARNING: Still have {uncertain_final:,} Uncertain in final!\")\n",
    "    else:\n",
    "        print(f\"\\n  ✓ No Uncertain in final classification\")\n",
    "    \n",
    "    if 'needs_review' in df.columns:\n",
    "        review_count = df['needs_review'].sum()\n",
    "        print(f\"\\nQuality Control:\")\n",
    "        print(f\"  Needs review: {review_count:,} ({review_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if 'final_confidence' in df.columns:\n",
    "        high_conf_final = (df['final_confidence'] >= 0.80).sum()\n",
    "        print(f\"  High confidence (≥0.80): {high_conf_final:,}\")\n",
    "    \n",
    "    print(f\"\\nOutput Files:\")\n",
    "    print(f\"  ✓ {OUTPUT_CLASSIFIED}\")\n",
    "    print(f\"  ✓ {OUTPUT_SEQUENCE_SUMMARY}\")\n",
    "    print(f\"  ✓ {OUTPUT_STATISTICS}\")\n",
    "    print(f\"  ✓ {OUTPUT_REPORT}\")\n",
    "    if feature_importance is not None:\n",
    "        print(f\"  ✓ {OUTPUT_FEATURE_IMPORTANCE}\")\n",
    "    if clf is not None:\n",
    "        print(f\"  ✓ {OUTPUT_MODEL}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nFinished: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"  1. Review flagged sequences in: {OUTPUT_CLASSIFIED}\")\n",
    "    print(f\"  2. Check detailed report: {OUTPUT_REPORT}\")\n",
    "    print(f\"  3. Use Authenticated sequences for downstream analysis\")\n",
    "    print(f\"  4. Intra-Species Variants require genomic validation\")\n",
    "    print(f\"  5. Consider reads < 4 as technical noise (excluded)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ========================\n",
    "# RUN PIPELINE\n",
    "# ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️  Pipeline interrupted by user\")\n",
    "        print(\"Exiting gracefully...\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(\"\\n\\n\" + \"=\"*80)\n",
    "        print(\"❌ CRITICAL ERROR OCCURRED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nError Type: {type(e).__name__}\")\n",
    "        print(f\"Error Message: {str(e)}\")\n",
    "        print(\"\\nFull Traceback:\")\n",
    "        print(\"-\"*80)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"-\"*80)\n",
    "        print(\"\\nTroubleshooting Tips:\")\n",
    "        print(\"  1. Check if input file exists and is readable\")\n",
    "        print(\"  2. Verify all required columns are present\")\n",
    "        print(\"  3. Ensure sufficient disk space for output\")\n",
    "        print(\"  4. Check write permissions for output directory\")\n",
    "        print(\"  5. Review error message above for specific issue\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
