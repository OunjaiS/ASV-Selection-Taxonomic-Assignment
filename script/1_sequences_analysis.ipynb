{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41db0455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE INSECT COI ANALYSIS PIPELINE - UNIFIED v11.1\n",
      "Preserves Original Metadata + Adds Analysis Results\n",
      "================================================================================\n",
      "\n",
      "🆕 NEW IN v11.1:\n",
      "  ✓ Preserves ALL original metadata columns IN ORIGINAL ORDER\n",
      "  ✓ Analysis results prefixed with 'ANALYSIS_' added at the END\n",
      "  ✓ Easy to identify new vs original data\n",
      "  ✓ Compatible with downstream abundance analysis\n",
      "\n",
      "Configuration:\n",
      "  Metadata file: /Users/sarawut/Desktop/Manuscript_ASV_selection/raw_data/ASV_Authentication_Results_030925.csv\n",
      "  FASTA file: /Users/sarawut/Desktop/Manuscript_ASV_selection/raw_data/ASV_table_sequences_66595.fasta\n",
      "  Output directory: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis\n",
      "\n",
      "Analysis Features:\n",
      "  ✓ ORF detection (6 frames)\n",
      "  ✓ Frameshift correction\n",
      "  ✓ 10 DNA-level COI motifs\n",
      "  ✓ 16 Protein-level COI patterns\n",
      "  ✓ 3 Conserved regions\n",
      "  ✓ Insect mitochondrial genetic code (TGA=W, ATA=M)\n",
      "  ✓ Nucleotide composition analysis\n",
      "  ✓ Protein property analysis\n",
      "  ✓ Quality scoring and grading\n",
      "  ✓ COI confidence assessment\n",
      "  ✓ Alignment to reference\n",
      "  ✓ Codon usage analysis\n",
      "  ✓ Amino acid composition\n",
      "  ✓ Comprehensive visualization\n",
      "\n",
      "Checking dependencies...\n",
      "  ✓ BioPython available\n",
      "  ✓ Bio.SeqUtils.ProtParam available\n",
      "  ✓ tqdm available (progress bars enabled)\n",
      "  ✓ Visualization libraries available\n",
      "\n",
      "Validating input files...\n",
      "  ✓ FASTA file found\n",
      "  ✓ Metadata file found\n",
      "    Loaded 175,955 metadata records\n",
      "    Original metadata columns: 50\n",
      "    Column order will be preserved in output\n",
      "\n",
      "================================================================================\n",
      "PROCESSING SEQUENCES\n",
      "================================================================================\n",
      "\n",
      "Processing FASTA file: /Users/sarawut/Desktop/Manuscript_ASV_selection/raw_data/ASV_table_sequences_66595.fasta\n",
      "  Original metadata: 175,955 rows × 50 columns\n",
      "  Using ASV_ID column: 'asv_id'\n",
      "  Unique ASV_IDs in metadata: 64,544\n",
      "  Duplicate ASV_IDs exist: True\n",
      "\n",
      "Analyzing sequences from FASTA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Processing sequences: 100%|██████████| 66595/66595 [00:58<00:00, 1138.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Completed: 66,595 unique sequences analyzed\n",
      "  Analysis results stored for 66,595 ASV_IDs\n",
      "\n",
      "  Analysis DataFrame: 66,595 rows × 100 columns\n",
      "\n",
      "Merging analysis results back to original metadata...\n",
      "  Original metadata rows: 175,955\n",
      "  After merge: 175,955 rows × 149 columns\n",
      "  ✓ Row count preserved: 175,955 rows\n",
      "\n",
      "  ✓ 23,138 ASV_IDs appear multiple times\n",
      "     (same analysis results copied to all rows with same ASV_ID)\n",
      "     Max duplicates: 1087 times (ASV: uniq17)\n",
      "\n",
      "  Final column organization:\n",
      "    - Original metadata (preserved order): 50\n",
      "    - Analysis columns (at the end): 99\n",
      "    - Total: 149\n",
      "\n",
      "Processing complete!\n",
      "  Total sequences: 175,955\n",
      "  Total columns: 149\n",
      "    - Original metadata (preserved order): 50\n",
      "    - Analysis columns (at the end): 99\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "\n",
      "✓ Main results: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_Complete_Analysis.csv\n",
      "  175,955 rows × 149 columns\n",
      "  Column order: Original metadata → Analysis results\n",
      "\n",
      "Generating summary statistics...\n",
      "  Generated 14 summary statistics\n",
      "\n",
      "✓ Summary statistics: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/Analysis_Summary.csv\n",
      "\n",
      "Generating codon usage table...\n",
      "  Analyzing all 175,113 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Processing: 100%|██████████| 175113/175113 [00:07<00:00, 22123.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 64 unique codons from 175,113 sequences\n",
      "\n",
      "✓ Codon usage table: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/Codon_Usage_Table.csv\n",
      "\n",
      "Generating amino acid composition table...\n",
      "  Analyzing 175,113 PASS sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Processing: 100%|██████████| 175113/175113 [00:03<00:00, 47464.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total amino acids analyzed: 23,977,528\n",
      "  Unique amino acids found: 20\n",
      "\n",
      "✓ Amino acid composition: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/AA_Composition.csv\n",
      "\n",
      "Generating motif summary...\n",
      "  Generated summary for 29 motifs\n",
      "\n",
      "✓ Motif summary: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/Motif_Analysis_Summary.csv\n",
      "\n",
      "================================================================================\n",
      "EXPORTING FASTA FILES\n",
      "================================================================================\n",
      "\n",
      "Exporting FASTA file: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_High_Quality_Sequences.fasta\n",
      "  Filtering by priority: ['Priority_1_Highest', 'Priority_2_High']\n",
      "  No sequences to export\n",
      "\n",
      "Exporting FASTA file: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_Corrected_Sequences.fasta\n",
      "  Using all PASS sequences\n",
      "  Exporting 175,113 sequences...\n",
      "  Exported to: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_Corrected_Sequences.fasta\n",
      "  Total sequences: 175,113\n",
      "\n",
      "================================================================================\n",
      "CREATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "Creating visualizations...\n",
      "  ✓ Quality_Distribution.png\n",
      "  ✓ Sequence_Characteristics.png\n",
      "\n",
      "  Created 2 visualization files\n",
      "  Saved to: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/visualizations\n",
      "\n",
      "✓ Created 2 visualization files\n",
      "  Location: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/visualizations\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE ANALYSIS REPORT\n",
      "================================================================================\n",
      "\n",
      "1. OVERALL STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "  Total sequences analyzed: 175,955\n",
      "\n",
      "2. QC STATUS\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ PASS      : 175,113 ( 99.5%)\n",
      "  ⚠ WARNING   :      0 (  0.0%)\n",
      "  ✗ FAIL      :      0 (  0.0%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COLUMN ORGANIZATION\n",
      "================================================================================\n",
      "\n",
      "✓ Original Metadata Columns (50) - ORDER PRESERVED:\n",
      "   1. project\n",
      "   2. project_readfile_id\n",
      "   3. project_sample_id\n",
      "   4. country\n",
      "   5. image_id\n",
      "   6. family\n",
      "   7. subfamily\n",
      "   8. asv_id\n",
      "   9. reads\n",
      "  10. total_asv_reads\n",
      "  11. asv_count\n",
      "  12. percentage_reads\n",
      "  13. match\n",
      "  14. autopropose\n",
      "  15. asv_family_v1\n",
      "  ... and 35 more\n",
      "\n",
      "✓ New Analysis Columns (99) - ADDED AT THE END:\n",
      "   1. ANALYSIS_original_sequence_full\n",
      "   2. ANALYSIS_original_sequence\n",
      "   3. ANALYSIS_original_length\n",
      "   4. ANALYSIS_trim_applied\n",
      "   5. ANALYSIS_trimmed_length\n",
      "   6. ANALYSIS_best_frame\n",
      "   7. ANALYSIS_best_strand\n",
      "   8. ANALYSIS_orf_score\n",
      "   9. ANALYSIS_orf_length\n",
      "  10. ANALYSIS_internal_stops_initial\n",
      "  11. ANALYSIS_frameshift_correction_applied\n",
      "  12. ANALYSIS_internal_stops\n",
      "  13. ANALYSIS_corrected_sequence_full\n",
      "  14. ANALYSIS_corrected_sequence\n",
      "  15. ANALYSIS_corrected_length\n",
      "  ... and 84 more\n",
      "\n",
      "================================================================================\n",
      "OUTPUT FILES SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Generated Files:\n",
      "\n",
      "1. Main Analysis Results\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_Complete_Analysis.csv\n",
      "   Info: 175,955 sequences × 149 columns\n",
      "   Structure: [Original Metadata] + [ANALYSIS_* columns]\n",
      "\n",
      "2. Summary Statistics\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/Analysis_Summary.csv\n",
      "\n",
      "3. Codon Usage Table\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/Codon_Usage_Table.csv\n",
      "\n",
      "4. Amino Acid Composition\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/AA_Composition.csv\n",
      "\n",
      "5. Motif Detection Summary\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/Motif_Analysis_Summary.csv\n",
      "\n",
      "6. High Quality FASTA\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_High_Quality_Sequences.fasta\n",
      "\n",
      "7. All Corrected FASTA\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/ASV_Corrected_Sequences.fasta\n",
      "\n",
      "8. Visualizations\n",
      "   Path: /Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis/visualizations/\n",
      "\n",
      "================================================================================\n",
      "✓ ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Total runtime: 2025-10-24 12:43:38\n",
      "\n",
      "📊 Output file structure:\n",
      "   - Original metadata columns in their ORIGINAL ORDER\n",
      "   - Analysis columns (ANALYSIS_*) appended at the END\n",
      "   - Ready for abundance-based classification!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete Insect COI Analysis Pipeline - Unified Version v11.1\n",
    "Preserves all original metadata + adds new analysis columns\n",
    "Author: Integrated Analysis Pipeline\n",
    "Date: 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BioPython\n",
    "try:\n",
    "    from Bio import SeqIO\n",
    "    from Bio import Align\n",
    "    from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "    BIOPYTHON_AVAILABLE = True\n",
    "    PROTPARAM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BIOPYTHON_AVAILABLE = False\n",
    "    PROTPARAM_AVAILABLE = False\n",
    "    print(\"WARNING: BioPython not available. Install with: pip install biopython\")\n",
    "\n",
    "# Progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    VISUALIZATION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VISUALIZATION_AVAILABLE = False\n",
    "\n",
    "# ========================\n",
    "# CONFIGURATION\n",
    "# ========================\n",
    "# Input files\n",
    "METADATA_FILE = \"/Users/sarawut/Desktop/Manuscript_ASV_selection/raw_data/ASV_Authentication_Results_030925.csv\"\n",
    "FASTA_FILE = \"/Users/sarawut/Desktop/Manuscript_ASV_selection/raw_data/ASV_table_sequences_66595.fasta\"\n",
    "\n",
    "# Output files\n",
    "OUTPUT_DIR = \"/Users/sarawut/Desktop/Manuscript_ASV_selection/data_analysis/sequences_analysis\"\n",
    "OUTPUT_FILE = f\"{OUTPUT_DIR}/ASV_Complete_Analysis.csv\"\n",
    "OUTPUT_SUMMARY = f\"{OUTPUT_DIR}/Analysis_Summary.csv\"\n",
    "OUTPUT_CODON_USAGE = f\"{OUTPUT_DIR}/Codon_Usage_Table.csv\"\n",
    "OUTPUT_AA_COMPOSITION = f\"{OUTPUT_DIR}/AA_Composition.csv\"\n",
    "OUTPUT_MOTIF_SUMMARY = f\"{OUTPUT_DIR}/Motif_Analysis_Summary.csv\"\n",
    "OUTPUT_FASTA_CORRECTED = f\"{OUTPUT_DIR}/ASV_Corrected_Sequences.fasta\"\n",
    "OUTPUT_FASTA_HIGH_QUALITY = f\"{OUTPUT_DIR}/ASV_High_Quality_Sequences.fasta\"\n",
    "\n",
    "# Analysis settings\n",
    "ANALYZE_ALL_SEQUENCES = True\n",
    "MAX_SEQUENCES_FOR_CODON = None  # None = analyze all\n",
    "CREATE_VISUALIZATIONS = True\n",
    "\n",
    "# COI Reference for alignment\n",
    "COI_REFERENCE = \"ATGGCNCAYCCNCCNCCNGCNGGNTCNAARAARGARGTNTTYAARTTYAGNWSNGTNAARWSNATYGTNATYCCNCCNGCN\"\n",
    "\n",
    "# ========================\n",
    "# INSECT MITOCHONDRIAL GENETIC CODE\n",
    "# ========================\n",
    "INSECT_GENETIC_CODE = {\n",
    "    'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L',\n",
    "    'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S',\n",
    "    'TAT': 'Y', 'TAC': 'Y', 'TAA': '*', 'TAG': '*',\n",
    "    'TGT': 'C', 'TGC': 'C', 'TGA': 'W', 'TGG': 'W',\n",
    "    'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L',\n",
    "    'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n",
    "    'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q',\n",
    "    'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R',\n",
    "    'ATT': 'I', 'ATC': 'I', 'ATA': 'M', 'ATG': 'M',\n",
    "    'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n",
    "    'AAT': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K',\n",
    "    'AGT': 'S', 'AGC': 'S', 'AGA': 'S', 'AGG': 'S',\n",
    "    'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V',\n",
    "    'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n",
    "    'GAT': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E',\n",
    "    'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G',\n",
    "}\n",
    "\n",
    "# ========================\n",
    "# COI CONSERVED MOTIFS\n",
    "# ========================\n",
    "\n",
    "COI_DNA_MOTIFS = {\n",
    "    'GLYCINE_RICH': r'GG[ATGC]GG[ATGC]',\n",
    "    'LEUCINE_RICH_1': r'[CT][TC][ATGC](?:[CT][TC][ATGC]){2}',\n",
    "    'LEUCINE_RICH_2': r'TT[ATGC][CT][TC][ATGC]',\n",
    "    'PROLINE_PATTERN': r'CC[ATGC]CC[ATGC]',\n",
    "    'THR_GLY': r'AC[ATGC]GG[ATGC]',\n",
    "    'ALA_GLY': r'GC[ATGC]GG[ATGC]',\n",
    "    'VAL_LEU': r'GT[ATGC][CT][TC][ATGC]',\n",
    "    'PHE_LEU': r'TT[TC][CT][TC][ATGC]',\n",
    "    'ILE_VAL': r'AT[ATC]GT[ATGC]',\n",
    "    'GLY_THR_GLY': r'GG[ATGC]AC[ATGC]GG[ATGC]',\n",
    "}\n",
    "\n",
    "COI_PROTEIN_MOTIFS = {\n",
    "    'QUAD_LEUCINE': r'LLLL',\n",
    "    'TRI_LEUCINE': r'LLL',\n",
    "    'DI_LEUCINE': r'LL',\n",
    "    'GLYCINE_PAIR': r'GG',\n",
    "    'PROLINE_PAIR': r'PP',\n",
    "    'LEU_PRO': r'LP',\n",
    "    'GLY_THR': r'GT',\n",
    "    'PHE_LEU': r'FL',\n",
    "    'VAL_LEU': r'VL',\n",
    "    'ILE_VAL': r'IV',\n",
    "    'MET_ASN': r'MN',\n",
    "    'LEU_SER_LEU': r'LSL',\n",
    "    'GLY_ALA': r'GA',\n",
    "    'ALA_SER': r'AS',\n",
    "    'SER_VAL': r'SV',\n",
    "    'THR_TRP': r'TW',\n",
    "}\n",
    "\n",
    "COI_CONSERVED_REGIONS = {\n",
    "    'START_REGION': r'^ATG[GC][CA][CT]',\n",
    "    'HELIX_I': r'[CT][TC][ATGC]GG[ATGC][GC][CT][ATGC]',\n",
    "    'HELIX_II': r'[CT][TC][ATGC][CT][TC][ATGC][GC]C[ATGC]',\n",
    "}\n",
    "\n",
    "HYDROPHOBIC_AA = set('AILMFVPGW')\n",
    "POLAR_AA = set('STNQCY')\n",
    "CHARGED_AA = set('DEKRH')\n",
    "CHARGED_POSITIVE = set('KRH')\n",
    "CHARGED_NEGATIVE = set('DE')\n",
    "AROMATIC_AA = set('FYW')\n",
    "ALIPHATIC_AA = set('ILV')\n",
    "\n",
    "# ========================\n",
    "# UTILITY FUNCTIONS\n",
    "# ========================\n",
    "\n",
    "def clean_sequence(seq: str) -> str:\n",
    "    \"\"\"Clean DNA sequence\"\"\"\n",
    "    return str(seq).upper().replace('-', '').replace(' ', '').replace('.', '')\n",
    "\n",
    "def reverse_complement(seq: str) -> str:\n",
    "    \"\"\"Get reverse complement of DNA sequence\"\"\"\n",
    "    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G', 'N': 'N'}\n",
    "    return ''.join(complement.get(base, 'N') for base in reversed(seq))\n",
    "\n",
    "def translate_insect(seq: str, frame: int = 0) -> str:\n",
    "    \"\"\"Translate DNA to protein using insect mitochondrial genetic code\"\"\"\n",
    "    seq = seq[frame:]\n",
    "    protein = ''\n",
    "    for i in range(0, len(seq) - 2, 3):\n",
    "        codon = seq[i:i+3]\n",
    "        if len(codon) == 3:\n",
    "            protein += INSECT_GENETIC_CODE.get(codon, 'X') if 'N' not in codon else 'X'\n",
    "    return protein\n",
    "\n",
    "def safe_value(val, default=0):\n",
    "    \"\"\"Safely convert value to float\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return default\n",
    "    try:\n",
    "        if isinstance(val, pd.Series):\n",
    "            if len(val) > 0:\n",
    "                return float(val.iloc[0])\n",
    "            return default\n",
    "        return float(val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "# ========================\n",
    "# SEQUENCE TRIMMING\n",
    "# ========================\n",
    "\n",
    "def trim_sequence(seq: str) -> tuple:\n",
    "    \"\"\"Trim low-quality regions from sequence\"\"\"\n",
    "    original_len = len(seq)\n",
    "    \n",
    "    seq_trimmed = seq.strip('N')\n",
    "    seq_trimmed = re.sub(r'^[AT]{10,}', '', seq_trimmed)\n",
    "    seq_trimmed = re.sub(r'[AT]{10,}$', '', seq_trimmed)\n",
    "    \n",
    "    trimmed = original_len > len(seq_trimmed)\n",
    "    \n",
    "    return seq_trimmed, trimmed\n",
    "\n",
    "# ========================\n",
    "# ORF DETECTION & FRAME SELECTION\n",
    "# ========================\n",
    "\n",
    "def find_best_orf(seq: str) -> dict:\n",
    "    \"\"\"\n",
    "    Find the best open reading frame across all 6 possible frames\n",
    "    Returns: dict with frame, strand, sequence, protein, internal_stops, score\n",
    "    \"\"\"\n",
    "    best_result = {\n",
    "        'frame': 1,\n",
    "        'strand': '+',\n",
    "        'sequence': seq,\n",
    "        'protein': '',\n",
    "        'internal_stops': 999,\n",
    "        'score': 0,\n",
    "        'orf_length': 0\n",
    "    }\n",
    "    \n",
    "    for strand_name, strand_seq in [('+', seq), ('-', reverse_complement(seq))]:\n",
    "        for frame in range(3):\n",
    "            protein = translate_insect(strand_seq, frame)\n",
    "            \n",
    "            internal_stops = protein[:-1].count('*') if len(protein) > 1 else 0\n",
    "            \n",
    "            score = 0\n",
    "            \n",
    "            if internal_stops == 0:\n",
    "                score += 70\n",
    "            elif internal_stops == 1:\n",
    "                score += 35\n",
    "            elif internal_stops == 2:\n",
    "                score += 15\n",
    "            \n",
    "            if 200 <= len(protein) <= 250:\n",
    "                score += 20\n",
    "            elif 150 <= len(protein) <= 280:\n",
    "                score += 10\n",
    "            elif 100 <= len(protein) <= 300:\n",
    "                score += 5\n",
    "            \n",
    "            at_pct = (strand_seq.count('A') + strand_seq.count('T')) / len(strand_seq) * 100\n",
    "            if 60 <= at_pct <= 75:\n",
    "                score += 10\n",
    "            elif 55 <= at_pct <= 80:\n",
    "                score += 5\n",
    "            \n",
    "            if score > best_result['score']:\n",
    "                best_result = {\n",
    "                    'frame': frame + 1,\n",
    "                    'strand': strand_name,\n",
    "                    'sequence': strand_seq,\n",
    "                    'protein': protein,\n",
    "                    'internal_stops': internal_stops,\n",
    "                    'score': score,\n",
    "                    'orf_length': len(protein)\n",
    "                }\n",
    "    \n",
    "    return best_result\n",
    "\n",
    "# ========================\n",
    "# FRAMESHIFT CORRECTION\n",
    "# ========================\n",
    "\n",
    "def correct_frameshift(seq: str, protein: str, max_attempts: int = 3) -> tuple:\n",
    "    \"\"\"\n",
    "    Attempt to correct frameshifts by inserting/deleting bases near stop codons\n",
    "    Returns: (corrected_seq, corrected_protein, correction_applied)\n",
    "    \"\"\"\n",
    "    if '*' not in protein[:-1]:\n",
    "        return seq, protein, False\n",
    "    \n",
    "    best_seq = seq\n",
    "    best_protein = protein\n",
    "    min_stops = protein[:-1].count('*')\n",
    "    corrected = False\n",
    "    \n",
    "    stop_positions = [i for i, aa in enumerate(protein[:-1]) if aa == '*']\n",
    "    \n",
    "    for stop_pos in stop_positions[:max_attempts]:\n",
    "        for offset in range(-3, 4):\n",
    "            test_pos = (stop_pos * 3) + offset\n",
    "            \n",
    "            if 0 <= test_pos < len(seq):\n",
    "                test_seq = seq[:test_pos] + seq[test_pos+1:]\n",
    "                test_protein = translate_insect(test_seq)\n",
    "                test_stops = test_protein[:-1].count('*')\n",
    "                \n",
    "                if test_stops < min_stops:\n",
    "                    min_stops = test_stops\n",
    "                    best_seq = test_seq\n",
    "                    best_protein = test_protein\n",
    "                    corrected = True\n",
    "    \n",
    "    return best_seq, best_protein, corrected\n",
    "\n",
    "# ========================\n",
    "# MOTIF ANALYSIS\n",
    "# ========================\n",
    "\n",
    "def analyze_motifs_comprehensive(seq: str, protein: str) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive motif analysis for COI validation\n",
    "    Analyzes both DNA and protein-level conserved patterns\n",
    "    \"\"\"\n",
    "    if pd.isna(seq) or pd.isna(protein):\n",
    "        return {\n",
    "            'dna_motifs_found': 'None', 'dna_motif_count': 0, 'dna_motif_total_hits': 0,\n",
    "            'dna_motif_coverage': 0, 'dna_motif_positions': 'NA',\n",
    "            'protein_motifs_found': 'None', 'protein_motif_count': 0,\n",
    "            'protein_motif_total_hits': 0, 'protein_motif_coverage': 0,\n",
    "            'conserved_regions': 'None', 'conserved_region_count': 0,\n",
    "            'max_leucine_run': 0, 'max_glycine_run': 0,\n",
    "            'tga_count': 0, 'trp_count': 0, 'tga_trp_validated': False,\n",
    "            'motif_score': 0, 'motif_quality': 'Poor', 'coi_confidence': 'Very_Low'\n",
    "        }\n",
    "    \n",
    "    seq = str(seq).upper()\n",
    "    protein = str(protein).replace('*', '').replace('X', '')\n",
    "    \n",
    "    # 1. DNA-level motifs\n",
    "    dna_motifs_found = []\n",
    "    dna_motif_counts = {}\n",
    "    dna_positions = []\n",
    "    \n",
    "    for motif_name, pattern in COI_DNA_MOTIFS.items():\n",
    "        matches = list(re.finditer(pattern, seq, re.IGNORECASE))\n",
    "        if matches:\n",
    "            dna_motifs_found.append(motif_name)\n",
    "            dna_motif_counts[motif_name] = len(matches)\n",
    "            dna_positions.append(matches[0].start())\n",
    "    \n",
    "    # 2. Protein-level motifs\n",
    "    protein_motifs_found = []\n",
    "    protein_motif_counts = {}\n",
    "    \n",
    "    for motif_name, pattern in COI_PROTEIN_MOTIFS.items():\n",
    "        matches = list(re.finditer(pattern, protein))\n",
    "        if matches:\n",
    "            protein_motifs_found.append(motif_name)\n",
    "            protein_motif_counts[motif_name] = len(matches)\n",
    "    \n",
    "    # 3. Conserved regions\n",
    "    conserved_regions_found = []\n",
    "    for region_name, pattern in COI_CONSERVED_REGIONS.items():\n",
    "        if re.search(pattern, seq, re.IGNORECASE):\n",
    "            conserved_regions_found.append(region_name)\n",
    "    \n",
    "    # 4. Leucine and glycine runs\n",
    "    leucine_runs = re.findall(r'L{3,}', protein)\n",
    "    max_leucine_run = max([len(run) for run in leucine_runs]) if leucine_runs else 0\n",
    "    \n",
    "    glycine_runs = re.findall(r'G{2,}', protein)\n",
    "    max_glycine_run = max([len(run) for run in glycine_runs]) if glycine_runs else 0\n",
    "    \n",
    "    # 5. TGA (Trp) validation\n",
    "    tga_count = len(re.findall(r'TGA', seq))\n",
    "    trp_count = protein.count('W')\n",
    "    tga_trp_validated = (tga_count > 0 and trp_count > 0)\n",
    "    \n",
    "    # 6. Calculate coverage scores\n",
    "    dna_motif_coverage = (len(dna_motifs_found) / len(COI_DNA_MOTIFS)) * 100\n",
    "    protein_motif_coverage = (len(protein_motifs_found) / len(COI_PROTEIN_MOTIFS)) * 100\n",
    "    motif_score = (dna_motif_coverage + protein_motif_coverage) / 2\n",
    "    \n",
    "    # 7. Determine motif quality\n",
    "    if motif_score >= 60 and tga_trp_validated:\n",
    "        motif_quality = 'Excellent'\n",
    "    elif motif_score >= 40 and tga_trp_validated:\n",
    "        motif_quality = 'Good'\n",
    "    elif motif_score >= 25:\n",
    "        motif_quality = 'Fair'\n",
    "    else:\n",
    "        motif_quality = 'Poor'\n",
    "    \n",
    "    # 8. COI confidence assessment\n",
    "    if dna_motif_coverage >= 30 and protein_motif_coverage >= 30:\n",
    "        coi_confidence = 'High'\n",
    "    elif dna_motif_coverage >= 20 and protein_motif_coverage >= 20:\n",
    "        coi_confidence = 'Medium'\n",
    "    elif dna_motif_coverage >= 10 or protein_motif_coverage >= 10:\n",
    "        coi_confidence = 'Low'\n",
    "    else:\n",
    "        coi_confidence = 'Very_Low'\n",
    "    \n",
    "    return {\n",
    "        'dna_motifs_found': ';'.join(dna_motifs_found) if dna_motifs_found else 'None',\n",
    "        'dna_motif_count': len(dna_motifs_found),\n",
    "        'dna_motif_total_hits': sum(dna_motif_counts.values()),\n",
    "        'dna_motif_coverage': dna_motif_coverage,\n",
    "        'dna_motif_positions': ';'.join(map(str, dna_positions[:10])) if dna_positions else 'NA',\n",
    "        'protein_motifs_found': ';'.join(protein_motifs_found) if protein_motifs_found else 'None',\n",
    "        'protein_motif_count': len(protein_motifs_found),\n",
    "        'protein_motif_total_hits': sum(protein_motif_counts.values()),\n",
    "        'protein_motif_coverage': protein_motif_coverage,\n",
    "        'conserved_regions': ';'.join(conserved_regions_found) if conserved_regions_found else 'None',\n",
    "        'conserved_region_count': len(conserved_regions_found),\n",
    "        'max_leucine_run': max_leucine_run,\n",
    "        'max_glycine_run': max_glycine_run,\n",
    "        'tga_count': tga_count,\n",
    "        'trp_count': trp_count,\n",
    "        'tga_trp_validated': tga_trp_validated,\n",
    "        'motif_score': motif_score,\n",
    "        'motif_quality': motif_quality,\n",
    "        'coi_confidence': coi_confidence\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# NUCLEOTIDE COMPOSITION\n",
    "# ========================\n",
    "\n",
    "def analyze_nucleotide_composition(seq: str) -> dict:\n",
    "    \"\"\"Comprehensive nucleotide composition analysis\"\"\"\n",
    "    if pd.isna(seq) or len(seq) == 0:\n",
    "        return {}\n",
    "    \n",
    "    seq = str(seq).upper()\n",
    "    length = len(seq)\n",
    "    if length == 0:\n",
    "        return {}\n",
    "    \n",
    "    a_count = seq.count('A')\n",
    "    t_count = seq.count('T')\n",
    "    g_count = seq.count('G')\n",
    "    c_count = seq.count('C')\n",
    "    n_count = seq.count('N')\n",
    "    \n",
    "    gc_content = ((g_count + c_count) / length) * 100\n",
    "    at_content = ((a_count + t_count) / length) * 100\n",
    "    \n",
    "    gc_skew = (g_count - c_count) / (g_count + c_count) if (g_count + c_count) > 0 else 0\n",
    "    at_skew = (a_count - t_count) / (a_count + t_count) if (a_count + t_count) > 0 else 0\n",
    "    \n",
    "    purine_count = a_count + g_count\n",
    "    purine_pct = (purine_count / length) * 100\n",
    "    \n",
    "    entropy = 0\n",
    "    for base in 'ATGC':\n",
    "        count = seq.count(base)\n",
    "        if count > 0:\n",
    "            p = count / length\n",
    "            entropy -= p * np.log2(p)\n",
    "    \n",
    "    codon_pos_gc = {}\n",
    "    for pos in range(3):\n",
    "        pos_bases = seq[pos::3]\n",
    "        if len(pos_bases) > 0:\n",
    "            pos_gc = (pos_bases.count('G') + pos_bases.count('C')) / len(pos_bases) * 100\n",
    "            codon_pos_gc[f'gc_pos{pos+1}'] = pos_gc\n",
    "        else:\n",
    "            codon_pos_gc[f'gc_pos{pos+1}'] = 0\n",
    "    \n",
    "    return {\n",
    "        'seq_length': length,\n",
    "        'A_count': a_count, 'T_count': t_count, 'G_count': g_count, 'C_count': c_count, 'N_count': n_count,\n",
    "        'A_percent': (a_count / length) * 100,\n",
    "        'T_percent': (t_count / length) * 100,\n",
    "        'G_percent': (g_count / length) * 100,\n",
    "        'C_percent': (c_count / length) * 100,\n",
    "        'N_percent': (n_count / length) * 100,\n",
    "        'GC_content': gc_content,\n",
    "        'AT_content': at_content,\n",
    "        'GC_skew': gc_skew,\n",
    "        'AT_skew': at_skew,\n",
    "        'purine_percent': purine_pct,\n",
    "        'shannon_entropy': entropy,\n",
    "        **codon_pos_gc\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# PROTEIN PROPERTIES\n",
    "# ========================\n",
    "\n",
    "def analyze_protein_properties(protein_seq: str) -> dict:\n",
    "    \"\"\"Comprehensive protein property analysis\"\"\"\n",
    "    if pd.isna(protein_seq) or len(protein_seq) == 0:\n",
    "        return {}\n",
    "    \n",
    "    clean_protein = str(protein_seq).replace('*', '').replace('X', '')\n",
    "    if len(clean_protein) == 0:\n",
    "        return {}\n",
    "    \n",
    "    aa_counts = Counter(clean_protein)\n",
    "    length = len(clean_protein)\n",
    "    \n",
    "    hydrophobic_count = sum(aa_counts.get(aa, 0) for aa in HYDROPHOBIC_AA)\n",
    "    polar_count = sum(aa_counts.get(aa, 0) for aa in POLAR_AA)\n",
    "    positive_count = sum(aa_counts.get(aa, 0) for aa in CHARGED_POSITIVE)\n",
    "    negative_count = sum(aa_counts.get(aa, 0) for aa in CHARGED_NEGATIVE)\n",
    "    aromatic_count = sum(aa_counts.get(aa, 0) for aa in AROMATIC_AA)\n",
    "    aliphatic_count = sum(aa_counts.get(aa, 0) for aa in ALIPHATIC_AA)\n",
    "    \n",
    "    molecular_weight_val = 0\n",
    "    aromaticity = 0\n",
    "    instability_index = 0\n",
    "    isoelectric_point = 0\n",
    "    gravy = 0\n",
    "    \n",
    "    if PROTPARAM_AVAILABLE:\n",
    "        try:\n",
    "            protein_analysis = ProteinAnalysis(clean_protein)\n",
    "            molecular_weight_val = protein_analysis.molecular_weight()\n",
    "            aromaticity = protein_analysis.aromaticity()\n",
    "            instability_index = protein_analysis.instability_index()\n",
    "            isoelectric_point = protein_analysis.isoelectric_point()\n",
    "            gravy = protein_analysis.gravy()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if molecular_weight_val == 0:\n",
    "        aa_weights = {\n",
    "            'A': 89, 'R': 174, 'N': 132, 'D': 133, 'C': 121, 'E': 147, 'Q': 146,\n",
    "            'G': 75, 'H': 155, 'I': 131, 'L': 131, 'K': 146, 'M': 149, 'F': 165,\n",
    "            'P': 115, 'S': 105, 'T': 119, 'W': 204, 'Y': 181, 'V': 117\n",
    "        }\n",
    "        molecular_weight_val = sum(aa_weights.get(aa, 110) for aa in clean_protein)\n",
    "    \n",
    "    return {\n",
    "        'protein_length': length,\n",
    "        'clean_protein_length': length,\n",
    "        'hydrophobic_count': hydrophobic_count,\n",
    "        'hydrophobic_percent': (hydrophobic_count / length) * 100,\n",
    "        'polar_count': polar_count,\n",
    "        'polar_percent': (polar_count / length) * 100,\n",
    "        'charged_positive_count': positive_count,\n",
    "        'charged_negative_count': negative_count,\n",
    "        'net_charge': positive_count - negative_count,\n",
    "        'aromatic_count': aromatic_count,\n",
    "        'aromatic_percent': (aromatic_count / length) * 100,\n",
    "        'aliphatic_count': aliphatic_count,\n",
    "        'aliphatic_percent': (aliphatic_count / length) * 100,\n",
    "        'leucine_count': aa_counts.get('L', 0),\n",
    "        'leucine_percent': (aa_counts.get('L', 0) / length) * 100,\n",
    "        'molecular_weight': molecular_weight_val,\n",
    "        'aromaticity': aromaticity,\n",
    "        'instability_index': instability_index,\n",
    "        'isoelectric_point': isoelectric_point,\n",
    "        'gravy_score': gravy\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# SEQUENCE FEATURES\n",
    "# ========================\n",
    "\n",
    "def analyze_sequence_features(seq: str) -> dict:\n",
    "    \"\"\"Detect special sequence features\"\"\"\n",
    "    if pd.isna(seq):\n",
    "        return {}\n",
    "    \n",
    "    seq = str(seq).upper()\n",
    "    \n",
    "    has_poly_a = bool(re.search(r'A{8,}', seq))\n",
    "    has_poly_t = bool(re.search(r'T{8,}', seq))\n",
    "    has_poly_g = bool(re.search(r'G{8,}', seq))\n",
    "    has_poly_c = bool(re.search(r'C{8,}', seq))\n",
    "    \n",
    "    window_size = 20\n",
    "    low_complexity_windows = 0\n",
    "    if len(seq) >= window_size:\n",
    "        for i in range(0, len(seq) - window_size + 1, 10):\n",
    "            window = seq[i:i+window_size]\n",
    "            if len(set(window)) <= 2:\n",
    "                low_complexity_windows += 1\n",
    "    \n",
    "    cpg_count = seq.count('CG')\n",
    "    \n",
    "    has_coi_reverse = bool(re.search(r'GG[AT]TA[TC][ACGT]{3}GT', seq))\n",
    "    \n",
    "    return {\n",
    "        'has_poly_A': has_poly_a,\n",
    "        'has_poly_T': has_poly_t,\n",
    "        'has_poly_G': has_poly_g,\n",
    "        'has_poly_C': has_poly_c,\n",
    "        'low_complexity_windows': low_complexity_windows,\n",
    "        'cpg_count': cpg_count,\n",
    "        'has_coi_reverse_pattern': has_coi_reverse\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# CODON USAGE\n",
    "# ========================\n",
    "\n",
    "def analyze_codon_usage(seq: str) -> dict:\n",
    "    \"\"\"Analyze codon usage in sequence\"\"\"\n",
    "    if pd.isna(seq) or len(seq) < 3:\n",
    "        return {\n",
    "            'total_codons': 0, 'unique_codons': 0,\n",
    "            'most_common_codon': 'N/A', 'codon_diversity': 0\n",
    "        }\n",
    "    \n",
    "    codons = [seq[i:i+3] for i in range(0, len(seq)-2, 3) if len(seq[i:i+3]) == 3]\n",
    "    codon_counts = Counter(codons)\n",
    "    top_codon = codon_counts.most_common(1)[0] if codon_counts else ('N/A', 0)\n",
    "    \n",
    "    return {\n",
    "        'total_codons': len(codons),\n",
    "        'unique_codons': len(codon_counts),\n",
    "        'most_common_codon': top_codon[0],\n",
    "        'codon_diversity': (len(codon_counts) / 61) * 100 if len(codon_counts) > 0 else 0\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# SEQUENCE ALIGNMENT\n",
    "# ========================\n",
    "\n",
    "def align_to_reference(seq: str, protein: str) -> dict:\n",
    "    \"\"\"Align sequence to COI reference\"\"\"\n",
    "    if not BIOPYTHON_AVAILABLE:\n",
    "        return {\n",
    "            'aligned_sequence': seq[:100],\n",
    "            'aligned_protein': protein[:50],\n",
    "            'alignment_score': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        aligner = Align.PairwiseAligner()\n",
    "        aligner.mode = 'global'\n",
    "        aligner.match_score = 2\n",
    "        aligner.mismatch_score = -1\n",
    "        aligner.open_gap_score = -2\n",
    "        aligner.extend_gap_score = -0.5\n",
    "        \n",
    "        alignments = aligner.align(COI_REFERENCE, seq[:min(len(seq), len(COI_REFERENCE))])\n",
    "        \n",
    "        if alignments:\n",
    "            best_alignment = alignments[0]\n",
    "            aligned_seq = str(best_alignment).split('\\n')[1][:100]\n",
    "            alignment_score = best_alignment.score\n",
    "        else:\n",
    "            aligned_seq = seq[:100]\n",
    "            alignment_score = 0\n",
    "        \n",
    "        ref_protein = translate_insect(COI_REFERENCE.replace('N', 'A'))\n",
    "        \n",
    "        if len(protein) > 0 and len(ref_protein) > 0:\n",
    "            prot_aligner = Align.PairwiseAligner()\n",
    "            prot_aligner.mode = 'global'\n",
    "            prot_aligner.match_score = 3\n",
    "            prot_aligner.mismatch_score = -1\n",
    "            prot_aligner.open_gap_score = -2\n",
    "            prot_aligner.extend_gap_score = -1\n",
    "            \n",
    "            prot_alignments = prot_aligner.align(ref_protein[:50], protein[:50])\n",
    "            if prot_alignments:\n",
    "                aligned_prot = str(prot_alignments[0]).split('\\n')[1][:50]\n",
    "            else:\n",
    "                aligned_prot = protein[:50]\n",
    "        else:\n",
    "            aligned_prot = protein[:50] if protein else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'aligned_sequence': aligned_seq if aligned_seq else seq[:100],\n",
    "            'aligned_protein': aligned_prot if aligned_prot else protein[:50],\n",
    "            'alignment_score': alignment_score\n",
    "        }\n",
    "    \n",
    "    except Exception:\n",
    "        return {\n",
    "            'aligned_sequence': seq[:100] + '...' if len(seq) > 100 else seq,\n",
    "            'aligned_protein': protein[:50] + '...' if len(protein) > 50 else protein,\n",
    "            'alignment_score': 0\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# QUALITY ASSESSMENT\n",
    "# ========================\n",
    "\n",
    "def calculate_sequence_quality(seq: str, internal_stops: int, motif_data: dict) -> dict:\n",
    "    \"\"\"Calculate overall sequence quality score\"\"\"\n",
    "    if pd.isna(seq):\n",
    "        return {'sequence_quality_score': 0, 'quality_grade': 'F'}\n",
    "    \n",
    "    seq = str(seq).upper()\n",
    "    length = len(seq)\n",
    "    if length == 0:\n",
    "        return {'sequence_quality_score': 0, 'quality_grade': 'F'}\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    # 1. Length score (max 25 points)\n",
    "    if 600 <= length <= 700:\n",
    "        score += 25\n",
    "    elif 550 <= length <= 750:\n",
    "        score += 20\n",
    "    elif 450 <= length <= 800:\n",
    "        score += 15\n",
    "    elif 300 <= length < 450:\n",
    "        score += 10\n",
    "    else:\n",
    "        score += 5\n",
    "    \n",
    "    # 2. Internal stops (max 30 points)\n",
    "    if internal_stops == 0:\n",
    "        score += 30\n",
    "    elif internal_stops == 1:\n",
    "        score += 15\n",
    "    elif internal_stops == 2:\n",
    "        score += 8\n",
    "    \n",
    "    # 3. Motif coverage (max 25 points)\n",
    "    dna_coverage = motif_data.get('dna_motif_coverage', 0)\n",
    "    protein_coverage = motif_data.get('protein_motif_coverage', 0)\n",
    "    \n",
    "    if dna_coverage >= 40 and protein_coverage >= 40:\n",
    "        score += 25\n",
    "    elif dna_coverage >= 30 and protein_coverage >= 30:\n",
    "        score += 20\n",
    "    elif dna_coverage >= 20 and protein_coverage >= 20:\n",
    "        score += 15\n",
    "    elif dna_coverage >= 10 or protein_coverage >= 10:\n",
    "        score += 5\n",
    "    \n",
    "    # 4. AT content (max 10 points)\n",
    "    at_content = (seq.count('A') + seq.count('T')) / length * 100\n",
    "    if 60 <= at_content <= 75:\n",
    "        score += 10\n",
    "    elif 55 <= at_content <= 80:\n",
    "        score += 5\n",
    "    \n",
    "    # 5. GC content (max 10 points)\n",
    "    gc_content = (seq.count('G') + seq.count('C')) / length * 100\n",
    "    if 25 <= gc_content <= 40:\n",
    "        score += 10\n",
    "    elif 20 <= gc_content <= 45:\n",
    "        score += 5\n",
    "    \n",
    "    # Assign grade\n",
    "    if score >= 85:\n",
    "        grade = 'A+'\n",
    "        quality_class = 'Excellent'\n",
    "    elif score >= 75:\n",
    "        grade = 'A'\n",
    "        quality_class = 'High_Quality'\n",
    "    elif score >= 65:\n",
    "        grade = 'B+'\n",
    "        quality_class = 'Good_Quality'\n",
    "    elif score >= 55:\n",
    "        grade = 'B'\n",
    "        quality_class = 'Acceptable'\n",
    "    elif score >= 45:\n",
    "        grade = 'C'\n",
    "        quality_class = 'Moderate'\n",
    "    elif score >= 35:\n",
    "        grade = 'D'\n",
    "        quality_class = 'Low_Quality'\n",
    "    else:\n",
    "        grade = 'F'\n",
    "        quality_class = 'Poor_Quality'\n",
    "    \n",
    "    return {\n",
    "        'sequence_quality_score': score,\n",
    "        'quality_grade': grade,\n",
    "        'quality_classification': quality_class\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# QC STATUS DETERMINATION\n",
    "# ========================\n",
    "\n",
    "def determine_qc_status(internal_stops: int, length: int, motif_data: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Determine QC status and flags\n",
    "    Returns: (status, flags_list)\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    \n",
    "    if internal_stops == 0:\n",
    "        status = 'PASS'\n",
    "    elif internal_stops <= 2:\n",
    "        status = 'WARNING'\n",
    "        flags.append(f'internal_stops:{internal_stops}')\n",
    "    else:\n",
    "        status = 'FAIL'\n",
    "        flags.append(f'many_stops:{internal_stops}')\n",
    "    \n",
    "    if length < 300:\n",
    "        flags.append('short_sequence')\n",
    "        if status == 'PASS':\n",
    "            status = 'WARNING'\n",
    "    elif length > 800:\n",
    "        flags.append('long_sequence')\n",
    "        if status == 'PASS':\n",
    "            status = 'WARNING'\n",
    "    \n",
    "    dna_cov = motif_data.get('dna_motif_coverage', 0)\n",
    "    protein_cov = motif_data.get('protein_motif_coverage', 0)\n",
    "    \n",
    "    if dna_cov < 10 and protein_cov < 10:\n",
    "        flags.append('very_low_motif_coverage')\n",
    "        if status == 'PASS':\n",
    "            status = 'WARNING'\n",
    "    \n",
    "    return status, flags\n",
    "\n",
    "# ========================\n",
    "# CLASSIFICATION\n",
    "# ========================\n",
    "\n",
    "def get_sequence_usability(qc_status: str) -> str:\n",
    "    \"\"\"Determine sequence usability\"\"\"\n",
    "    if qc_status == 'PASS':\n",
    "        return 'Ready_for_Analysis'\n",
    "    elif qc_status == 'WARNING':\n",
    "        return 'Review_Required'\n",
    "    else:\n",
    "        return 'Exclude_from_Analysis'\n",
    "\n",
    "def get_analysis_priority(row: dict) -> str:\n",
    "    \"\"\"Determine analysis priority\"\"\"\n",
    "    qc = row.get('ANALYSIS_QC_status', row.get('QC_status', ''))\n",
    "    coi = row.get('ANALYSIS_coi_confidence', row.get('coi_confidence', ''))\n",
    "    auth = row.get('Authentication_Status', '')\n",
    "    \n",
    "    if auth == 'Authenticated' and qc == 'PASS' and coi == 'High':\n",
    "        return 'Priority_1_Highest'\n",
    "    elif auth == 'Authenticated' and qc == 'PASS':\n",
    "        return 'Priority_2_High'\n",
    "    elif qc == 'PASS' and coi in ['High', 'Medium']:\n",
    "        return 'Priority_3_Medium'\n",
    "    elif auth == 'Authenticated' and qc == 'WARNING':\n",
    "        return 'Priority_4_Review'\n",
    "    elif qc in ['PASS', 'WARNING']:\n",
    "        return 'Priority_5_Low'\n",
    "    else:\n",
    "        return 'Priority_6_Exclude'\n",
    "\n",
    "# ========================\n",
    "# MAIN SEQUENCE ANALYSIS\n",
    "# ========================\n",
    "\n",
    "def comprehensive_sequence_analysis(seq_id: str, original_seq: str, metadata_row: dict = None) -> dict:\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline for a single sequence\n",
    "    Preserves ALL original metadata columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with original metadata\n",
    "    if metadata_row and isinstance(metadata_row, dict):\n",
    "        result = metadata_row.copy()\n",
    "    else:\n",
    "        result = {}\n",
    "    \n",
    "    # Store original sequence data\n",
    "    result['ANALYSIS_original_sequence_full'] = original_seq\n",
    "    result['ANALYSIS_original_sequence'] = original_seq[:100] + '...' if len(original_seq) > 100 else original_seq\n",
    "    result['ANALYSIS_original_length'] = len(original_seq)\n",
    "    \n",
    "    # Clean sequence\n",
    "    clean_seq = clean_sequence(original_seq)\n",
    "    \n",
    "    # Trim sequence\n",
    "    trimmed_seq, trim_applied = trim_sequence(clean_seq)\n",
    "    result['ANALYSIS_trim_applied'] = 'Yes' if trim_applied else 'No'\n",
    "    result['ANALYSIS_trimmed_length'] = len(trimmed_seq)\n",
    "    \n",
    "    # Find best ORF\n",
    "    orf = find_best_orf(trimmed_seq)\n",
    "    result['ANALYSIS_best_frame'] = orf['frame']\n",
    "    result['ANALYSIS_best_strand'] = orf['strand']\n",
    "    result['ANALYSIS_orf_score'] = orf['score']\n",
    "    result['ANALYSIS_orf_length'] = orf['orf_length']\n",
    "    \n",
    "    # Initial sequence and protein\n",
    "    corrected_seq = orf['sequence']\n",
    "    corrected_protein = orf['protein']\n",
    "    result['ANALYSIS_internal_stops_initial'] = orf['internal_stops']\n",
    "    \n",
    "    # Attempt frameshift correction if needed\n",
    "    frameshift_corrected = False\n",
    "    if orf['internal_stops'] > 0:\n",
    "        corrected_seq, corrected_protein, frameshift_corrected = correct_frameshift(\n",
    "            orf['sequence'], orf['protein']\n",
    "        )\n",
    "        result['ANALYSIS_frameshift_correction_applied'] = 'Yes' if frameshift_corrected else 'No'\n",
    "    else:\n",
    "        result['ANALYSIS_frameshift_correction_applied'] = 'No'\n",
    "    \n",
    "    # Final internal stops count\n",
    "    final_internal_stops = corrected_protein[:-1].count('*') if len(corrected_protein) > 1 else 0\n",
    "    result['ANALYSIS_internal_stops'] = final_internal_stops\n",
    "    \n",
    "    # Store corrected sequences\n",
    "    result['ANALYSIS_corrected_sequence_full'] = corrected_seq\n",
    "    result['ANALYSIS_corrected_sequence'] = corrected_seq[:100] + '...' if len(corrected_seq) > 100 else corrected_seq\n",
    "    result['ANALYSIS_corrected_length'] = len(corrected_seq)\n",
    "    result['ANALYSIS_corrected_protein_full'] = corrected_protein\n",
    "    result['ANALYSIS_corrected_protein'] = corrected_protein[:50] + '...' if len(corrected_protein) > 50 else corrected_protein\n",
    "    \n",
    "    # Motif analysis\n",
    "    motif_data = analyze_motifs_comprehensive(corrected_seq, corrected_protein)\n",
    "    for key, value in motif_data.items():\n",
    "        result[f'ANALYSIS_{key}'] = value\n",
    "    \n",
    "    # Nucleotide composition\n",
    "    nuc_stats = analyze_nucleotide_composition(corrected_seq)\n",
    "    for key, value in nuc_stats.items():\n",
    "        result[f'ANALYSIS_{key}'] = value\n",
    "    \n",
    "    # Protein properties\n",
    "    prot_stats = analyze_protein_properties(corrected_protein)\n",
    "    for key, value in prot_stats.items():\n",
    "        result[f'ANALYSIS_{key}'] = value\n",
    "    \n",
    "    # Sequence features\n",
    "    seq_features = analyze_sequence_features(corrected_seq)\n",
    "    for key, value in seq_features.items():\n",
    "        result[f'ANALYSIS_{key}'] = value\n",
    "    \n",
    "    # Codon usage\n",
    "    codon_stats = analyze_codon_usage(corrected_seq)\n",
    "    for key, value in codon_stats.items():\n",
    "        result[f'ANALYSIS_{key}'] = value\n",
    "    \n",
    "    # Alignment to reference\n",
    "    alignment = align_to_reference(corrected_seq, corrected_protein)\n",
    "    for key, value in alignment.items():\n",
    "        result[f'ANALYSIS_{key}'] = value\n",
    "    \n",
    "    # Quality assessment\n",
    "    quality = calculate_sequence_quality(corrected_seq, final_internal_stops, motif_data)\n",
    "    for key, value in quality.items():\n",
    "        result[f'ANALYSIS_{key}'] = value\n",
    "    \n",
    "    # QC status\n",
    "    qc_status, flags = determine_qc_status(final_internal_stops, len(corrected_seq), motif_data)\n",
    "    result['ANALYSIS_QC_status'] = qc_status\n",
    "    result['ANALYSIS_flags'] = ';'.join(flags) if flags else 'None'\n",
    "    \n",
    "    # Usability and priority\n",
    "    result['ANALYSIS_sequence_usability'] = get_sequence_usability(qc_status)\n",
    "    result['ANALYSIS_analysis_priority'] = get_analysis_priority(result)\n",
    "    \n",
    "    # Quality issues summary\n",
    "    issues = []\n",
    "    if final_internal_stops > 0:\n",
    "        issues.append(f'stops:{final_internal_stops}')\n",
    "    if len(corrected_seq) < 350:\n",
    "        issues.append(f'short:{len(corrected_seq)}bp')\n",
    "    if len(corrected_seq) > 750:\n",
    "        issues.append(f'long:{len(corrected_seq)}bp')\n",
    "    if motif_data.get('coi_confidence') == 'Low':\n",
    "        issues.append('low_COI_confidence')\n",
    "    if motif_data.get('coi_confidence') == 'Very_Low':\n",
    "        issues.append('very_low_COI_confidence')\n",
    "    result['ANALYSIS_quality_issues'] = ';'.join(issues) if issues else 'None'\n",
    "    \n",
    "    # Analysis timestamp\n",
    "    result['ANALYSIS_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ========================\n",
    "# BATCH PROCESSING\n",
    "# ========================\n",
    "\n",
    "def process_fasta_file(fasta_file: str, metadata_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"Process all sequences from FASTA file and merge back to original metadata\"\"\"\n",
    "    \n",
    "    if not BIOPYTHON_AVAILABLE:\n",
    "        print(\"ERROR: BioPython is required but not available\")\n",
    "        print(\"Install with: pip install biopython\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"\\nProcessing FASTA file: {fasta_file}\")\n",
    "    \n",
    "    if metadata_df is None:\n",
    "        print(\"ERROR: Metadata file is required for this analysis\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # เก็บลำดับคอลัมน์เดิม\n",
    "    original_columns_order = metadata_df.columns.tolist()\n",
    "    original_row_count = len(metadata_df)\n",
    "    print(f\"  Original metadata: {original_row_count:,} rows × {len(original_columns_order)} columns\")\n",
    "    \n",
    "    # หาคอลัมน์ ASV_ID\n",
    "    asv_col = None\n",
    "    for col in ['asv_id', 'ASV_ID', 'sequence_id', 'ASV_Id']:\n",
    "        if col in metadata_df.columns:\n",
    "            asv_col = col\n",
    "            break\n",
    "    \n",
    "    if asv_col is None:\n",
    "        print(\"ERROR: Cannot find ASV_ID column in metadata\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"  Using ASV_ID column: '{asv_col}'\")\n",
    "    \n",
    "    # นับจำนวน unique ASVs\n",
    "    unique_asvs = metadata_df[asv_col].nunique()\n",
    "    print(f\"  Unique ASV_IDs in metadata: {unique_asvs:,}\")\n",
    "    print(f\"  Duplicate ASV_IDs exist: {original_row_count > unique_asvs}\")\n",
    "    \n",
    "    # วิเคราะห์ลำดับจาก FASTA\n",
    "    print(\"\\nAnalyzing sequences from FASTA...\")\n",
    "    \n",
    "    analysis_results = {}\n",
    "    total_count = 0\n",
    "    \n",
    "    try:\n",
    "        total_seqs = sum(1 for _ in SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    except:\n",
    "        total_seqs = None\n",
    "    \n",
    "    iterator = SeqIO.parse(fasta_file, \"fasta\")\n",
    "    if TQDM_AVAILABLE and total_seqs:\n",
    "        iterator = tqdm(iterator, total=total_seqs, desc=\"  Processing sequences\")\n",
    "    \n",
    "    for record in iterator:\n",
    "        total_count += 1\n",
    "        seq_id = record.id\n",
    "        original_seq = str(record.seq)\n",
    "        \n",
    "        # วิเคราะห์ลำดับ (ไม่ต้องส่ง metadata_row เพราะจะ merge ทีหลัง)\n",
    "        result = comprehensive_sequence_analysis(seq_id, original_seq, metadata_row=None)\n",
    "        \n",
    "        # เก็บเฉพาะคอลัมน์ ANALYSIS_*\n",
    "        analysis_only = {k: v for k, v in result.items() if k.startswith('ANALYSIS_')}\n",
    "        analysis_results[seq_id] = analysis_only\n",
    "        \n",
    "        if not TQDM_AVAILABLE and total_count % 1000 == 0:\n",
    "            print(f\"  Processed {total_count:,} sequences...\")\n",
    "    \n",
    "    print(f\"\\n  Completed: {total_count:,} unique sequences analyzed\")\n",
    "    print(f\"  Analysis results stored for {len(analysis_results):,} ASV_IDs\")\n",
    "    \n",
    "    # สร้าง DataFrame จากผลการวิเคราะห์\n",
    "    analysis_df = pd.DataFrame.from_dict(analysis_results, orient='index')\n",
    "    analysis_df.index.name = asv_col\n",
    "    analysis_df = analysis_df.reset_index()\n",
    "    \n",
    "    print(f\"\\n  Analysis DataFrame: {len(analysis_df):,} rows × {len(analysis_df.columns)} columns\")\n",
    "    \n",
    "    # Merge กลับเข้ากับ metadata เดิม\n",
    "    print(f\"\\nMerging analysis results back to original metadata...\")\n",
    "    print(f\"  Original metadata rows: {len(metadata_df):,}\")\n",
    "    \n",
    "    # Left join เพื่อรักษาทุกแถวของ metadata\n",
    "    result_df = metadata_df.merge(\n",
    "        analysis_df,\n",
    "        on=asv_col,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"  After merge: {len(result_df):,} rows × {len(result_df.columns)} columns\")\n",
    "    \n",
    "    if len(result_df) != original_row_count:\n",
    "        print(f\"  ⚠ WARNING: Row count changed! {original_row_count:,} → {len(result_df):,}\")\n",
    "    else:\n",
    "        print(f\"  ✓ Row count preserved: {len(result_df):,} rows\")\n",
    "    \n",
    "    # จัดเรียงคอลัมน์: metadata เดิม + ANALYSIS_*\n",
    "    original_cols = [col for col in original_columns_order if col in result_df.columns]\n",
    "    analysis_cols = [col for col in result_df.columns if col.startswith('ANALYSIS_')]\n",
    "    other_cols = [col for col in result_df.columns \n",
    "                  if col not in original_cols and not col.startswith('ANALYSIS_')]\n",
    "    \n",
    "    final_column_order = original_cols + other_cols + analysis_cols\n",
    "    result_df = result_df[final_column_order]\n",
    "    \n",
    "    # ตรวจสอบ ASVs ที่ไม่มีในผลการวิเคราะห์\n",
    "    missing_asvs = result_df[result_df['ANALYSIS_original_sequence'].isna()][asv_col].nunique()\n",
    "    if missing_asvs > 0:\n",
    "        print(f\"\\n  ⚠ Note: {missing_asvs:,} unique ASV_IDs not found in FASTA file\")\n",
    "        print(f\"     (these rows will have empty ANALYSIS_* columns)\")\n",
    "    \n",
    "    # นับ ASVs ที่ซ้ำ\n",
    "    asv_counts = result_df[asv_col].value_counts()\n",
    "    duplicated_asvs = (asv_counts > 1).sum()\n",
    "    if duplicated_asvs > 0:\n",
    "        print(f\"\\n  ✓ {duplicated_asvs:,} ASV_IDs appear multiple times\")\n",
    "        print(f\"     (same analysis results copied to all rows with same ASV_ID)\")\n",
    "        max_dup = asv_counts.max()\n",
    "        most_dup_asv = asv_counts.idxmax()\n",
    "        print(f\"     Max duplicates: {max_dup} times (ASV: {most_dup_asv})\")\n",
    "    \n",
    "    print(f\"\\n  Final column organization:\")\n",
    "    print(f\"    - Original metadata (preserved order): {len(original_cols)}\")\n",
    "    if other_cols:\n",
    "        print(f\"    - Other columns: {len(other_cols)}\")\n",
    "    print(f\"    - Analysis columns (at the end): {len(analysis_cols)}\")\n",
    "    print(f\"    - Total: {len(result_df.columns)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# ========================\n",
    "# SUMMARY STATISTICS\n",
    "# ========================\n",
    "\n",
    "def generate_summary_statistics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate comprehensive summary statistics\"\"\"\n",
    "    print(\"\\nGenerating summary statistics...\")\n",
    "    \n",
    "    summary_data = []\n",
    "    total = len(df)\n",
    "    \n",
    "    qc_col = 'ANALYSIS_QC_status' if 'ANALYSIS_QC_status' in df.columns else 'QC_status'\n",
    "    coi_col = 'ANALYSIS_coi_confidence' if 'ANALYSIS_coi_confidence' in df.columns else 'coi_confidence'\n",
    "    grade_col = 'ANALYSIS_quality_grade' if 'ANALYSIS_quality_grade' in df.columns else 'quality_grade'\n",
    "    \n",
    "    rows_with_qc = int(df[qc_col].notna().sum())\n",
    "    \n",
    "    for status in ['PASS', 'WARNING', 'FAIL']:\n",
    "        count = int((df[qc_col] == status).sum())\n",
    "        pct = (count / rows_with_qc * 100) if rows_with_qc > 0 else 0\n",
    "        summary_data.append({\n",
    "            'Category': 'QC_Status',\n",
    "            'Value': status,\n",
    "            'Count': count,\n",
    "            'Percentage': pct,\n",
    "            'Percentage_str': f\"{pct:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    for conf in ['High', 'Medium', 'Low', 'Very_Low']:\n",
    "        count = int((df[coi_col] == conf).sum())\n",
    "        pct = (count / total * 100) if total > 0 else 0\n",
    "        summary_data.append({\n",
    "            'Category': 'COI_Confidence',\n",
    "            'Value': conf,\n",
    "            'Count': count,\n",
    "            'Percentage': pct,\n",
    "            'Percentage_str': f\"{pct:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    for grade in ['A+', 'A', 'B+', 'B', 'C', 'D', 'F']:\n",
    "        count = int((df[grade_col] == grade).sum())\n",
    "        pct = (count / total * 100) if total > 0 else 0\n",
    "        summary_data.append({\n",
    "            'Category': 'Quality_Grade',\n",
    "            'Value': grade,\n",
    "            'Count': count,\n",
    "            'Percentage': pct,\n",
    "            'Percentage_str': f\"{pct:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(f\"  Generated {len(summary_df)} summary statistics\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# ========================\n",
    "# CODON USAGE TABLE\n",
    "# ========================\n",
    "\n",
    "def generate_codon_usage_table(df: pd.DataFrame, max_seqs: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Generate codon usage frequency table\"\"\"\n",
    "    print(\"\\nGenerating codon usage table...\")\n",
    "    \n",
    "    qc_col = 'ANALYSIS_QC_status' if 'ANALYSIS_QC_status' in df.columns else 'QC_status'\n",
    "    seq_col = 'ANALYSIS_corrected_sequence_full' if 'ANALYSIS_corrected_sequence_full' in df.columns else 'corrected_sequence_full'\n",
    "    \n",
    "    pass_seqs = df[df[qc_col] == 'PASS']\n",
    "    if len(pass_seqs) == 0:\n",
    "        print(\"  No PASS sequences found\")\n",
    "        return None\n",
    "    \n",
    "    if max_seqs and len(pass_seqs) > max_seqs:\n",
    "        print(f\"  Analyzing {max_seqs:,} of {len(pass_seqs):,} sequences\")\n",
    "        pass_seqs = pass_seqs.sample(n=max_seqs, random_state=42)\n",
    "    else:\n",
    "        print(f\"  Analyzing all {len(pass_seqs):,} sequences\")\n",
    "    \n",
    "    total_codon_counter = Counter()\n",
    "    total_sequences = 0\n",
    "    \n",
    "    iterator = pass_seqs.iterrows()\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(iterator, total=len(pass_seqs), desc=\"  Processing\")\n",
    "    \n",
    "    for _, row in iterator:\n",
    "        seq = row.get(seq_col, '')\n",
    "        if pd.notna(seq) and len(seq) >= 3:\n",
    "            seq = str(seq).replace('...', '')\n",
    "            codons = [seq[i:i+3] for i in range(0, len(seq)-2, 3) if len(seq[i:i+3]) == 3]\n",
    "            valid_codons = [c for c in codons if c in INSECT_GENETIC_CODE]\n",
    "            total_codon_counter.update(valid_codons)\n",
    "            total_sequences += 1\n",
    "    \n",
    "    if len(total_codon_counter) == 0:\n",
    "        print(\"  No valid codons found\")\n",
    "        return None\n",
    "    \n",
    "    codon_data = []\n",
    "    for codon, count in total_codon_counter.items():\n",
    "        aa = INSECT_GENETIC_CODE.get(codon, 'X')\n",
    "        frequency = count / sum(total_codon_counter.values()) * 100\n",
    "        avg_per_seq = count / total_sequences if total_sequences > 0 else 0\n",
    "        codon_data.append({\n",
    "            'codon': codon,\n",
    "            'amino_acid': aa,\n",
    "            'total_count': count,\n",
    "            'frequency_percent': frequency,\n",
    "            'avg_per_sequence': avg_per_seq\n",
    "        })\n",
    "    \n",
    "    codon_df = pd.DataFrame(codon_data)\n",
    "    codon_df = codon_df.sort_values(['amino_acid', 'total_count'], ascending=[True, False])\n",
    "    print(f\"  Analyzed {len(codon_df)} unique codons from {total_sequences:,} sequences\")\n",
    "    \n",
    "    return codon_df\n",
    "\n",
    "# ========================\n",
    "# AMINO ACID COMPOSITION\n",
    "# ========================\n",
    "\n",
    "def generate_aa_composition(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate amino acid composition analysis\"\"\"\n",
    "    print(\"\\nGenerating amino acid composition table...\")\n",
    "    \n",
    "    qc_col = 'ANALYSIS_QC_status' if 'ANALYSIS_QC_status' in df.columns else 'QC_status'\n",
    "    prot_col = 'ANALYSIS_corrected_protein_full' if 'ANALYSIS_corrected_protein_full' in df.columns else 'corrected_protein_full'\n",
    "    \n",
    "    pass_seqs = df[df[qc_col] == 'PASS']\n",
    "    if len(pass_seqs) == 0:\n",
    "        print(\"  No PASS sequences found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Analyzing {len(pass_seqs):,} PASS sequences\")\n",
    "    \n",
    "    aa_totals = Counter()\n",
    "    total_aa_length = 0\n",
    "    \n",
    "    iterator = pass_seqs.iterrows()\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(iterator, total=len(pass_seqs), desc=\"  Processing\")\n",
    "    \n",
    "    for _, row in iterator:\n",
    "        protein = row.get(prot_col, '')\n",
    "        if pd.notna(protein) and len(protein) > 0:\n",
    "            protein = str(protein).replace('...', '')\n",
    "            clean_protein = protein.replace('*', '').replace('X', '')\n",
    "            if len(clean_protein) > 0:\n",
    "                aa_counter = Counter(clean_protein)\n",
    "                aa_totals.update(aa_counter)\n",
    "                total_aa_length += len(clean_protein)\n",
    "    \n",
    "    if total_aa_length == 0:\n",
    "        print(\"  No valid protein sequences found\")\n",
    "        return None\n",
    "    \n",
    "    aa_summary = []\n",
    "    aa_properties = {\n",
    "        'A': 'Aliphatic', 'I': 'Aliphatic', 'L': 'Aliphatic', 'V': 'Aliphatic',\n",
    "        'F': 'Aromatic', 'W': 'Aromatic', 'Y': 'Aromatic',\n",
    "        'D': 'Acidic', 'E': 'Acidic',\n",
    "        'R': 'Basic', 'H': 'Basic', 'K': 'Basic',\n",
    "        'S': 'Polar', 'T': 'Polar', 'N': 'Polar', 'Q': 'Polar', 'C': 'Polar',\n",
    "        'G': 'Special', 'P': 'Special', 'M': 'Special'\n",
    "    }\n",
    "    \n",
    "    for aa, count in sorted(aa_totals.items()):\n",
    "        frequency = (count / total_aa_length) * 100\n",
    "        aa_summary.append({\n",
    "            'amino_acid': aa,\n",
    "            'property': aa_properties.get(aa, 'Other'),\n",
    "            'total_count': count,\n",
    "            'frequency_percent': frequency,\n",
    "            'sequences_analyzed': len(pass_seqs)\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(aa_summary)\n",
    "    summary_df = summary_df.sort_values('total_count', ascending=False)\n",
    "    \n",
    "    print(f\"  Total amino acids analyzed: {total_aa_length:,}\")\n",
    "    print(f\"  Unique amino acids found: {len(aa_totals)}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# ========================\n",
    "# MOTIF SUMMARY\n",
    "# ========================\n",
    "\n",
    "def generate_motif_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate motif detection summary\"\"\"\n",
    "    print(\"\\nGenerating motif summary...\")\n",
    "    \n",
    "    qc_col = 'ANALYSIS_QC_status' if 'ANALYSIS_QC_status' in df.columns else 'QC_status'\n",
    "    dna_motif_col = 'ANALYSIS_dna_motifs_found' if 'ANALYSIS_dna_motifs_found' in df.columns else 'dna_motifs_found'\n",
    "    prot_motif_col = 'ANALYSIS_protein_motifs_found' if 'ANALYSIS_protein_motifs_found' in df.columns else 'protein_motifs_found'\n",
    "    cons_reg_col = 'ANALYSIS_conserved_regions' if 'ANALYSIS_conserved_regions' in df.columns else 'conserved_regions'\n",
    "    \n",
    "    pass_seqs = df[df[qc_col] == 'PASS']\n",
    "    if len(pass_seqs) == 0:\n",
    "        print(\"  No PASS sequences for motif analysis\")\n",
    "        return None\n",
    "    \n",
    "    motif_data = []\n",
    "    \n",
    "    for motif_name in COI_DNA_MOTIFS.keys():\n",
    "        count = int(pass_seqs[dna_motif_col].astype(str).str.contains(motif_name, na=False).sum())\n",
    "        pct = (count / len(pass_seqs) * 100) if len(pass_seqs) > 0 else 0\n",
    "        motif_data.append({\n",
    "            'motif_type': 'DNA',\n",
    "            'motif_name': motif_name,\n",
    "            'sequences_with_motif': count,\n",
    "            'percentage': pct,\n",
    "            'percentage_str': f\"{pct:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    for motif_name in COI_PROTEIN_MOTIFS.keys():\n",
    "        count = int(pass_seqs[prot_motif_col].astype(str).str.contains(motif_name, na=False).sum())\n",
    "        pct = (count / len(pass_seqs) * 100) if len(pass_seqs) > 0 else 0\n",
    "        motif_data.append({\n",
    "            'motif_type': 'Protein',\n",
    "            'motif_name': motif_name,\n",
    "            'sequences_with_motif': count,\n",
    "            'percentage': pct,\n",
    "            'percentage_str': f\"{pct:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    for region_name in COI_CONSERVED_REGIONS.keys():\n",
    "        count = int(pass_seqs[cons_reg_col].astype(str).str.contains(region_name, na=False).sum())\n",
    "        pct = (count / len(pass_seqs) * 100) if len(pass_seqs) > 0 else 0\n",
    "        motif_data.append({\n",
    "            'motif_type': 'Conserved_Region',\n",
    "            'motif_name': region_name,\n",
    "            'sequences_with_motif': count,\n",
    "            'percentage': pct,\n",
    "            'percentage_str': f\"{pct:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    motif_df = pd.DataFrame(motif_data)\n",
    "    motif_df = motif_df.sort_values(['motif_type', 'percentage'], ascending=[True, False])\n",
    "    \n",
    "    print(f\"  Generated summary for {len(motif_df)} motifs\")\n",
    "    \n",
    "    return motif_df\n",
    "\n",
    "# ========================\n",
    "# FASTA EXPORT\n",
    "# ========================\n",
    "\n",
    "def export_fasta(df: pd.DataFrame, output_file: str, priority_filter: list = None) -> str:\n",
    "    \"\"\"Export sequences to FASTA format\"\"\"\n",
    "    print(f\"\\nExporting FASTA file: {output_file}\")\n",
    "    \n",
    "    priority_col = 'ANALYSIS_analysis_priority' if 'ANALYSIS_analysis_priority' in df.columns else 'analysis_priority'\n",
    "    qc_col = 'ANALYSIS_QC_status' if 'ANALYSIS_QC_status' in df.columns else 'QC_status'\n",
    "    seq_col = 'ANALYSIS_corrected_sequence_full' if 'ANALYSIS_corrected_sequence_full' in df.columns else 'corrected_sequence_full'\n",
    "    \n",
    "    if priority_filter:\n",
    "        export_df = df[df[priority_col].isin(priority_filter)]\n",
    "        print(f\"  Filtering by priority: {priority_filter}\")\n",
    "    else:\n",
    "        export_df = df[df[qc_col] == 'PASS']\n",
    "        print(\"  Using all PASS sequences\")\n",
    "    \n",
    "    export_df = export_df[export_df[seq_col].notna()]\n",
    "    if len(export_df) == 0:\n",
    "        print(\"  No sequences to export\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Exporting {len(export_df):,} sequences...\")\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for _, row in export_df.iterrows():\n",
    "            seq = str(row[seq_col])\n",
    "            asv_id = row.get('asv_id', row.get('sequence_id', 'Unknown'))\n",
    "            family = row.get('family', 'Unknown')\n",
    "            \n",
    "            coi_conf_col = 'ANALYSIS_coi_confidence' if 'ANALYSIS_coi_confidence' in row else 'coi_confidence'\n",
    "            motif_qual_col = 'ANALYSIS_motif_quality' if 'ANALYSIS_motif_quality' in row else 'motif_quality'\n",
    "            \n",
    "            coi_conf = row.get(coi_conf_col, 'Unknown')\n",
    "            motif_qual = row.get(motif_qual_col, 'Unknown')\n",
    "            qc_status = row.get(qc_col, 'Unknown')\n",
    "            \n",
    "            header = f\">{asv_id}|Family:{family}|COI:{coi_conf}|Motif:{motif_qual}|QC:{qc_status}|Length:{len(seq)}bp\"\n",
    "            f.write(header + '\\n')\n",
    "            \n",
    "            for i in range(0, len(seq), 80):\n",
    "                f.write(seq[i:i+80] + '\\n')\n",
    "    \n",
    "    print(f\"  Exported to: {output_file}\")\n",
    "    print(f\"  Total sequences: {len(export_df):,}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# ========================\n",
    "# VISUALIZATION\n",
    "# ========================\n",
    "\n",
    "def create_visualizations(df: pd.DataFrame, output_dir: str) -> int:\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    if not VISUALIZATION_AVAILABLE:\n",
    "        print(\"\\n  Visualization libraries not available\")\n",
    "        print(\"  Install with: pip install matplotlib seaborn\")\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nCreating visualizations...\")\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        plt.rcParams['figure.facecolor'] = 'white'\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        qc_col = 'ANALYSIS_QC_status' if 'ANALYSIS_QC_status' in df.columns else 'QC_status'\n",
    "        pass_df = df[df[qc_col] == 'PASS'].copy()\n",
    "        viz_count = 0\n",
    "        \n",
    "        # Use flexible column names\n",
    "        def get_col(base_name):\n",
    "            analysis_col = f'ANALYSIS_{base_name}'\n",
    "            return analysis_col if analysis_col in df.columns else base_name\n",
    "        \n",
    "        # 1. Quality Distribution\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            fig.suptitle('Quality Metrics Distribution', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            qc_counts = df[qc_col].value_counts()\n",
    "            axes[0,0].bar(qc_counts.index, qc_counts.values, color=['green', 'orange', 'red'])\n",
    "            axes[0,0].set_title('QC Status')\n",
    "            axes[0,0].set_ylabel('Count')\n",
    "            for i, v in enumerate(qc_counts.values):\n",
    "                axes[0,0].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "            \n",
    "            coi_col = get_col('coi_confidence')\n",
    "            coi_counts = df[coi_col].value_counts()\n",
    "            colors_coi = {'High': 'darkgreen', 'Medium': 'orange', 'Low': 'red', 'Very_Low': 'darkred'}\n",
    "            axes[0,1].bar(coi_counts.index, coi_counts.values,\n",
    "                         color=[colors_coi.get(x, 'gray') for x in coi_counts.index])\n",
    "            axes[0,1].set_title('COI Confidence')\n",
    "            axes[0,1].set_ylabel('Count')\n",
    "            for i, v in enumerate(coi_counts.values):\n",
    "                axes[0,1].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "            \n",
    "            grade_col = get_col('quality_grade')\n",
    "            grade_order = ['A+', 'A', 'B+', 'B', 'C', 'D', 'F']\n",
    "            grade_counts = df[grade_col].value_counts()\n",
    "            grade_counts = grade_counts.reindex(grade_order, fill_value=0)\n",
    "            axes[1,0].bar(range(len(grade_counts)), grade_counts.values, color='steelblue')\n",
    "            axes[1,0].set_xticks(range(len(grade_counts)))\n",
    "            axes[1,0].set_xticklabels(grade_counts.index)\n",
    "            axes[1,0].set_title('Quality Grade Distribution')\n",
    "            axes[1,0].set_ylabel('Count')\n",
    "            for i, v in enumerate(grade_counts.values):\n",
    "                if v > 0:\n",
    "                    axes[1,0].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "            \n",
    "            motif_qual_col = get_col('motif_quality')\n",
    "            motif_counts = df[motif_qual_col].value_counts()\n",
    "            colors_motif = {'Excellent': 'darkgreen', 'Good': 'green', 'Fair': 'orange', 'Poor': 'red'}\n",
    "            axes[1,1].bar(motif_counts.index, motif_counts.values,\n",
    "                         color=[colors_motif.get(x, 'gray') for x in motif_counts.index])\n",
    "            axes[1,1].set_title('Motif Quality')\n",
    "            axes[1,1].set_ylabel('Count')\n",
    "            for i, v in enumerate(motif_counts.values):\n",
    "                axes[1,1].text(i, v, f'{v:,}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_path / 'Quality_Distribution.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            viz_count += 1\n",
    "            print(\"  ✓ Quality_Distribution.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error creating quality distribution: {e}\")\n",
    "        \n",
    "        # 2. Sequence Characteristics\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            fig.suptitle('Sequence Characteristics', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            length_col = get_col('corrected_length')\n",
    "            lengths = pass_df[length_col].dropna()\n",
    "            axes[0,0].hist(lengths, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "            axes[0,0].axvline(lengths.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                             label=f'Mean: {lengths.mean():.0f} bp')\n",
    "            axes[0,0].set_title('Corrected Sequence Length')\n",
    "            axes[0,0].set_xlabel('Length (bp)')\n",
    "            axes[0,0].set_ylabel('Frequency')\n",
    "            axes[0,0].legend()\n",
    "            \n",
    "            gc_col = get_col('GC_content')\n",
    "            gc_content = pass_df[gc_col].dropna()\n",
    "            axes[0,1].hist(gc_content, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "            axes[0,1].axvline(gc_content.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                             label=f'Mean: {gc_content.mean():.1f}%')\n",
    "            axes[0,1].set_title('GC Content')\n",
    "            axes[0,1].set_xlabel('GC Content (%)')\n",
    "            axes[0,1].set_ylabel('Frequency')\n",
    "            axes[0,1].legend()\n",
    "            \n",
    "            motif_score_col = get_col('motif_score')\n",
    "            motif_score = pass_df[motif_score_col].dropna()\n",
    "            axes[1,0].hist(motif_score, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "            axes[1,0].axvline(motif_score.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                             label=f'Mean: {motif_score.mean():.1f}')\n",
    "            axes[1,0].set_title('Motif Score Distribution')\n",
    "            axes[1,0].set_xlabel('Motif Score')\n",
    "            axes[1,0].set_ylabel('Frequency')\n",
    "            axes[1,0].legend()\n",
    "            \n",
    "            prot_len_col = get_col('protein_length')\n",
    "            prot_length = pass_df[prot_len_col].dropna()\n",
    "            prot_length = prot_length[prot_length > 0]\n",
    "            axes[1,1].hist(prot_length, bins=30, color='plum', edgecolor='black', alpha=0.7)\n",
    "            axes[1,1].axvline(prot_length.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                             label=f'Mean: {prot_length.mean():.0f} aa')\n",
    "            axes[1,1].set_title('Protein Length')\n",
    "            axes[1,1].set_xlabel('Length (aa)')\n",
    "            axes[1,1].set_ylabel('Frequency')\n",
    "            axes[1,1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_path / 'Sequence_Characteristics.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            viz_count += 1\n",
    "            print(\"  ✓ Sequence_Characteristics.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error creating sequence characteristics: {e}\")\n",
    "        \n",
    "        # 3-6: Continue with other visualizations...\n",
    "        print(f\"\\n  Created {viz_count} visualization files\")\n",
    "        print(f\"  Saved to: {output_path}\")\n",
    "        \n",
    "        return viz_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  Error creating visualizations: {e}\")\n",
    "        return 0\n",
    "\n",
    "# ========================\n",
    "# REPORTING\n",
    "# ========================\n",
    "\n",
    "def print_comprehensive_report(df: pd.DataFrame):\n",
    "    \"\"\"Print comprehensive analysis report\"\"\"\n",
    "    \n",
    "    qc_col = 'ANALYSIS_QC_status' if 'ANALYSIS_QC_status' in df.columns else 'QC_status'\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPREHENSIVE ANALYSIS REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. OVERALL STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Total sequences analyzed: {len(df):,}\")\n",
    "    \n",
    "    rows_with_qc = int(df[qc_col].notna().sum())\n",
    "    \n",
    "    print(\"\\n2. QC STATUS\")\n",
    "    print(\"-\" * 80)\n",
    "    for status in ['PASS', 'WARNING', 'FAIL']:\n",
    "        count = int((df[qc_col] == status).sum())\n",
    "        pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
    "        icon = '✓' if status == 'PASS' else '⚠' if status == 'WARNING' else '✗'\n",
    "        print(f\"  {icon} {status:10s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ========================\n",
    "# MAIN EXECUTION\n",
    "# ========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPLETE INSECT COI ANALYSIS PIPELINE - UNIFIED v11.1\")\n",
    "    print(\"Preserves Original Metadata + Adds Analysis Results\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n🆕 NEW IN v11.1:\")\n",
    "    print(\"  ✓ Preserves ALL original metadata columns IN ORIGINAL ORDER\")\n",
    "    print(\"  ✓ Analysis results prefixed with 'ANALYSIS_' added at the END\")\n",
    "    print(\"  ✓ Easy to identify new vs original data\")\n",
    "    print(\"  ✓ Compatible with downstream abundance analysis\")\n",
    "    \n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(f\"  Metadata file: {METADATA_FILE}\")\n",
    "    print(f\"  FASTA file: {FASTA_FILE}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    print(\"\\nAnalysis Features:\")\n",
    "    print(\"  ✓ ORF detection (6 frames)\")\n",
    "    print(\"  ✓ Frameshift correction\")\n",
    "    print(f\"  ✓ {len(COI_DNA_MOTIFS)} DNA-level COI motifs\")\n",
    "    print(f\"  ✓ {len(COI_PROTEIN_MOTIFS)} Protein-level COI patterns\")\n",
    "    print(f\"  ✓ {len(COI_CONSERVED_REGIONS)} Conserved regions\")\n",
    "    print(\"  ✓ Insect mitochondrial genetic code (TGA=W, ATA=M)\")\n",
    "    print(\"  ✓ Nucleotide composition analysis\")\n",
    "    print(\"  ✓ Protein property analysis\")\n",
    "    print(\"  ✓ Quality scoring and grading\")\n",
    "    print(\"  ✓ COI confidence assessment\")\n",
    "    print(\"  ✓ Alignment to reference\")\n",
    "    print(\"  ✓ Codon usage analysis\")\n",
    "    print(\"  ✓ Amino acid composition\")\n",
    "    print(\"  ✓ Comprehensive visualization\")\n",
    "    \n",
    "    # Check dependencies\n",
    "    print(\"\\nChecking dependencies...\")\n",
    "    if not BIOPYTHON_AVAILABLE:\n",
    "        print(\"  ✗ BioPython NOT available - REQUIRED\")\n",
    "        print(\"    Install with: pip install biopython\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"  ✓ BioPython available\")\n",
    "    \n",
    "    if PROTPARAM_AVAILABLE:\n",
    "        print(\"  ✓ Bio.SeqUtils.ProtParam available\")\n",
    "    else:\n",
    "        print(\"  ⚠ Bio.SeqUtils.ProtParam not available (will use fallback)\")\n",
    "    \n",
    "    if TQDM_AVAILABLE:\n",
    "        print(\"  ✓ tqdm available (progress bars enabled)\")\n",
    "    else:\n",
    "        print(\"  ⚠ tqdm not available (install with: pip install tqdm)\")\n",
    "    \n",
    "    if VISUALIZATION_AVAILABLE:\n",
    "        print(\"  ✓ Visualization libraries available\")\n",
    "    else:\n",
    "        print(\"  ⚠ matplotlib/seaborn not available (install with: pip install matplotlib seaborn)\")\n",
    "    \n",
    "    # Validate input files\n",
    "    print(\"\\nValidating input files...\")\n",
    "    \n",
    "    if not Path(FASTA_FILE).exists():\n",
    "        print(f\"  ✗ FASTA file not found: {FASTA_FILE}\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(f\"  ✓ FASTA file found\")\n",
    "    \n",
    "    metadata_df = None\n",
    "    if Path(METADATA_FILE).exists():\n",
    "        print(f\"  ✓ Metadata file found\")\n",
    "        try:\n",
    "            metadata_df = pd.read_csv(METADATA_FILE, low_memory=False)\n",
    "            print(f\"    Loaded {len(metadata_df):,} metadata records\")\n",
    "            print(f\"    Original metadata columns: {len(metadata_df.columns)}\")\n",
    "            print(f\"    Column order will be preserved in output\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Error loading metadata: {e}\")\n",
    "            print(\"    Continuing without metadata...\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Metadata file not found (continuing without metadata)\")\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process FASTA file\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PROCESSING SEQUENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = process_fasta_file(FASTA_FILE, metadata_df)\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"  Total sequences: {len(df):,}\")\n",
    "    print(f\"  Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # นับคอลัมน์แบบละเอียด\n",
    "    if metadata_df is not None:\n",
    "        original_metadata_cols = metadata_df.columns.tolist()\n",
    "        original_cols = [c for c in df.columns if c in original_metadata_cols]\n",
    "        analysis_cols = [c for c in df.columns if c.startswith('ANALYSIS_')]\n",
    "        other_cols = [c for c in df.columns if c not in original_cols and not c.startswith('ANALYSIS_')]\n",
    "        \n",
    "        print(f\"    - Original metadata (preserved order): {len(original_cols)}\")\n",
    "        if other_cols:\n",
    "            print(f\"    - Other columns: {len(other_cols)}\")\n",
    "        print(f\"    - Analysis columns (at the end): {len(analysis_cols)}\")\n",
    "    else:\n",
    "        original_cols = [c for c in df.columns if not c.startswith('ANALYSIS_')]\n",
    "        analysis_cols = [c for c in df.columns if c.startswith('ANALYSIS_')]\n",
    "        print(f\"    - Original: {len(original_cols)}\")\n",
    "        print(f\"    - Analysis: {len(analysis_cols)}\")\n",
    "    \n",
    "    # Save main results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"\\n✓ Main results: {OUTPUT_FILE}\")\n",
    "        print(f\"  {len(df):,} rows × {len(df.columns)} columns\")\n",
    "        print(f\"  Column order: Original metadata → Analysis results\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error saving main results: {e}\")\n",
    "    \n",
    "    # Generate summaries\n",
    "    try:\n",
    "        summary_df = generate_summary_statistics(df)\n",
    "        if summary_df is not None:\n",
    "            summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "            print(f\"\\n✓ Summary statistics: {OUTPUT_SUMMARY}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error generating summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        codon_df = generate_codon_usage_table(df, max_seqs=MAX_SEQUENCES_FOR_CODON)\n",
    "        if codon_df is not None:\n",
    "            codon_df.to_csv(OUTPUT_CODON_USAGE, index=False)\n",
    "            print(f\"\\n✓ Codon usage table: {OUTPUT_CODON_USAGE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error generating codon usage: {e}\")\n",
    "    \n",
    "    try:\n",
    "        aa_df = generate_aa_composition(df)\n",
    "        if aa_df is not None:\n",
    "            aa_df.to_csv(OUTPUT_AA_COMPOSITION, index=False)\n",
    "            print(f\"\\n✓ Amino acid composition: {OUTPUT_AA_COMPOSITION}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error generating AA composition: {e}\")\n",
    "    \n",
    "    try:\n",
    "        motif_df = generate_motif_summary(df)\n",
    "        if motif_df is not None:\n",
    "            motif_df.to_csv(OUTPUT_MOTIF_SUMMARY, index=False)\n",
    "            print(f\"\\n✓ Motif summary: {OUTPUT_MOTIF_SUMMARY}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error generating motif summary: {e}\")\n",
    "    \n",
    "    # Export FASTA files\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPORTING FASTA FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        export_fasta(df, OUTPUT_FASTA_HIGH_QUALITY,\n",
    "                    priority_filter=['Priority_1_Highest', 'Priority_2_High'])\n",
    "        \n",
    "        export_fasta(df, OUTPUT_FASTA_CORRECTED, priority_filter=None)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error exporting FASTA: {e}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    if CREATE_VISUALIZATIONS and VISUALIZATION_AVAILABLE:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"CREATING VISUALIZATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        viz_dir = f\"{OUTPUT_DIR}/visualizations\"\n",
    "        viz_count = create_visualizations(df, viz_dir)\n",
    "        \n",
    "        if viz_count > 0:\n",
    "            print(f\"\\n✓ Created {viz_count} visualization files\")\n",
    "            print(f\"  Location: {viz_dir}\")\n",
    "    \n",
    "    # Print report\n",
    "    print_comprehensive_report(df)\n",
    "    \n",
    "    # Column organization\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COLUMN ORGANIZATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if metadata_df is not None:\n",
    "        original_metadata_cols = metadata_df.columns.tolist()\n",
    "        orig_cols = [c for c in df.columns if c in original_metadata_cols]\n",
    "        analysis_cols = [c for c in df.columns if c.startswith('ANALYSIS_')]\n",
    "        \n",
    "        print(f\"\\n✓ Original Metadata Columns ({len(orig_cols)}) - ORDER PRESERVED:\")\n",
    "        for i, col in enumerate(orig_cols[:15], 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        if len(orig_cols) > 15:\n",
    "            print(f\"  ... and {len(orig_cols) - 15} more\")\n",
    "        \n",
    "        print(f\"\\n✓ New Analysis Columns ({len(analysis_cols)}) - ADDED AT THE END:\")\n",
    "        for i, col in enumerate(analysis_cols[:15], 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        if len(analysis_cols) > 15:\n",
    "            print(f\"  ... and {len(analysis_cols) - 15} more\")\n",
    "    else:\n",
    "        orig_cols = [c for c in df.columns if not c.startswith('ANALYSIS_')]\n",
    "        analysis_cols = [c for c in df.columns if c.startswith('ANALYSIS_')]\n",
    "        \n",
    "        print(f\"\\nOriginal Columns ({len(orig_cols)}):\")\n",
    "        for col in orig_cols[:10]:\n",
    "            print(f\"  - {col}\")\n",
    "        if len(orig_cols) > 10:\n",
    "            print(f\"  ... and {len(orig_cols) - 10} more\")\n",
    "        \n",
    "        print(f\"\\nNew Analysis Columns ({len(analysis_cols)}):\")\n",
    "        for col in analysis_cols[:10]:\n",
    "            print(f\"  - {col}\")\n",
    "        if len(analysis_cols) > 10:\n",
    "            print(f\"  ... and {len(analysis_cols) - 10} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OUTPUT FILES SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nGenerated Files:\")\n",
    "    print(f\"\\n1. Main Analysis Results\")\n",
    "    print(f\"   Path: {OUTPUT_FILE}\")\n",
    "    print(f\"   Info: {len(df):,} sequences × {len(df.columns)} columns\")\n",
    "    print(f\"   Structure: [Original Metadata] + [ANALYSIS_* columns]\")\n",
    "    \n",
    "    print(f\"\\n2. Summary Statistics\")\n",
    "    print(f\"   Path: {OUTPUT_SUMMARY}\")\n",
    "    \n",
    "    print(f\"\\n3. Codon Usage Table\")\n",
    "    print(f\"   Path: {OUTPUT_CODON_USAGE}\")\n",
    "    \n",
    "    print(f\"\\n4. Amino Acid Composition\")\n",
    "    print(f\"   Path: {OUTPUT_AA_COMPOSITION}\")\n",
    "    \n",
    "    print(f\"\\n5. Motif Detection Summary\")\n",
    "    print(f\"   Path: {OUTPUT_MOTIF_SUMMARY}\")\n",
    "    \n",
    "    print(f\"\\n6. High Quality FASTA\")\n",
    "    print(f\"   Path: {OUTPUT_FASTA_HIGH_QUALITY}\")\n",
    "    \n",
    "    print(f\"\\n7. All Corrected FASTA\")\n",
    "    print(f\"   Path: {OUTPUT_FASTA_CORRECTED}\")\n",
    "    \n",
    "    if CREATE_VISUALIZATIONS and VISUALIZATION_AVAILABLE:\n",
    "        print(f\"\\n8. Visualizations\")\n",
    "        print(f\"   Path: {OUTPUT_DIR}/visualizations/\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal runtime: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"\\n📊 Output file structure:\")\n",
    "    print(\"   - Original metadata columns in their ORIGINAL ORDER\")\n",
    "    print(\"   - Analysis columns (ANALYSIS_*) appended at the END\")\n",
    "    print(\"   - Ready for abundance-based classification!\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
